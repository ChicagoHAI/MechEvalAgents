{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Circuit Exam - GPT2-Small\n",
    "\n",
    "This exam evaluates understanding of the sarcasm detection circuit documented in the research.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Answer all questions based solely on the provided documentation\n",
    "2. For multiple choice questions, provide the letter (A, B, C, or D)\n",
    "3. For free generation questions, provide clear, concise answers\n",
    "4. For code questions, complete the TODO sections in the provided stubs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Knowledge Points\n",
    "\n",
    "The sarcasm circuit research covers the following key concepts:\n",
    "\n",
    "### 1. **Circuit Discovery Method**\n",
    "- Differential activation analysis on paired sarcastic/literal examples\n",
    "- L2 norm measurement of activation differences\n",
    "- Budget-constrained component selection\n",
    "\n",
    "### 2. **Circuit Architecture**\n",
    "- 54 total components (1 input, 10 MLPs, 43 attention heads)\n",
    "- Write budget: 11,200 dimensions (100% utilization)\n",
    "- Three-stage hierarchical processing\n",
    "\n",
    "### 3. **Key Components**\n",
    "- m2 (Layer 2 MLP): Primary sarcasm detector (32.47 differential)\n",
    "- m11 (Layer 11 MLP): Final pre-output processing (22.30 differential)\n",
    "- a11.h8, a11.h0: Output integration heads (3.33, 2.74 differential)\n",
    "- Excluded: m3, m4 (minimal differential < 6.5)\n",
    "\n",
    "### 4. **Three-Stage Processing**\n",
    "- **Stage 1 (L0-L2)**: Early incongruity detection via m2\n",
    "- **Stage 2 (L3-L7)**: Signal propagation and refinement\n",
    "- **Stage 3 (L8-L11)**: Final integration into output\n",
    "\n",
    "### 5. **Linguistic Features**\n",
    "- Discourse markers: \"Oh\", \"Wow\", \"Just\"\n",
    "- Positive sentiment words in negative contexts\n",
    "- Contradiction between literal and contextual meaning\n",
    "\n",
    "### 6. **Mechanistic Insights**\n",
    "- Sarcasm detection is early (Layer 2, not middle layers)\n",
    "- MLPs dominate (7,680 dims) vs attention (2,752 dims)\n",
    "- Pattern matching rather than semantic reversal\n",
    "\n",
    "### 7. **Comparison to IOI Circuit**\n",
    "- Different primary mechanism (MLP vs attention)\n",
    "- Different key layers (early vs late)\n",
    "- Different circuit density (dense vs sparse)\n",
    "\n",
    "### 8. **Limitations and Future Work**\n",
    "- No causal validation via ablation\n",
    "- Small dataset (40 examples, 5 analyzed in detail)\n",
    "- Synthetic data vs real-world sarcasm\n",
    "- Budget maximization (minimal circuit likely smaller)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Multiple Choice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the primary computational mechanism used by GPT2-small for sarcasm detection, according to the documented circuit?\n",
    "\n",
    "A) Attention-based information routing across sequence positions  \n",
    "B) Late-layer sentiment polarity reversal  \n",
    "C) Early-layer MLP-based incongruity detection  \n",
    "D) Distributed gradient computation across all 12 layers\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Which component shows the most dominant differential activation in the sarcasm circuit, and what is its approximate differential activation value?\n",
    "\n",
    "A) a11.h8 (Layer 11, Head 8) with differential ~3.33  \n",
    "B) m2 (Layer 2 MLP) with differential ~32.47  \n",
    "C) m11 (Layer 11 MLP) with differential ~22.30  \n",
    "D) a4.h11 (Layer 4, Head 11) with differential ~1.40\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the total write budget utilized by the documented sarcasm circuit?\n",
    "\n",
    "A) 8,448 dimensions (75% of budget)  \n",
    "B) 9,600 dimensions (86% of budget)  \n",
    "C) 10,240 dimensions (91% of budget)  \n",
    "D) 11,200 dimensions (100% of budget)\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Which two MLP layers were excluded from the circuit due to minimal differential activation?\n",
    "\n",
    "A) m0 and m1  \n",
    "B) m3 and m4  \n",
    "C) m5 and m6  \n",
    "D) m10 and m11\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What method was used to identify components causally important for sarcasm detection?\n",
    "\n",
    "A) Gradient-based attribution analysis  \n",
    "B) Systematic ablation testing with behavioral metrics  \n",
    "C) Differential activation analysis on paired examples  \n",
    "D) Linear probing with supervised sarcasm classifiers\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "How many attention heads were included in the final circuit, and which layer contains the most important attention heads?\n",
    "\n",
    "A) 43 heads total, with the most important in Layer 11  \n",
    "B) 101 heads total, with the most important in Layer 4  \n",
    "C) 54 heads total, with the most important in Layer 2  \n",
    "D) 19 heads total, with the most important in Layer 6\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "In the differential activation analysis method, activations were averaged over which dimension to handle variable-length inputs?\n",
    "\n",
    "A) Batch dimension  \n",
    "B) Sequence position dimension  \n",
    "C) Model dimension (d_model)  \n",
    "D) Head dimension (d_head)\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "What is a key limitation of using differential activation (L2 norm of activation differences) as the selection criterion for circuit components?\n",
    "\n",
    "A) It cannot handle variable-length sequences  \n",
    "B) It requires expensive gradient computation  \n",
    "C) It only works for attention mechanisms, not MLPs  \n",
    "D) High differential activation does not guarantee causal importance\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "According to the documentation, what is the dimension of each attention head's output in GPT2-small?\n",
    "\n",
    "A) 768 dimensions  \n",
    "B) 128 dimensions  \n",
    "C) 64 dimensions  \n",
    "D) 32 dimensions\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Free Generation Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The documentation describes a three-stage hierarchical process for sarcasm detection. Describe each stage, identify the key components involved, and explain the computational function performed at each stage.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Compare the sarcasm circuit to the Indirect Object Identification (IOI) circuit along four dimensions: primary mechanism, key layer, circuit size, and relative importance of attention vs. MLPs. What does this comparison suggest about linguistic task processing in transformers?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Explain the key linguistic features that distinguish sarcastic from literal sentences in the dataset. How might these features enable Layer 2 MLP to detect incongruity?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "The original hypothesis predicted that middle layers (L4-L7) would be the primary detection site, but empirical evidence showed Layer 2 as the primary detector. Explain this discrepancy and what it reveals about the mechanistic difference between the predicted and actual sarcasm processing.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "The circuit uses 10 MLPs (7,680 dims) versus 43 attention heads (2,752 dims). Given the budget-constrained selection algorithm described in the documentation, explain why this distribution occurred and what it implies about the relative importance of MLPs vs. attention for sarcasm detection.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Based on the documented circuit structure and the exclusion of m3 and m4, propose a hypothesis for why these specific middle layers might show minimal differential activation. What experiments would you conduct to test this hypothesis?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "\n",
    "The documentation mentions that the circuit hasn't been validated with ablation testing. Design a systematic ablation experiment to test the sufficiency and necessity of the identified circuit components. Your design should address both individual component importance and potential interaction effects.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Code Questions\n",
    "\n",
    "Complete the following code exercises to verify claims made in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Question 1 (CQ1): Write Budget Verification\n",
    "\n",
    "Write code to verify the write budget calculation for the documented circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads) and the dimension specifications (d_model=768, d_head=64), compute the total write cost and verify it matches the documented 11,200 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Question 1: Write Budget Verification\n",
    "\n",
    "# Given specifications\n",
    "d_model = 768  # Dimension for input embedding and MLPs\n",
    "d_head = 64    # Dimension for attention heads\n",
    "\n",
    "# Circuit composition\n",
    "num_input = 1\n",
    "num_mlps = 10\n",
    "num_attention_heads = 43\n",
    "\n",
    "# TODO: Calculate the total write cost\n",
    "# Hint: total_cost = (input_cost) + (mlp_cost) + (attention_head_cost)\n",
    "total_write_cost = 0  # Replace with your calculation\n",
    "\n",
    "# TODO: Verify if it matches the documented budget\n",
    "documented_budget = 11200\n",
    "matches = False  # Replace with your verification\n",
    "\n",
    "print(f\"Calculated total write cost: {total_write_cost}\")\n",
    "print(f\"Documented budget: {documented_budget}\")\n",
    "print(f\"Budget matches documentation: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Question 2 (CQ2): Differential Activation Percentage Verification\n",
    "\n",
    "The documentation claims m2 is approximately 45% stronger than m11 in differential activation. Write code to verify this claim by computing the percentage difference between m2's differential (32.47) and m11's differential (22.30), and check if it's approximately 45%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Question 2: Differential Activation Percentage Verification\n",
    "\n",
    "# Given differential activation values\n",
    "m2_diff = 32.47\n",
    "m11_diff = 22.30\n",
    "\n",
    "# TODO: Calculate the percentage by which m2 is stronger than m11\n",
    "# Hint: percentage_stronger = ((m2_diff - m11_diff) / m11_diff) * 100\n",
    "percentage_stronger = 0.0  # Replace with your calculation\n",
    "\n",
    "# TODO: Check if it's approximately 45% (within ±2% tolerance)\n",
    "claimed_percentage = 45.0\n",
    "tolerance = 2.0\n",
    "approximately_correct = False  # Replace with your verification\n",
    "\n",
    "print(f\"m2 differential: {m2_diff}\")\n",
    "print(f\"m11 differential: {m11_diff}\")\n",
    "print(f\"Percentage stronger: {percentage_stronger:.2f}%\")\n",
    "print(f\"Claimed percentage: {claimed_percentage}%\")\n",
    "print(f\"Approximately correct (±{tolerance}%): {approximately_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Question 3 (CQ3): Attention Head Distribution Verification\n",
    "\n",
    "The circuit includes attention heads distributed across layers. Write code to verify the documented distribution: 9 heads in early layers (L0-L3), 19 heads in middle layers (L4-L7), and 15 heads in late layers (L8-L11). Parse the provided list of attention head components and compute the actual distribution to verify these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Question 3: Attention Head Distribution Verification\n",
    "\n",
    "# Sample attention head components from the documentation\n",
    "# In practice, you would have all 43 heads - this is a representative sample\n",
    "# Format: \"a{layer}.h{head}\"\n",
    "attention_heads = [\n",
    "    \"a11.h8\", \"a11.h0\", \"a4.h11\", \"a9.h3\", \"a6.h11\", \"a8.h5\", \n",
    "    \"a9.h10\", \"a5.h3\", \"a10.h5\", \"a11.h3\", \"a0.h2\", \"a1.h5\",\n",
    "    \"a2.h8\", \"a3.h1\", \"a4.h3\", \"a4.h7\", \"a5.h9\", \"a5.h11\",\n",
    "    \"a6.h2\", \"a6.h5\", \"a6.h8\", \"a7.h1\", \"a7.h4\", \"a7.h10\",\n",
    "    \"a8.h0\", \"a8.h3\", \"a8.h9\", \"a9.h1\", \"a9.h5\", \"a9.h8\",\n",
    "    \"a10.h0\", \"a10.h2\", \"a10.h8\", \"a10.h11\", \"a11.h1\", \"a11.h5\",\n",
    "    \"a0.h7\", \"a1.h2\", \"a2.h3\", \"a3.h6\", \"a7.h9\", \"a8.h11\", \"a11.h9\"\n",
    "]\n",
    "\n",
    "# TODO: Parse the layer number from each attention head component\n",
    "# Hint: Extract the number between 'a' and '.h' (e.g., \"a11.h8\" -> layer 11)\n",
    "\n",
    "# TODO: Count heads in each layer range\n",
    "early_layers_count = 0   # L0-L3\n",
    "middle_layers_count = 0  # L4-L7\n",
    "late_layers_count = 0    # L8-L11\n",
    "\n",
    "# TODO: Verify against documented distribution\n",
    "documented_early = 9\n",
    "documented_middle = 19\n",
    "documented_late = 15\n",
    "\n",
    "distribution_matches = False  # Replace with your verification\n",
    "\n",
    "print(f\"Early layers (L0-L3): {early_layers_count} heads (documented: {documented_early})\")\n",
    "print(f\"Middle layers (L4-L7): {middle_layers_count} heads (documented: {documented_middle})\")\n",
    "print(f\"Late layers (L8-L11): {late_layers_count} heads (documented: {documented_late})\")\n",
    "print(f\"Distribution matches documentation: {distribution_matches}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
