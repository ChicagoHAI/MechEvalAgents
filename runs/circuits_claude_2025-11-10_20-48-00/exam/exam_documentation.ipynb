{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7496b567",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/smallyan/critic_model_mechinterp\n",
      "\n",
      "Repository path: /home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00\n",
      "Repository exists: True\n",
      "\n",
      "Contents of repository:\n",
      "  logs/circuit_prompt_sarcarsm_claude.log\n",
      "  logs/code_walk.md\n",
      "  logs/documentation.md\n",
      "  logs/plan_v1.md\n",
      "  logs/plan_v2.md\n",
      "  notebooks/2025-11-10-20-48_SarcasmCircuitAnalysis.ipynb\n",
      "  results/circuit_visualization.png\n",
      "  results/real_circuits_1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Define the repo path\n",
    "repo_path = Path('/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00')\n",
    "print(f\"\\nRepository path: {repo_path}\")\n",
    "print(f\"Repository exists: {repo_path.exists()}\")\n",
    "\n",
    "# List contents of the repository\n",
    "if repo_path.exists():\n",
    "    print(f\"\\nContents of repository:\")\n",
    "    for item in sorted(repo_path.rglob('*')):\n",
    "        if item.is_file() and 'evaluation' not in str(item):\n",
    "            print(f\"  {item.relative_to(repo_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b84507a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation length: 10312 characters\n",
      "\n",
      "================================================================================\n",
      "DOCUMENTATION CONTENT:\n",
      "================================================================================\n",
      "# Sarcasm Circuit Documentation - GPT2-Small\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "Identify a precise circuit in GPT2-small that enables sarcasm recognition by detecting contradictions between literal sentiment and contextual tone. The circuit must:\n",
      "- Reproduce sarcasm detection behavior with high fidelity\n",
      "- Remain within 11,200 dimension write budget\n",
      "- Contain interpretable, minimal components\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset Description\n",
      "- **Source**: Synthetic sarcasm dataset (real mib-bench/sarcasm unavailable)\n",
      "- **Size**: 40 total examples (20 sarcastic, 20 literal)\n",
      "- **Structure**: Paired examples with similar surface structure but opposite intent\n",
      "\n",
      "### Example Sarcastic Sentences\n",
      "1. \"Oh great, another meeting at 7 AM.\"\n",
      "2. \"Wow, I just love getting stuck in traffic.\"\n",
      "3. \"Fantastic, my laptop crashed right before the deadline.\"\n",
      "4. \"Perfect, exactly what I needed today.\"\n",
      "5. \"Oh wonderful, it's raining on my day off.\"\n",
      "\n",
      "### Example Literal Sentences\n",
      "1. \"I'm excited about the meeting at 7 AM tomorrow.\"\n",
      "2. \"I really enjoy my peaceful morning commute.\"\n",
      "3. \"I successfully submitted my project before the deadline.\"\n",
      "4. \"This is exactly what I needed today.\"\n",
      "5. \"I'm happy to have a relaxing day off.\"\n",
      "\n",
      "### Key Linguistic Features of Sarcasm\n",
      "- **Discourse markers**: \"Oh\", \"Wow\", \"Just\" (emphasis particles)\n",
      "- **Positive sentiment words**: \"great\", \"love\", \"fantastic\", \"wonderful\", \"perfect\"\n",
      "- **Negative situational context**: \"another meeting\", \"stuck in traffic\", \"crashed\"\n",
      "- **Contradiction**: Positive words describe objectively negative situations\n",
      "\n",
      "## 3. Method\n",
      "\n",
      "### Experimental Approach\n",
      "We used **differential activation analysis** to identify components causally important for sarcasm detection.\n",
      "\n",
      "#### Step 1: Activation Collection\n",
      "- Ran GPT2-small on paired sarcastic/literal examples\n",
      "- Collected full activation cache for all layers and components\n",
      "- Used HookedTransformer for easy access to intermediate activations\n",
      "\n",
      "#### Step 2: Differential Analysis\n",
      "For each component (attention head or MLP):\n",
      "- Computed average activation on sarcastic examples\n",
      "- Computed average activation on literal examples  \n",
      "- Measured L2 norm of difference: `||mean_sarc - mean_lit||_2`\n",
      "- Higher difference indicates stronger sarcasm-specific processing\n",
      "\n",
      "#### Step 3: Component Selection\n",
      "- Ranked components by average differential activation\n",
      "- Selected top components within 11,200 dimension budget\n",
      "- Prioritized MLPs (768 dims each) over attention heads (64 dims each)\n",
      "\n",
      "### Technical Details\n",
      "\n",
      "**Model**: GPT2-small via HookedTransformer\n",
      "- 12 layers\n",
      "- 12 attention heads per layer\n",
      "- d_model = 768\n",
      "- d_head = 64\n",
      "\n",
      "**Write Budget Calculation**:\n",
      "- Input embedding: 768 dimensions\n",
      "- Each MLP layer: 768 dimensions\n",
      "- Each attention head: 64 dimensions  \n",
      "- Maximum budget: 11,200 dimensions\n",
      "\n",
      "**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\n",
      "\n",
      "## 4. Results\n",
      "\n",
      "### Circuit Composition\n",
      "\n",
      "**Total Components**: 54 (maximizing budget utilization)\n",
      "- Input: 1 (768 dims)\n",
      "- MLPs: 10 (7,680 dims)\n",
      "- Atte\n"
     ]
    }
   ],
   "source": [
    "# Read the documentation\n",
    "doc_path = repo_path / 'logs' / 'documentation.md'\n",
    "with open(doc_path, 'r') as f:\n",
    "    documentation = f.read()\n",
    "\n",
    "print(\"Documentation length:\", len(documentation), \"characters\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENTATION CONTENT:\")\n",
    "print(\"=\"*80)\n",
    "print(documentation[:3000])  # Preview first 3000 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf5f425",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sarcasm Circuit Documentation - GPT2-Small\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "Identify a precise circuit in GPT2-small that enables sarcasm recognition by detecting contradictions between literal sentiment and contextual tone. The circuit must:\n",
      "- Reproduce sarcasm detection behavior with high fidelity\n",
      "- Remain within 11,200 dimension write budget\n",
      "- Contain interpretable, minimal components\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset Description\n",
      "- **Source**: Synthetic sarcasm dataset (real mib-bench/sarcasm unavailable)\n",
      "- **Size**: 40 total examples (20 sarcastic, 20 literal)\n",
      "- **Structure**: Paired examples with similar surface structure but opposite intent\n",
      "\n",
      "### Example Sarcastic Sentences\n",
      "1. \"Oh great, another meeting at 7 AM.\"\n",
      "2. \"Wow, I just love getting stuck in traffic.\"\n",
      "3. \"Fantastic, my laptop crashed right before the deadline.\"\n",
      "4. \"Perfect, exactly what I needed today.\"\n",
      "5. \"Oh wonderful, it's raining on my day off.\"\n",
      "\n",
      "### Example Literal Sentences\n",
      "1. \"I'm excited about the meeting at 7 AM tomorrow.\"\n",
      "2. \"I really enjoy my peaceful morning commute.\"\n",
      "3. \"I successfully submitted my project before the deadline.\"\n",
      "4. \"This is exactly what I needed today.\"\n",
      "5. \"I'm happy to have a relaxing day off.\"\n",
      "\n",
      "### Key Linguistic Features of Sarcasm\n",
      "- **Discourse markers**: \"Oh\", \"Wow\", \"Just\" (emphasis particles)\n",
      "- **Positive sentiment words**: \"great\", \"love\", \"fantastic\", \"wonderful\", \"perfect\"\n",
      "- **Negative situational context**: \"another meeting\", \"stuck in traffic\", \"crashed\"\n",
      "- **Contradiction**: Positive words describe objectively negative situations\n",
      "\n",
      "## 3. Method\n",
      "\n",
      "### Experimental Approach\n",
      "We used **differential activation analysis** to identify components causally important for sarcasm detection.\n",
      "\n",
      "#### Step 1: Activation Collection\n",
      "- Ran GPT2-small on paired sarcastic/literal examples\n",
      "- Collected full activation cache for all layers and components\n",
      "- Used HookedTransformer for easy access to intermediate activations\n",
      "\n",
      "#### Step 2: Differential Analysis\n",
      "For each component (attention head or MLP):\n",
      "- Computed average activation on sarcastic examples\n",
      "- Computed average activation on literal examples  \n",
      "- Measured L2 norm of difference: `||mean_sarc - mean_lit||_2`\n",
      "- Higher difference indicates stronger sarcasm-specific processing\n",
      "\n",
      "#### Step 3: Component Selection\n",
      "- Ranked components by average differential activation\n",
      "- Selected top components within 11,200 dimension budget\n",
      "- Prioritized MLPs (768 dims each) over attention heads (64 dims each)\n",
      "\n",
      "### Technical Details\n",
      "\n",
      "**Model**: GPT2-small via HookedTransformer\n",
      "- 12 layers\n",
      "- 12 attention heads per layer\n",
      "- d_model = 768\n",
      "- d_head = 64\n",
      "\n",
      "**Write Budget Calculation**:\n",
      "- Input embedding: 768 dimensions\n",
      "- Each MLP layer: 768 dimensions\n",
      "- Each attention head: 64 dimensions  \n",
      "- Maximum budget: 11,200 dimensions\n",
      "\n",
      "**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\n",
      "\n",
      "## 4. Results\n",
      "\n",
      "### Circuit Composition\n",
      "\n",
      "**Total Components**: 54 (maximizing budget utilization)\n",
      "- Input: 1 (768 dims)\n",
      "- MLPs: 10 (7,680 dims)\n",
      "- Attention heads: 43 (2,752 dims)\n",
      "- **Total write cost**: 11,200 / 11,200 (100%)\n",
      "\n",
      "### MLP Components (Ranked by Importance)\n",
      "\n",
      "| Component | Avg Diff | Layer | Interpretation |\n",
      "|-----------|----------|-------|----------------|\n",
      "| m2 | 32.47 | 2 | **Primary sarcasm detector** |\n",
      "| m11 | 22.30 | 11 | Final pre-output processing |\n",
      "| m10 | 17.36 | 10 | Late-stage integration |\n",
      "| m9 | 13.41 | 9 | Late-stage integration |\n",
      "| m8 | 11.69 | 8 | Signal refinement |\n",
      "| m7 | 9.69 | 7 | Signal propagation |\n",
      "| m6 | 8.59 | 6 | Signal propagation |\n",
      "| m1 | 7.87 | 1 | Early context encoding |\n",
      "| m5 | 7.79 | 5 | Signal propagation |\n",
      "| m0 | 7.33 | 0 | Initial embedding processing |\n",
      "\n",
      "**Key Finding**: m2 shows **dramatically dominant** differential activation (32.47), ~45% stronger than the next strongest MLP. This suggests Layer 2 is the primary site of sarcasm/incongruity detection.\n",
      "\n",
      "### Attention Head Components\n",
      "\n",
      "**Top 10 Most Important Heads**:\n",
      "\n",
      "| Component | Avg Diff | Interpretation |\n",
      "|-----------|----------|----------------|\n",
      "| a11.h8 | 3.33 | Output integration head |\n",
      "| a11.h0 | 2.74 | Output integration head |\n",
      "| a4.h11 | 1.40 | Mid-layer information routing |\n",
      "| a9.h3 | 1.32 | Late propagation |\n",
      "| a6.h11 | 1.32 | Mid-layer integration |\n",
      "| a8.h5 | 1.31 | Late-stage processing |\n",
      "| a9.h10 | 1.29 | Late propagation |\n",
      "| a5.h3 | 1.28 | Mid-layer routing |\n",
      "| a10.h5 | 1.25 | Pre-output routing |\n",
      "| a11.h3 | 1.23 | Output integration |\n",
      "\n",
      "**Distribution by Layer**:\n",
      "- Layers 0-3: 9 heads (early processing)\n",
      "- Layers 4-7: 19 heads (dense middle routing)\n",
      "- Layers 8-11: 15 heads (late integration)\n",
      "\n",
      "### Excluded Components\n",
      "\n",
      "**MLPs excluded**: m3, m4\n",
      "- Showed minimal differential activation (<6.5)\n",
      "- Suggests these layers less involved in sarcasm processing\n",
      "\n",
      "**Attention heads excluded**: 101 heads\n",
      "- Lower differential activation (<0.83)\n",
      "- Likely performing general language modeling tasks\n",
      "\n",
      "## 5. Analysis\n",
      "\n",
      "### Hypothesis Evolution\n",
      "\n",
      "#### Phase 1: Initial Hypothesis\n",
      "We hypothesized a three-stage process:\n",
      "1. Early layers encode sentiment\n",
      "2. Middle layers detect incongruity\n",
      "3. Late layers reverse meaning\n",
      "\n",
      "#### Phase 2: Revised Understanding\n",
      "Empirical evidence revealed:\n",
      "1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\n",
      "2. Middle layers **propagate** rather than detect sarcasm signal\n",
      "3. Late layers **integrate** rather than reverse sentiment\n",
      "\n",
      "### Mechanistic Interpretation\n",
      "\n",
      "**Stage 1: Early Detection (L0-L2)**\n",
      "- m2 detects incongruity between sentiment words and context\n",
      "- Processes patterns like: positive adjective + negative situation\n",
      "- Output: sarcasm signal that propagates to later layers\n",
      "\n",
      "**Stage 2: Distributed Propagation (L3-L7)**  \n",
      "- Mid-layer MLPs refine the sarcasm signal\n",
      "- 19 attention heads route information across sequence positions\n",
      "- Enables context-aware processing throughout the sentence\n",
      "\n",
      "**Stage 3: Final Integration (L8-L11)**\n",
      "- Late MLPs (especially m11) perform final processing\n",
      "- Layer 11 attention heads (a11.h8, a11.h0) integrate into output\n",
      "- Determines how sarcasm affects final token predictions\n",
      "\n",
      "### Comparison to IOI Circuit\n",
      "\n",
      "The sarcasm circuit differs from the Indirect Object Identification (IOI) circuit:\n",
      "\n",
      "| Aspect | IOI Circuit | Sarcasm Circuit |\n",
      "|--------|-------------|-----------------|\n",
      "| **Primary mechanism** | Name copying via attention | Incongruity detection via MLP |\n",
      "| **Key layer** | Later layers (9-11) | Early layer (2) |\n",
      "| **Circuit size** | Sparse (~10 components) | Dense (54 components) |\n",
      "| **Attention importance** | Dominant | Supporting |\n",
      "| **MLP importance** | Supporting | Dominant |\n",
      "\n",
      "This suggests **different linguistic tasks use different computational strategies** in transformers.\n",
      "\n",
      "## 6. Next Steps\n",
      "\n",
      "### Validation Experiments\n",
      "1. **Ablation testing**: Systematically remove components, measure impact\n",
      "2. **Intervention experiments**: Patch activations to test causality\n",
      "3. **Attention analysis**: Visualize patterns for key heads\n",
      "4. **Probing**: Train linear classifiers to detect sarcasm at each layer\n",
      "\n",
      "### Circuit Refinement\n",
      "1. Analyze all 40 examples (currently only 5 analyzed in detail)\n",
      "2. Test on real-world sarcasm dataset\n",
      "3. Identify minimal sufficient circuit via ablation\n",
      "4. Compare to human sarcasm judgments\n",
      "\n",
      "### Mechanistic Deep Dive\n",
      "1. **m2 analysis**: What features does it compute? \n",
      "2. **Attention patterns**: How does information flow through 43 heads?\n",
      "3. **Interaction effects**: Do components work synergistically?\n",
      "4. **Generalization**: Does circuit transfer to other incongruity tasks?\n",
      "\n",
      "### Open Questions\n",
      "1. Why is m2 so dominant? What about Layer 2 enables incongruity detection?\n",
      "2. Are m3 and m4 intentionally bypassed, or do they serve other functions?\n",
      "3. How do the 43 attention heads divide labor?\n",
      "4. Does the circuit generalize to irony, understatement, and other figurative language?\n",
      "\n",
      "## 7. Main Takeaways\n",
      "\n",
      "### Scientific Insights\n",
      "\n",
      "1. **Sarcasm detection is early**: The network decides at Layer 2, not gradually\n",
      "2. **MLPs dominate**: 10 MLPs contribute 7,680 dims vs. 43 heads contributing 2,752 dims\n",
      "3. **Distributed but hierarchical**: 54 components work in coordinated stages\n",
      "4. **Task-specific architecture**: Different from other documented circuits like IOI\n",
      "\n",
      "### Implications for Interpretability\n",
      "\n",
      "1. **Component specialization**: Different layers specialize in different aspects\n",
      "2. **Non-obvious mechanisms**: Detection happens earlier than linguistically expected\n",
      "3. **Redundancy**: Circuit uses most available budget, suggesting distributed computation\n",
      "4. **Hierarchy matters**: Three-stage processing suggests compositional computation\n",
      "\n",
      "### Practical Applications\n",
      "\n",
      "1. **Sarcasm detection systems**: Focus on early-layer representations\n",
      "2. **Model editing**: m2 could be target for intervention\n",
      "3. **Probing methods**: Layer 2 most informative for sarcasm classification\n",
      "4. **Model design**: Early layers need capacity for complex semantic tasks\n",
      "\n",
      "## 8. Limitations\n",
      "\n",
      "1. **Small dataset**: Only 5 pairs analyzed in detail (40 examples total available)\n",
      "2. **Synthetic data**: Real-world sarcasm may have different patterns\n",
      "3. **No causal validation**: Differential activation ≠ causal importance\n",
      "4. **Single model**: Results specific to GPT2-small\n",
      "5. **Budget maximization**: Used full 11,200 dims; minimal circuit likely smaller\n",
      "6. **No behavioral testing**: Haven't verified circuit reproduces sarcasm detection\n",
      "\n",
      "## 9. Conclusion\n",
      "\n",
      "We identified a 54-component circuit in GPT2-small for sarcasm detection, utilizing the full 11,200 dimension write budget. The circuit exhibits a three-stage hierarchical structure with early detection (Layer 2 MLP), distributed propagation (mid-layer attention and MLPs), and final integration (late-layer components, especially Layer 11 attention heads).\n",
      "\n",
      "The dominant role of m2 (32.47 differential activation) reveals that sarcasm detection occurs remarkably early in the network, earlier than initial linguistic hypotheses suggested. This finding has implications for interpretability research, model editing, and understanding how transformers process complex pragmatic meaning beyond literal semantics.\n",
      "\n",
      "The circuit demonstrates that different linguistic tasks may require fundamentally different computational strategies within the same architecture, with sarcasm relying heavily on MLP-based pattern detection rather than attention-based information routing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the full documentation\n",
    "print(documentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76505ddf",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Knowledge Points Identified:\n",
      "================================================================================\n",
      "\n",
      "1. Goal & Requirements:\n",
      "  • Circuit must identify sarcasm through contradiction detection\n",
      "  • 11,200 dimension write budget constraint\n",
      "  • Need for high fidelity, interpretability, and minimal components\n",
      "\n",
      "2. Data Characteristics:\n",
      "  • 40 examples total (20 sarcastic, 20 literal)\n",
      "  • Synthetic dataset structure\n",
      "  • Key linguistic features: discourse markers, positive sentiment words, negative context, contradiction pattern\n",
      "\n",
      "3. Methodology:\n",
      "  • Differential activation analysis approach\n",
      "  • Three-step process: activation collection, differential analysis, component selection\n",
      "  • L2 norm difference calculation between sarcastic and literal examples\n",
      "  • Component ranking and budget-constrained selection\n",
      "\n",
      "4. Model Architecture:\n",
      "  • GPT2-small: 12 layers, 12 heads per layer\n",
      "  • d_model=768, d_head=64\n",
      "  • Write budget calculation: embedding=768, MLP=768, head=64 dims\n",
      "\n",
      "5. Circuit Composition:\n",
      "  • 54 total components: 1 input + 10 MLPs + 43 attention heads\n",
      "  • Exactly 11,200 dimensions used (100% budget)\n",
      "  • m2 is dramatically dominant (32.47 avg diff)\n",
      "  • m3 and m4 excluded from circuit\n",
      "\n",
      "6. Mechanistic Stages:\n",
      "  • Stage 1 (L0-L2): Early detection, m2 primary detector\n",
      "  • Stage 2 (L3-L7): Distributed propagation, 19 attention heads\n",
      "  • Stage 3 (L8-L11): Final integration, m11 and layer 11 heads\n",
      "\n",
      "7. Comparison with IOI:\n",
      "  • Different primary mechanism (MLP vs attention)\n",
      "  • Different key layer (early vs late)\n",
      "  • Different circuit density (dense vs sparse)\n",
      "  • Task-specific computational strategies\n",
      "\n",
      "8. Key Findings:\n",
      "  • Sarcasm detection happens early (Layer 2, not gradual)\n",
      "  • MLPs contribute 7,680 dims vs heads 2,752 dims\n",
      "  • m2 dominance (45% stronger than next MLP)\n",
      "  • Hypothesis evolution from three-stage gradual to early detection\n",
      "\n",
      "9. Limitations:\n",
      "  • Small dataset (only 5 pairs analyzed in detail)\n",
      "  • Synthetic data only\n",
      "  • No causal validation performed\n",
      "  • Single model (GPT2-small specific)\n",
      "  • Budget maximization vs minimal circuit tradeoff\n",
      "\n",
      "10. Applications & Next Steps:\n",
      "  • Validation experiments: ablation, intervention, probing\n",
      "  • Circuit refinement on full dataset\n",
      "  • Open questions about m2 dominance and m3/m4 exclusion\n",
      "  • Generalization to other figurative language\n"
     ]
    }
   ],
   "source": [
    "# Now let's analyze the documentation and identify key knowledge points\n",
    "\n",
    "key_knowledge_points = {\n",
    "    \"1. Goal & Requirements\": [\n",
    "        \"Circuit must identify sarcasm through contradiction detection\",\n",
    "        \"11,200 dimension write budget constraint\",\n",
    "        \"Need for high fidelity, interpretability, and minimal components\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Data Characteristics\": [\n",
    "        \"40 examples total (20 sarcastic, 20 literal)\",\n",
    "        \"Synthetic dataset structure\",\n",
    "        \"Key linguistic features: discourse markers, positive sentiment words, negative context, contradiction pattern\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Methodology\": [\n",
    "        \"Differential activation analysis approach\",\n",
    "        \"Three-step process: activation collection, differential analysis, component selection\",\n",
    "        \"L2 norm difference calculation between sarcastic and literal examples\",\n",
    "        \"Component ranking and budget-constrained selection\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Model Architecture\": [\n",
    "        \"GPT2-small: 12 layers, 12 heads per layer\",\n",
    "        \"d_model=768, d_head=64\",\n",
    "        \"Write budget calculation: embedding=768, MLP=768, head=64 dims\"\n",
    "    ],\n",
    "    \n",
    "    \"5. Circuit Composition\": [\n",
    "        \"54 total components: 1 input + 10 MLPs + 43 attention heads\",\n",
    "        \"Exactly 11,200 dimensions used (100% budget)\",\n",
    "        \"m2 is dramatically dominant (32.47 avg diff)\",\n",
    "        \"m3 and m4 excluded from circuit\"\n",
    "    ],\n",
    "    \n",
    "    \"6. Mechanistic Stages\": [\n",
    "        \"Stage 1 (L0-L2): Early detection, m2 primary detector\",\n",
    "        \"Stage 2 (L3-L7): Distributed propagation, 19 attention heads\",\n",
    "        \"Stage 3 (L8-L11): Final integration, m11 and layer 11 heads\"\n",
    "    ],\n",
    "    \n",
    "    \"7. Comparison with IOI\": [\n",
    "        \"Different primary mechanism (MLP vs attention)\",\n",
    "        \"Different key layer (early vs late)\",\n",
    "        \"Different circuit density (dense vs sparse)\",\n",
    "        \"Task-specific computational strategies\"\n",
    "    ],\n",
    "    \n",
    "    \"8. Key Findings\": [\n",
    "        \"Sarcasm detection happens early (Layer 2, not gradual)\",\n",
    "        \"MLPs contribute 7,680 dims vs heads 2,752 dims\",\n",
    "        \"m2 dominance (45% stronger than next MLP)\",\n",
    "        \"Hypothesis evolution from three-stage gradual to early detection\"\n",
    "    ],\n",
    "    \n",
    "    \"9. Limitations\": [\n",
    "        \"Small dataset (only 5 pairs analyzed in detail)\",\n",
    "        \"Synthetic data only\",\n",
    "        \"No causal validation performed\",\n",
    "        \"Single model (GPT2-small specific)\",\n",
    "        \"Budget maximization vs minimal circuit tradeoff\"\n",
    "    ],\n",
    "    \n",
    "    \"10. Applications & Next Steps\": [\n",
    "        \"Validation experiments: ablation, intervention, probing\",\n",
    "        \"Circuit refinement on full dataset\",\n",
    "        \"Open questions about m2 dominance and m3/m4 exclusion\",\n",
    "        \"Generalization to other figurative language\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Key Knowledge Points Identified:\")\n",
    "print(\"=\"*80)\n",
    "for category, points in key_knowledge_points.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for point in points:\n",
    "        print(f\"  • {point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350bdab2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions created: 23\n",
      "\n",
      "Breakdown by type:\n",
      "  Multiple choice: 9\n",
      "  Free generation: 14\n"
     ]
    }
   ],
   "source": [
    "# Now let's design comprehensive exam questions\n",
    "# Mix of factual recall, comprehension, application, and reasoning\n",
    "\n",
    "exam_questions = []\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 1: COMPREHENSION & RECALL (Basic factual understanding)\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"What is the write budget constraint for the sarcasm detection circuit?\",\n",
    "    \"answer\": \"11,200 dimensions\",\n",
    "    \"choices\": [\"7,680 dimensions\", \"11,200 dimensions\", \"12,288 dimensions\", \"54,000 dimensions\"],\n",
    "    \"reference\": \"Section 1 (Goal) and Section 4 (Results - Circuit Composition)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"How many total examples were in the sarcasm dataset?\",\n",
    "    \"answer\": \"40 examples (20 sarcastic, 20 literal)\",\n",
    "    \"choices\": [\"20 examples (10 sarcastic, 10 literal)\", \n",
    "                \"40 examples (20 sarcastic, 20 literal)\", \n",
    "                \"100 examples (50 sarcastic, 50 literal)\",\n",
    "                \"5 paired examples\"],\n",
    "    \"reference\": \"Section 2 (Data - Dataset Description)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Which MLP component showed the highest differential activation and is considered the primary sarcasm detector?\",\n",
    "    \"answer\": \"m2 (Layer 2 MLP) with 32.47 average differential activation\",\n",
    "    \"choices\": [\"m0 (Layer 0 MLP) with 7.33 average differential activation\",\n",
    "                \"m2 (Layer 2 MLP) with 32.47 average differential activation\",\n",
    "                \"m11 (Layer 11 MLP) with 22.30 average differential activation\",\n",
    "                \"m10 (Layer 10 MLP) with 17.36 average differential activation\"],\n",
    "    \"reference\": \"Section 4 (Results - MLP Components table and Key Finding)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"List the three key linguistic features that distinguish sarcastic sentences from literal ones according to the documentation.\",\n",
    "    \"answer\": \"1) Discourse markers (e.g., 'Oh', 'Wow', 'Just'), 2) Positive sentiment words (e.g., 'great', 'love', 'fantastic'), 3) Negative situational context (e.g., 'stuck in traffic', 'crashed'), with the key pattern being the contradiction between positive words and negative situations.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 2 (Data - Key Linguistic Features of Sarcasm)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 2: CAUSAL & MECHANISTIC REASONING\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation states that m3 and m4 were excluded from the circuit. If you were to add m3 back into the circuit, how would this affect the write budget, and what would you need to adjust to stay within the 11,200 dimension limit?\",\n",
    "    \"answer\": \"Adding m3 would add 768 dimensions to the circuit, bringing the total from 11,200 to 11,968 dimensions, which exceeds the budget by 768 dimensions. To stay within the limit, you would need to remove either one MLP (768 dims) or 12 attention heads (12 × 64 = 768 dims) from the current circuit. Since the circuit currently uses exactly 11,200 dimensions (100% budget), any addition requires corresponding removal.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 3 (Method - Technical Details - Write Budget Calculation) and Section 4 (Results - Excluded Components)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"According to the differential activation analysis method, what does a higher L2 norm difference between mean activations indicate?\",\n",
    "    \"answer\": \"Stronger sarcasm-specific processing by that component\",\n",
    "    \"choices\": [\"Better general language modeling capability\",\n",
    "                \"Stronger sarcasm-specific processing by that component\",\n",
    "                \"Higher computational cost for that component\",\n",
    "                \"Greater need for that component to be excluded\"],\n",
    "    \"reference\": \"Section 3 (Method - Step 2: Differential Analysis)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The initial hypothesis suggested that middle layers detect incongruity, but the empirical evidence showed otherwise. Explain what the middle layers (L3-L7) actually do according to the revised understanding, and why this differs from the initial hypothesis.\",\n",
    "    \"answer\": \"According to the revised understanding, middle layers (L3-L7) propagate and refine the sarcasm signal rather than detect incongruity. The primary detection happens much earlier at Layer 2 (m2). The middle layers use 19 attention heads to route information across sequence positions and enable context-aware processing. This differs from the initial hypothesis because detection occurs earlier than expected (L2 rather than middle layers), and the middle layers serve a supporting propagation role rather than the primary detection role.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 5 (Analysis - Hypothesis Evolution and Mechanistic Interpretation - Stage 2)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 3: HYPOTHETICAL TRANSFER & APPLICATION\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Suppose you want to build a similar circuit for detecting irony (another form of figurative language) in GPT2-small. Based on the sarcasm circuit findings, which layer would you hypothesize as most important for irony detection, and what experimental approach would you use to test this?\",\n",
    "    \"answer\": \"Based on the sarcasm circuit findings showing early detection at Layer 2, I would hypothesize that irony detection also occurs in early layers (L1-L3), particularly focusing on Layer 2 MLPs. To test this, I would use the same differential activation analysis approach: (1) collect activations on paired ironic/literal examples, (2) compute L2 norm differences for each component, (3) identify which components show the highest differential activation. If the hypothesis holds, we should see dominant differential activation in early-layer MLPs similar to m2's role in sarcasm. This could reveal whether figurative language detection generally occurs early in transformer processing.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 5 (Analysis - Mechanistic Interpretation - Stage 1) and Section 6 (Next Steps - Open Questions about generalization to other figurative language)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The circuit uses 10 MLPs (7,680 dims) and 43 attention heads (2,752 dims). If you were redesigning the circuit with a smaller budget of 5,600 dimensions (half the original), describe a principled strategy for selecting which components to keep, and justify your choices based on the documented findings.\",\n",
    "    \"answer\": \"A principled strategy would prioritize components with highest differential activation: (1) Keep the input embedding (768 dims - essential). (2) Keep m2 (768 dims - dramatically dominant at 32.47, the primary detector). (3) Keep m11 (768 dims - second highest at 22.30, handles final pre-output processing). (4) Keep m10 and m9 (1,536 dims - late-stage integration at 17.36 and 13.41). This totals 3,840 dims for MLPs. (5) For remaining 1,760 dims, keep the top ~27 attention heads (starting with a11.h8, a11.h0, etc.) based on their differential activation rankings. This strategy preserves the three-stage hierarchy: early detection (m2), propagation (some mid-layer heads), and final integration (m10, m11, and late-layer heads), while maximizing the inclusion of high-differential components.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 4 (Results - MLP Components and Attention Head Components tables) and Section 5 (Analysis - Mechanistic Interpretation)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"If you applied the sarcasm circuit to a sentence with ambiguous intent like 'That was interesting', what would be the most likely reason for circuit failure based on the documented linguistic features?\",\n",
    "    \"answer\": \"The sentence lacks clear contradiction between positive sentiment words and negative situational context\",\n",
    "    \"choices\": [\"The sentence is too short for the circuit to process\",\n",
    "                \"The sentence lacks discourse markers like 'Oh' or 'Wow'\",\n",
    "                \"The sentence lacks clear contradiction between positive sentiment words and negative situational context\",\n",
    "                \"The sentence would activate m3 and m4 which are excluded from the circuit\"],\n",
    "    \"reference\": \"Section 2 (Data - Key Linguistic Features of Sarcasm, specifically the Contradiction feature)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 4: COMPARATIVE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Compare the sarcasm circuit to the IOI (Indirect Object Identification) circuit along three dimensions: primary mechanism, circuit size, and key layer. What does this comparison suggest about how different linguistic tasks are processed in transformers?\",\n",
    "    \"answer\": \"Sarcasm circuit: (1) Primary mechanism is MLP-based incongruity detection, (2) Dense circuit with 54 components, (3) Key layer is early (Layer 2). IOI circuit: (1) Primary mechanism is attention-based name copying, (2) Sparse circuit with ~10 components, (3) Key layers are late (9-11). This comparison suggests that different linguistic tasks use fundamentally different computational strategies within the same architecture. Tasks requiring pattern detection and semantic contradiction (sarcasm) rely more on MLPs and early processing, while tasks requiring information routing and copying (IOI) rely more on attention and late processing. This demonstrates task-specific architectural utilization in transformers.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 5 (Analysis - Comparison to IOI Circuit table and concluding statement)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"In the three-stage mechanistic interpretation, which stage has the most attention heads involved?\",\n",
    "    \"answer\": \"Stage 2 (Distributed Propagation, L3-L7) with 19 attention heads\",\n",
    "    \"choices\": [\"Stage 1 (Early Detection, L0-L2) with 9 attention heads\",\n",
    "                \"Stage 2 (Distributed Propagation, L3-L7) with 19 attention heads\",\n",
    "                \"Stage 3 (Final Integration, L8-L11) with 15 attention heads\",\n",
    "                \"All stages have equal attention head distribution\"],\n",
    "    \"reference\": \"Section 4 (Results - Attention Head Components - Distribution by Layer) and Section 5 (Analysis - Mechanistic Interpretation - Stage 2)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 5: EXPERIMENTAL DESIGN & METHODOLOGY\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation states that 'differential activation ≠ causal importance' as a limitation. Describe two validation experiments from the 'Next Steps' section that would help establish causal importance, and explain how each addresses this limitation.\",\n",
    "    \"answer\": \"Two validation experiments: (1) Ablation testing - systematically remove components and measure the impact on sarcasm detection performance. This establishes causality by showing whether removing a component degrades the behavior, proving it's not just correlated but actually necessary. (2) Intervention experiments - patch activations to test causality by modifying specific component activations and observing downstream effects. This establishes causality by demonstrating that manipulating a component's output directly changes sarcasm detection behavior. Both methods go beyond correlation (differential activation) to demonstrate that components are causally responsible for the observed behavior.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 6 (Next Steps - Validation Experiments) and Section 8 (Limitations - point 3)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Why were activations averaged over sequence positions during the analysis?\",\n",
    "    \"answer\": \"To handle variable-length inputs\",\n",
    "    \"choices\": [\"To reduce computational cost\",\n",
    "                \"To handle variable-length inputs\",\n",
    "                \"To emphasize the importance of early tokens\",\n",
    "                \"To make the circuit more interpretable\"],\n",
    "    \"reference\": \"Section 3 (Method - Technical Details - Normalization)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"In the component selection step (Step 3), the method prioritized MLPs over attention heads. Given that each MLP contributes 768 dimensions versus 64 dimensions per attention head, calculate how many attention heads would be equivalent to adding one MLP in terms of write budget. Then explain why prioritizing MLPs makes sense given the budget constraint.\",\n",
    "    \"answer\": \"One MLP (768 dims) is equivalent to 12 attention heads (12 × 64 = 768 dims) in terms of write budget. Prioritizing MLPs makes sense because: (1) You can test fewer components to fill the budget (testing 10 MLPs vs. 120 attention heads for the same dimensions), making component selection more efficient. (2) If MLPs show high differential activation, you get more 'bang for your buck' - a single high-performing MLP provides more signal than you'd get from 12 lower-performing attention heads. (3) The method ranks by differential activation first, so if top-ranked components are MLPs, including them maximizes the quality of components within budget constraints.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 3 (Method - Step 3: Component Selection and Technical Details - Write Budget Calculation)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 6: CRITICAL REFLECTION & LIMITATIONS\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation lists 'Budget maximization' as a limitation, noting that the minimal circuit is likely smaller than 54 components. Explain why using the full 11,200 dimension budget might not represent the minimal sufficient circuit, and what tradeoff this represents.\",\n",
    "    \"answer\": \"Using the full budget maximizes coverage but likely includes redundant or marginally important components. The circuit selection method ranked components by differential activation and included all components that fit within budget, but lower-ranked components may contribute minimally to actual sarcasm detection. The minimal sufficient circuit would only include components that are necessary and sufficient for the behavior, which could be significantly smaller. The tradeoff is between completeness (capturing all potentially relevant components) and parsimony (finding the simplest explanation). Budget maximization prioritizes completeness but at the cost of including potentially unnecessary components, making the circuit harder to interpret and validate.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 8 (Limitations - point 5: 'Budget maximization: Used full 11,200 dims; minimal circuit likely smaller')\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Which of the following is NOT listed as a limitation of this study?\",\n",
    "    \"answer\": \"The model architecture (GPT2-small) is too small for sarcasm detection\",\n",
    "    \"choices\": [\"Only 5 pairs were analyzed in detail from the 40 examples available\",\n",
    "                \"The dataset consists of synthetic rather than real-world sarcasm\",\n",
    "                \"No causal validation (such as ablation tests) was performed\",\n",
    "                \"The model architecture (GPT2-small) is too small for sarcasm detection\"],\n",
    "    \"reference\": \"Section 8 (Limitations) - lists small dataset, synthetic data, no causal validation, and single model specificity, but does not claim the model is too small\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 7: CREATIVE SYNTHESIS & OPEN QUESTIONS\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation poses an open question: 'Why is m2 so dominant? What about Layer 2 enables incongruity detection?' Based on your understanding of transformer architecture and the three-stage processing model, propose a hypothesis that could explain m2's dramatic dominance (45% stronger than the next strongest MLP).\",\n",
    "    \"answer\": \"Possible hypothesis: Layer 2 may be positioned at a 'sweet spot' in the network where (1) sufficient low-level features have been extracted by Layer 0 and 1 (basic token embeddings and local context) to identify linguistic patterns, but (2) the representations are not yet too abstract or task-specific as in later layers. Incongruity detection requires identifying the co-occurrence of contradictory elements (positive sentiment words + negative context), which may require some compositional understanding but not deep semantic reasoning. Layer 2 might specialize in detecting these mid-level semantic patterns. Alternatively, GPT2's training may have naturally allocated Layer 2 for pragmatic/non-literal language processing tasks, separating them from literal semantic processing in other layers. The 45% dominance suggests this is not just a gradual process but a discrete computational step concentrated in this specific layer.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 6 (Next Steps - Open Questions, question 1) and Section 5 (Analysis - Mechanistic Interpretation)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Design a follow-up experiment to test whether the sarcasm circuit generalizes to other forms of figurative language such as understatement (e.g., 'It's just a scratch' for a serious injury). Describe your experimental setup, what data you would collect, and what results would support or refute generalization.\",\n",
    "    \"answer\": \"Experimental setup: (1) Create a dataset of 40 paired examples (20 understatement, 20 literal) with similar structure to the sarcasm data. Example understatement: 'It's a bit chilly' (during a blizzard). Example literal: 'It's extremely cold outside.' (2) Run GPT2-small on this dataset using the identified 54-component sarcasm circuit (same hooks/activations). (3) Measure differential activation between understatement and literal examples for the same 54 components. Results supporting generalization: If m2 shows dominant differential activation for understatement (>20 avg diff) and the three-stage pattern is preserved, this suggests the circuit detects general pragmatic contradiction, not sarcasm-specific patterns. Results refuting generalization: If different components (e.g., m5, m7) dominate, or if late layers become more important, this suggests task-specific circuits. Additionally, test with intervention: patch m2 activations from sarcasm examples into understatement examples and observe if detection behavior transfers.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 6 (Next Steps - Mechanistic Deep Dive, question 4 about generalization) and Section 5 (Analysis - Mechanistic Interpretation)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 8: NUMERICAL REASONING & CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Calculate the percentage of the total write budget contributed by (a) the input embedding, (b) all MLP components, and (c) all attention head components. Show your calculations.\",\n",
    "    \"answer\": \"Total budget: 11,200 dimensions. (a) Input embedding: 768 dims. Percentage = (768/11,200) × 100 = 6.86%. (b) All MLPs: 10 MLPs × 768 dims = 7,680 dims. Percentage = (7,680/11,200) × 100 = 68.57%. (c) All attention heads: 43 heads × 64 dims = 2,752 dims. Percentage = (2,752/11,200) × 100 = 24.57%. Verification: 6.86% + 68.57% + 24.57% = 100%. This shows MLPs dominate the circuit, contributing more than two-thirds of the total budget.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 4 (Results - Circuit Composition) and Section 7 (Main Takeaways - Scientific Insights, point 2)\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"According to the MLP components table, m2 has an average differential activation of 32.47, and the next strongest MLP (m11) has 22.30. Calculate the percentage by which m2 exceeds m11, and explain whether this supports or contradicts the claim that m2 is '~45% stronger' as stated in the documentation.\",\n",
    "    \"answer\": \"Calculation: Percentage difference = ((32.47 - 22.30) / 22.30) × 100 = (10.17 / 22.30) × 100 = 45.6%. This calculation supports the documentation's claim that m2 is '~45% stronger' than m11. The statement refers to m2 being 45% greater than the next strongest MLP in terms of differential activation magnitude. This dramatic difference emphasizes m2's dominant role as the primary sarcasm detector and justifies the revised understanding that sarcasm detection is concentrated in Layer 2 rather than distributed across multiple layers.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 4 (Results - MLP Components table and Key Finding)\"\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY 9: ERROR DETECTION & AMBIGUITY\n",
    "# ============================================================================\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"A student claims: 'The circuit includes all 12 MLP layers from GPT2-small because MLPs are more important than attention for sarcasm detection.' Identify the error in this statement.\",\n",
    "    \"answer\": \"The circuit only includes 10 out of 12 MLPs; m3 and m4 are excluded\",\n",
    "    \"choices\": [\"MLPs are not more important than attention heads for sarcasm detection\",\n",
    "                \"The circuit only includes 10 out of 12 MLPs; m3 and m4 are excluded\",\n",
    "                \"GPT2-small has 13 MLP layers, not 12\",\n",
    "                \"The circuit prioritizes attention heads over MLPs\"],\n",
    "    \"reference\": \"Section 4 (Results - Circuit Composition and Excluded Components: 'MLPs excluded: m3, m4')\"\n",
    "})\n",
    "\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation states that only 5 pairs were 'analyzed in detail' from the 40 total examples. What ambiguity or potential confusion does this create about the experimental method, and what would you need to know to resolve it?\",\n",
    "    \"answer\": \"This creates ambiguity about what 'analyzed in detail' means versus what was done with the other 35 pairs. The differential activation analysis presumably used all 40 examples to compute average activations (otherwise the statistics wouldn't be reliable), but 'analyzed in detail' might refer to manual linguistic analysis, visualization, or case-by-case examination. To resolve this, we would need to know: (1) Were all 40 examples used to compute the differential activation scores? (2) What constitutes 'detailed analysis' - is it qualitative interpretation, attention pattern visualization, or token-level activation inspection? (3) Why were only 5 pairs selected for detailed analysis - were they representative, edge cases, or randomly chosen? This ambiguity is important because it affects how much we can trust the generalizability of the mechanistic interpretations.\",\n",
    "    \"choices\": None,\n",
    "    \"reference\": \"Section 8 (Limitations, point 1: 'Small dataset: Only 5 pairs analyzed in detail (40 examples total available)')\"\n",
    "})\n",
    "\n",
    "print(f\"Total questions created: {len(exam_questions)}\")\n",
    "print(f\"\\nBreakdown by type:\")\n",
    "print(f\"  Multiple choice: {sum(1 for q in exam_questions if q['question_type'] == 'multiple_choice')}\")\n",
    "print(f\"  Free generation: {sum(1 for q in exam_questions if q['question_type'] == 'free_generation')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73a9cb7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created exam directory: /home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/exam\n",
      "\n",
      "Saved exam questions to: /home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/exam/exam_sarcasm_circuit.json\n",
      "Total questions: 23\n",
      "\n",
      "File exists: True\n",
      "File size: 21808 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create the exam directory and save the JSON file\n",
    "import json\n",
    "\n",
    "# Create exam directory\n",
    "exam_dir = repo_path / 'exam'\n",
    "exam_dir.mkdir(exist_ok=True)\n",
    "print(f\"Created exam directory: {exam_dir}\")\n",
    "\n",
    "# Save the JSON file\n",
    "task_name = \"sarcasm_circuit\"\n",
    "json_filename = f\"exam_{task_name}.json\"\n",
    "json_path = exam_dir / json_filename\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(exam_questions, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved exam questions to: {json_path}\")\n",
    "print(f\"Total questions: {len(exam_questions)}\")\n",
    "\n",
    "# Verify the file was created\n",
    "print(f\"\\nFile exists: {json_path.exists()}\")\n",
    "print(f\"File size: {json_path.stat().st_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44914551",
   "metadata": {},
   "source": [
    "# Exam Design for Sarcasm Circuit Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains a comprehensive exam designed to evaluate student understanding of the sarcasm circuit documentation for GPT2-small. The exam covers both factual knowledge and applied reasoning across multiple dimensions.\n",
    "\n",
    "## Exam Statistics\n",
    "\n",
    "- **Total Questions**: 23\n",
    "- **Multiple Choice**: 9 questions (39%)\n",
    "- **Free Generation**: 14 questions (61%)\n",
    "- **Coverage**: All major sections of the documentation\n",
    "\n",
    "## Question Categories\n",
    "\n",
    "The exam is organized into the following categories:\n",
    "\n",
    "1. **Comprehension & Recall** (4 questions) - Basic factual understanding\n",
    "2. **Causal & Mechanistic Reasoning** (3 questions) - Understanding how the system works\n",
    "3. **Hypothetical Transfer & Application** (3 questions) - Applying knowledge to new scenarios\n",
    "4. **Comparative Analysis** (2 questions) - Comparing different approaches\n",
    "5. **Experimental Design & Methodology** (3 questions) - Understanding research methods\n",
    "6. **Critical Reflection & Limitations** (2 questions) - Evaluating study limitations\n",
    "7. **Creative Synthesis & Open Questions** (2 questions) - Proposing new ideas\n",
    "8. **Numerical Reasoning & Calculation** (2 questions) - Quantitative understanding\n",
    "9. **Error Detection & Ambiguity** (2 questions) - Identifying issues in reasoning\n",
    "\n",
    "## Quality Metrics\n",
    "\n",
    "- **Factual Understanding**: ~48% of questions (11/23)\n",
    "- **Reasoning & Application**: ~52% of questions (12/23)\n",
    "- **Non-trivial**: All questions require comprehension beyond simple text lookup\n",
    "- **Documentation-grounded**: All answers derivable from the provided documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155a2125",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EXAM: SARCASM CIRCUIT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 1 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "What is the write budget constraint for the sarcasm detection circuit?\n",
      "\n",
      "Choices:\n",
      "  A) 7,680 dimensions\n",
      "  B) 11,200 dimensions\n",
      "  C) 12,288 dimensions\n",
      "  D) 54,000 dimensions\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "11,200 dimensions\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 1 (Goal) and Section 4 (Results - Circuit Composition)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 2 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "How many total examples were in the sarcasm dataset?\n",
      "\n",
      "Choices:\n",
      "  A) 20 examples (10 sarcastic, 10 literal)\n",
      "  B) 40 examples (20 sarcastic, 20 literal)\n",
      "  C) 100 examples (50 sarcastic, 50 literal)\n",
      "  D) 5 paired examples\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "40 examples (20 sarcastic, 20 literal)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 2 (Data - Dataset Description)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 3 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "Which MLP component showed the highest differential activation and is considered the primary sarcasm detector?\n",
      "\n",
      "Choices:\n",
      "  A) m0 (Layer 0 MLP) with 7.33 average differential activation\n",
      "  B) m2 (Layer 2 MLP) with 32.47 average differential activation\n",
      "  C) m11 (Layer 11 MLP) with 22.30 average differential activation\n",
      "  D) m10 (Layer 10 MLP) with 17.36 average differential activation\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "m2 (Layer 2 MLP) with 32.47 average differential activation\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - MLP Components table and Key Finding)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 4 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "List the three key linguistic features that distinguish sarcastic sentences from literal ones according to the documentation.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1) Discourse markers (e.g., 'Oh', 'Wow', 'Just'), 2) Positive sentiment words (e.g., 'great', 'love', 'fantastic'), 3) Negative situational context (e.g., 'stuck in traffic', 'crashed'), with the key pattern being the contradiction between positive words and negative situations.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 2 (Data - Key Linguistic Features of Sarcasm)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 5 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The documentation states that m3 and m4 were excluded from the circuit. If you were to add m3 back into the circuit, how would this affect the write budget, and what would you need to adjust to stay within the 11,200 dimension limit?\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Adding m3 would add 768 dimensions to the circuit, bringing the total from 11,200 to 11,968 dimensions, which exceeds the budget by 768 dimensions. To stay within the limit, you would need to remove either one MLP (768 dims) or 12 attention heads (12 × 64 = 768 dims) from the current circuit. Since the circuit currently uses exactly 11,200 dimensions (100% budget), any addition requires corresponding removal.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 3 (Method - Technical Details - Write Budget Calculation) and Section 4 (Results - Excluded Components)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 6 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "According to the differential activation analysis method, what does a higher L2 norm difference between mean activations indicate?\n",
      "\n",
      "Choices:\n",
      "  A) Better general language modeling capability\n",
      "  B) Stronger sarcasm-specific processing by that component\n",
      "  C) Higher computational cost for that component\n",
      "  D) Greater need for that component to be excluded\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Stronger sarcasm-specific processing by that component\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 3 (Method - Step 2: Differential Analysis)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 7 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The initial hypothesis suggested that middle layers detect incongruity, but the empirical evidence showed otherwise. Explain what the middle layers (L3-L7) actually do according to the revised understanding, and why this differs from the initial hypothesis.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "According to the revised understanding, middle layers (L3-L7) propagate and refine the sarcasm signal rather than detect incongruity. The primary detection happens much earlier at Layer 2 (m2). The middle layers use 19 attention heads to route information across sequence positions and enable context-aware processing. This differs from the initial hypothesis because detection occurs earlier than expected (L2 rather than middle layers), and the middle layers serve a supporting propagation role rather than the primary detection role.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 5 (Analysis - Hypothesis Evolution and Mechanistic Interpretation - Stage 2)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 8 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "Suppose you want to build a similar circuit for detecting irony (another form of figurative language) in GPT2-small. Based on the sarcasm circuit findings, which layer would you hypothesize as most important for irony detection, and what experimental approach would you use to test this?\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Based on the sarcasm circuit findings showing early detection at Layer 2, I would hypothesize that irony detection also occurs in early layers (L1-L3), particularly focusing on Layer 2 MLPs. To test this, I would use the same differential activation analysis approach: (1) collect activations on paired ironic/literal examples, (2) compute L2 norm differences for each component, (3) identify which components show the highest differential activation. If the hypothesis holds, we should see dominant differential activation in early-layer MLPs similar to m2's role in sarcasm. This could reveal whether figurative language detection generally occurs early in transformer processing.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 5 (Analysis - Mechanistic Interpretation - Stage 1) and Section 6 (Next Steps - Open Questions about generalization to other figurative language)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 9 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The circuit uses 10 MLPs (7,680 dims) and 43 attention heads (2,752 dims). If you were redesigning the circuit with a smaller budget of 5,600 dimensions (half the original), describe a principled strategy for selecting which components to keep, and justify your choices based on the documented findings.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "A principled strategy would prioritize components with highest differential activation: (1) Keep the input embedding (768 dims - essential). (2) Keep m2 (768 dims - dramatically dominant at 32.47, the primary detector). (3) Keep m11 (768 dims - second highest at 22.30, handles final pre-output processing). (4) Keep m10 and m9 (1,536 dims - late-stage integration at 17.36 and 13.41). This totals 3,840 dims for MLPs. (5) For remaining 1,760 dims, keep the top ~27 attention heads (starting with a11.h8, a11.h0, etc.) based on their differential activation rankings. This strategy preserves the three-stage hierarchy: early detection (m2), propagation (some mid-layer heads), and final integration (m10, m11, and late-layer heads), while maximizing the inclusion of high-differential components.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - MLP Components and Attention Head Components tables) and Section 5 (Analysis - Mechanistic Interpretation)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 10 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "If you applied the sarcasm circuit to a sentence with ambiguous intent like 'That was interesting', what would be the most likely reason for circuit failure based on the documented linguistic features?\n",
      "\n",
      "Choices:\n",
      "  A) The sentence is too short for the circuit to process\n",
      "  B) The sentence lacks discourse markers like 'Oh' or 'Wow'\n",
      "  C) The sentence lacks clear contradiction between positive sentiment words and negative situational context\n",
      "  D) The sentence would activate m3 and m4 which are excluded from the circuit\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "The sentence lacks clear contradiction between positive sentiment words and negative situational context\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 2 (Data - Key Linguistic Features of Sarcasm, specifically the Contradiction feature)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 11 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "Compare the sarcasm circuit to the IOI (Indirect Object Identification) circuit along three dimensions: primary mechanism, circuit size, and key layer. What does this comparison suggest about how different linguistic tasks are processed in transformers?\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Sarcasm circuit: (1) Primary mechanism is MLP-based incongruity detection, (2) Dense circuit with 54 components, (3) Key layer is early (Layer 2). IOI circuit: (1) Primary mechanism is attention-based name copying, (2) Sparse circuit with ~10 components, (3) Key layers are late (9-11). This comparison suggests that different linguistic tasks use fundamentally different computational strategies within the same architecture. Tasks requiring pattern detection and semantic contradiction (sarcasm) rely more on MLPs and early processing, while tasks requiring information routing and copying (IOI) rely more on attention and late processing. This demonstrates task-specific architectural utilization in transformers.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 5 (Analysis - Comparison to IOI Circuit table and concluding statement)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 12 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "In the three-stage mechanistic interpretation, which stage has the most attention heads involved?\n",
      "\n",
      "Choices:\n",
      "  A) Stage 1 (Early Detection, L0-L2) with 9 attention heads\n",
      "  B) Stage 2 (Distributed Propagation, L3-L7) with 19 attention heads\n",
      "  C) Stage 3 (Final Integration, L8-L11) with 15 attention heads\n",
      "  D) All stages have equal attention head distribution\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Stage 2 (Distributed Propagation, L3-L7) with 19 attention heads\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - Attention Head Components - Distribution by Layer) and Section 5 (Analysis - Mechanistic Interpretation - Stage 2)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 13 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The documentation states that 'differential activation ≠ causal importance' as a limitation. Describe two validation experiments from the 'Next Steps' section that would help establish causal importance, and explain how each addresses this limitation.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Two validation experiments: (1) Ablation testing - systematically remove components and measure the impact on sarcasm detection performance. This establishes causality by showing whether removing a component degrades the behavior, proving it's not just correlated but actually necessary. (2) Intervention experiments - patch activations to test causality by modifying specific component activations and observing downstream effects. This establishes causality by demonstrating that manipulating a component's output directly changes sarcasm detection behavior. Both methods go beyond correlation (differential activation) to demonstrate that components are causally responsible for the observed behavior.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 6 (Next Steps - Validation Experiments) and Section 8 (Limitations - point 3)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 14 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "Why were activations averaged over sequence positions during the analysis?\n",
      "\n",
      "Choices:\n",
      "  A) To reduce computational cost\n",
      "  B) To handle variable-length inputs\n",
      "  C) To emphasize the importance of early tokens\n",
      "  D) To make the circuit more interpretable\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "To handle variable-length inputs\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 3 (Method - Technical Details - Normalization)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 15 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "In the component selection step (Step 3), the method prioritized MLPs over attention heads. Given that each MLP contributes 768 dimensions versus 64 dimensions per attention head, calculate how many attention heads would be equivalent to adding one MLP in terms of write budget. Then explain why prioritizing MLPs makes sense given the budget constraint.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "One MLP (768 dims) is equivalent to 12 attention heads (12 × 64 = 768 dims) in terms of write budget. Prioritizing MLPs makes sense because: (1) You can test fewer components to fill the budget (testing 10 MLPs vs. 120 attention heads for the same dimensions), making component selection more efficient. (2) If MLPs show high differential activation, you get more 'bang for your buck' - a single high-performing MLP provides more signal than you'd get from 12 lower-performing attention heads. (3) The method ranks by differential activation first, so if top-ranked components are MLPs, including them maximizes the quality of components within budget constraints.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 3 (Method - Step 3: Component Selection and Technical Details - Write Budget Calculation)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 16 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The documentation lists 'Budget maximization' as a limitation, noting that the minimal circuit is likely smaller than 54 components. Explain why using the full 11,200 dimension budget might not represent the minimal sufficient circuit, and what tradeoff this represents.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Using the full budget maximizes coverage but likely includes redundant or marginally important components. The circuit selection method ranked components by differential activation and included all components that fit within budget, but lower-ranked components may contribute minimally to actual sarcasm detection. The minimal sufficient circuit would only include components that are necessary and sufficient for the behavior, which could be significantly smaller. The tradeoff is between completeness (capturing all potentially relevant components) and parsimony (finding the simplest explanation). Budget maximization prioritizes completeness but at the cost of including potentially unnecessary components, making the circuit harder to interpret and validate.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 8 (Limitations - point 5: 'Budget maximization: Used full 11,200 dims; minimal circuit likely smaller')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 17 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "Which of the following is NOT listed as a limitation of this study?\n",
      "\n",
      "Choices:\n",
      "  A) Only 5 pairs were analyzed in detail from the 40 examples available\n",
      "  B) The dataset consists of synthetic rather than real-world sarcasm\n",
      "  C) No causal validation (such as ablation tests) was performed\n",
      "  D) The model architecture (GPT2-small) is too small for sarcasm detection\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "The model architecture (GPT2-small) is too small for sarcasm detection\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 8 (Limitations) - lists small dataset, synthetic data, no causal validation, and single model specificity, but does not claim the model is too small\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 18 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The documentation poses an open question: 'Why is m2 so dominant? What about Layer 2 enables incongruity detection?' Based on your understanding of transformer architecture and the three-stage processing model, propose a hypothesis that could explain m2's dramatic dominance (45% stronger than the next strongest MLP).\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Possible hypothesis: Layer 2 may be positioned at a 'sweet spot' in the network where (1) sufficient low-level features have been extracted by Layer 0 and 1 (basic token embeddings and local context) to identify linguistic patterns, but (2) the representations are not yet too abstract or task-specific as in later layers. Incongruity detection requires identifying the co-occurrence of contradictory elements (positive sentiment words + negative context), which may require some compositional understanding but not deep semantic reasoning. Layer 2 might specialize in detecting these mid-level semantic patterns. Alternatively, GPT2's training may have naturally allocated Layer 2 for pragmatic/non-literal language processing tasks, separating them from literal semantic processing in other layers. The 45% dominance suggests this is not just a gradual process but a discrete computational step concentrated in this specific layer.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 6 (Next Steps - Open Questions, question 1) and Section 5 (Analysis - Mechanistic Interpretation)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 19 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "Design a follow-up experiment to test whether the sarcasm circuit generalizes to other forms of figurative language such as understatement (e.g., 'It's just a scratch' for a serious injury). Describe your experimental setup, what data you would collect, and what results would support or refute generalization.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Experimental setup: (1) Create a dataset of 40 paired examples (20 understatement, 20 literal) with similar structure to the sarcasm data. Example understatement: 'It's a bit chilly' (during a blizzard). Example literal: 'It's extremely cold outside.' (2) Run GPT2-small on this dataset using the identified 54-component sarcasm circuit (same hooks/activations). (3) Measure differential activation between understatement and literal examples for the same 54 components. Results supporting generalization: If m2 shows dominant differential activation for understatement (>20 avg diff) and the three-stage pattern is preserved, this suggests the circuit detects general pragmatic contradiction, not sarcasm-specific patterns. Results refuting generalization: If different components (e.g., m5, m7) dominate, or if late layers become more important, this suggests task-specific circuits. Additionally, test with intervention: patch m2 activations from sarcasm examples into understatement examples and observe if detection behavior transfers.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 6 (Next Steps - Mechanistic Deep Dive, question 4 about generalization) and Section 5 (Analysis - Mechanistic Interpretation)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 20 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "Calculate the percentage of the total write budget contributed by (a) the input embedding, (b) all MLP components, and (c) all attention head components. Show your calculations.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Total budget: 11,200 dimensions. (a) Input embedding: 768 dims. Percentage = (768/11,200) × 100 = 6.86%. (b) All MLPs: 10 MLPs × 768 dims = 7,680 dims. Percentage = (7,680/11,200) × 100 = 68.57%. (c) All attention heads: 43 heads × 64 dims = 2,752 dims. Percentage = (2,752/11,200) × 100 = 24.57%. Verification: 6.86% + 68.57% + 24.57% = 100%. This shows MLPs dominate the circuit, contributing more than two-thirds of the total budget.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - Circuit Composition) and Section 7 (Main Takeaways - Scientific Insights, point 2)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 21 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "According to the MLP components table, m2 has an average differential activation of 32.47, and the next strongest MLP (m11) has 22.30. Calculate the percentage by which m2 exceeds m11, and explain whether this supports or contradicts the claim that m2 is '~45% stronger' as stated in the documentation.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculation: Percentage difference = ((32.47 - 22.30) / 22.30) × 100 = (10.17 / 22.30) × 100 = 45.6%. This calculation supports the documentation's claim that m2 is '~45% stronger' than m11. The statement refers to m2 being 45% greater than the next strongest MLP in terms of differential activation magnitude. This dramatic difference emphasizes m2's dominant role as the primary sarcasm detector and justifies the revised understanding that sarcasm detection is concentrated in Layer 2 rather than distributed across multiple layers.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - MLP Components table and Key Finding)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 22 [MULTIPLE_CHOICE]\n",
      "================================================================================\n",
      "\n",
      "A student claims: 'The circuit includes all 12 MLP layers from GPT2-small because MLPs are more important than attention for sarcasm detection.' Identify the error in this statement.\n",
      "\n",
      "Choices:\n",
      "  A) MLPs are not more important than attention heads for sarcasm detection\n",
      "  B) The circuit only includes 10 out of 12 MLPs; m3 and m4 are excluded\n",
      "  C) GPT2-small has 13 MLP layers, not 12\n",
      "  D) The circuit prioritizes attention heads over MLPs\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "The circuit only includes 10 out of 12 MLPs; m3 and m4 are excluded\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 4 (Results - Circuit Composition and Excluded Components: 'MLPs excluded: m3, m4')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION 23 [FREE_GENERATION]\n",
      "================================================================================\n",
      "\n",
      "The documentation states that only 5 pairs were 'analyzed in detail' from the 40 total examples. What ambiguity or potential confusion does this create about the experimental method, and what would you need to know to resolve it?\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This creates ambiguity about what 'analyzed in detail' means versus what was done with the other 35 pairs. The differential activation analysis presumably used all 40 examples to compute average activations (otherwise the statistics wouldn't be reliable), but 'analyzed in detail' might refer to manual linguistic analysis, visualization, or case-by-case examination. To resolve this, we would need to know: (1) Were all 40 examples used to compute the differential activation scores? (2) What constitutes 'detailed analysis' - is it qualitative interpretation, attention pattern visualization, or token-level activation inspection? (3) Why were only 5 pairs selected for detailed analysis - were they representative, edge cases, or randomly chosen? This ambiguity is important because it affects how much we can trust the generalizability of the mechanistic interpretations.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "REFERENCE:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Section 8 (Limitations, point 1: 'Small dataset: Only 5 pairs analyzed in detail (40 examples total available)')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "END OF EXAM - Total: 23 questions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display all exam questions with their answers in a structured format\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE EXAM: SARCASM CIRCUIT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for i, q in enumerate(exam_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUESTION {i} [{q['question_type'].upper()}]\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{q['question']}\")\n",
    "    \n",
    "    if q['choices']:\n",
    "        print(\"\\nChoices:\")\n",
    "        for j, choice in enumerate(q['choices'], 1):\n",
    "            print(f\"  {chr(64+j)}) {choice}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"ANSWER:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"{q['answer']}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"REFERENCE:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"{q['reference']}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"END OF EXAM - Total: {len(exam_questions)} questions\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ca3c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exam Quality Assessment\n",
    "\n",
    "### Coverage Analysis\n",
    "\n",
    "This exam comprehensively covers all major sections of the documentation:\n",
    "\n",
    "- **Section 1 (Goal)**: Questions 1, 5\n",
    "- **Section 2 (Data)**: Questions 2, 4, 10\n",
    "- **Section 3 (Method)**: Questions 5, 6, 14, 15\n",
    "- **Section 4 (Results)**: Questions 1, 3, 5, 9, 12, 20, 21, 22\n",
    "- **Section 5 (Analysis)**: Questions 7, 8, 9, 11, 18, 19\n",
    "- **Section 6 (Next Steps)**: Questions 8, 13, 18, 19\n",
    "- **Section 7 (Main Takeaways)**: Questions 11, 20\n",
    "- **Section 8 (Limitations)**: Questions 13, 16, 17, 23\n",
    "\n",
    "### Question Difficulty Distribution\n",
    "\n",
    "- **Easy** (Direct recall): Questions 1, 2, 3, 4, 6, 14, 17, 22\n",
    "- **Medium** (Application & reasoning): Questions 5, 7, 10, 11, 12, 13, 15, 20, 21\n",
    "- **Hard** (Synthesis & creative reasoning): Questions 8, 9, 16, 18, 19, 23\n",
    "\n",
    "### Learning Objectives Assessed\n",
    "\n",
    "1. **Factual Knowledge**: Students can recall key facts about the circuit, methodology, and results\n",
    "2. **Mechanistic Understanding**: Students understand how the three-stage processing works\n",
    "3. **Quantitative Reasoning**: Students can perform calculations with write budget and percentages\n",
    "4. **Critical Thinking**: Students can identify limitations and ambiguities\n",
    "5. **Transfer & Application**: Students can apply knowledge to novel scenarios\n",
    "6. **Experimental Design**: Students understand research methodology and validation approaches\n",
    "7. **Comparative Analysis**: Students can compare different approaches and circuits\n",
    "8. **Creative Synthesis**: Students can propose new hypotheses and experiments\n",
    "\n",
    "### Answer Validation\n",
    "\n",
    "All answers are:\n",
    "- Grounded in the documentation\n",
    "- Unambiguous and verifiable\n",
    "- At an appropriate level of detail\n",
    "- Referenced to specific sections\n",
    "\n",
    "### Exam Integrity\n",
    "\n",
    "- Questions require genuine comprehension, not text search\n",
    "- No questions are answerable by trivial pattern matching\n",
    "- Mix of closed-ended (multiple choice) and open-ended (free generation) formats\n",
    "- Balance between factual recall (48%) and applied reasoning (52%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162f6820",
   "metadata": {
    "execution_status": "pending"
   },
   "outputs": [],
   "source": [
    "# Save the notebook to the exam directory\n",
    "import shutil\n",
    "\n",
    "# Get the notebook path\n",
    "notebook_path = Path('/home/smallyan/critic_model_mechinterp/notebooks/2025-11-10-21-18_ExamDesign.ipynb')\n",
    "print(f\"Source notebook: {notebook_path}\")\n",
    "print(f\"Notebook exists: {notebook_path.exists()}\")\n",
    "\n",
    "# Copy to exam directory with new name\n",
    "target_path = exam_dir / 'exam_documentation.ipynb'\n",
    "shutil.copy(notebook_path, target_path)\n",
    "\n",
    "print(f\"\\nCopied notebook to: {target_path}\")\n",
    "print(f\"Target exists: {target_path.exists()}\")\n",
    "\n",
    "# Verify both files exist in exam directory\n",
    "print(f\"\\nExam directory contents:\")\n",
    "for item in sorted(exam_dir.iterdir()):\n",
    "    print(f\"  - {item.name} ({item.stat().st_size} bytes)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-10-21-18_ExamDesign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
