[
  {
    "question_id": 1,
    "question_type": "multiple_choice",
    "question": "What is the primary computational mechanism used by GPT2-small for sarcasm detection, according to the documented circuit?",
    "gold_answer": "C",
    "student_answer": "C",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 5 (Analysis) - Mechanistic Interpretation; Section 7 (Main Takeaways) - 'MLPs dominate' and 'Sarcasm detection is early'",
    "external_reference": false
  },
  {
    "question_id": 2,
    "question_type": "multiple_choice",
    "question": "Which component shows the most dominant differential activation in the sarcasm circuit, and what is its approximate differential activation value?",
    "gold_answer": "B",
    "student_answer": "B",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 4 (Results) - MLP Components table; Section 5 (Analysis) - 'Layer 2 MLP (m2) is primary detector'",
    "external_reference": false
  },
  {
    "question_id": 3,
    "question_type": "multiple_choice",
    "question": "What is the total write budget utilized by the documented sarcasm circuit?",
    "gold_answer": "D",
    "student_answer": "D",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 4 (Results) - Circuit Composition: 'Total write cost: 11,200 / 11,200 (100%)'",
    "external_reference": false
  },
  {
    "question_id": 4,
    "question_type": "free_generation",
    "question": "The documentation describes a three-stage hierarchical process for sarcasm detection. Describe each stage, identify the key components involved, and explain the computational function performed at each stage.",
    "gold_answer": "The three-stage process is:\n\n**Stage 1: Early Detection (L0-L2)** - The primary component is m2 (Layer 2 MLP), supported by m0 and m1. Function: Detect incongruity between sentiment words and contextual markers by identifying mismatch patterns like positive words combined with negative situations.\n\n**Stage 2: Signal Propagation and Refinement (L3-L7)** - Key components include m5, m6, m7 MLPs and 19 attention heads in layers 4-7. Function: Propagate and refine the sarcasm signal detected by m2, distributing information across sequence positions for context-aware processing.\n\n**Stage 3: Final Integration (L8-L11)** - Critical components are m8, m9, m10, m11 MLPs and Layer 11 attention heads (especially a11.h8 and a11.h0). Function: Perform final processing and integration of the sarcasm signal into the output representation, determining how sarcasm affects final token predictions.",
    "student_answer": "The three-stage hierarchical process consists of: (1) Early Detection at layers 0-2, primarily through m2 MLP detecting incongruity between sentiment words and negative context; (2) Distributed Propagation at layers 3-7, where mid-layer MLPs refine the signal and 19 attention heads route information across positions; (3) Final Integration at layers 8-11, where late MLPs (especially m11) perform final processing and Layer 11 attention heads integrate the sarcasm signal into output predictions.",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Section 5 (Analysis) - Mechanistic Interpretation (all three stages); Plan v2 - Revised Mechanistic Model",
    "external_reference": false
  },
  {
    "question_id": 5,
    "question_type": "multiple_choice",
    "question": "Which two MLP layers were excluded from the circuit due to minimal differential activation?",
    "gold_answer": "B",
    "student_answer": "B",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 4 (Results) - Excluded Components: 'MLPs excluded: m3, m4 - Showed minimal differential activation (<6.5)'",
    "external_reference": false
  },
  {
    "question_id": 6,
    "question_type": "free_generation",
    "question": "Compare the sarcasm circuit to the Indirect Object Identification (IOI) circuit along four dimensions: primary mechanism, key layer, circuit size, and relative importance of attention vs. MLPs. What does this comparison suggest about linguistic task processing in transformers?",
    "gold_answer": "The comparison reveals:\n\n1. **Primary mechanism**: IOI uses name copying via attention; sarcasm uses incongruity detection via MLP\n2. **Key layer**: IOI relies on later layers (9-11); sarcasm relies on early layer (2)\n3. **Circuit size**: IOI is sparse (~10 components); sarcasm is dense (54 components)\n4. **Attention vs. MLPs**: IOI has dominant attention with supporting MLPs; sarcasm has dominant MLPs with supporting attention\n\nThis suggests that different linguistic tasks use fundamentally different computational strategies within the same transformer architecture. Simple reference-based tasks may use attention-heavy circuits, while complex semantic/pragmatic tasks requiring pattern recognition may rely more heavily on MLPs.",
    "student_answer": "The comparison shows: (1) Primary mechanism - IOI uses name copying via attention, Sarcasm uses incongruity detection via MLP; (2) Key layer - IOI operates in later layers (9-11), Sarcasm in early layer 2; (3) Circuit size - IOI is sparse (~10 components), Sarcasm is dense (54 components); (4) Attention vs MLP - IOI is attention-dominant with supporting MLPs, Sarcasm is MLP-dominant with supporting attention. This suggests that different linguistic tasks use fundamentally different computational strategies within the same transformer architecture.",
    "score": 4.0,
    "feedback": "Good answer with most key concepts present.",
    "reference": "Section 5 (Analysis) - Comparison to IOI Circuit table",
    "external_reference": false
  },
  {
    "question_id": 7,
    "question_type": "multiple_choice",
    "question": "What method was used to identify components causally important for sarcasm detection?",
    "gold_answer": "C",
    "student_answer": "C",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 3 (Method) - Experimental Approach: 'We used differential activation analysis'",
    "external_reference": false
  },
  {
    "question_id": 8,
    "question_type": "free_generation",
    "question": "Explain the key linguistic features that distinguish sarcastic from literal sentences in the dataset. How might these features enable Layer 2 MLP to detect incongruity?",
    "gold_answer": "Key linguistic features of sarcasm in the dataset:\n\n1. **Discourse markers**: Emphasis particles like 'Oh', 'Wow', 'Just' that signal pragmatic meaning\n2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\n3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\n4. **Contradiction**: Positive words describing objectively negative situations\n\nLayer 2 MLP likely detects incongruity by:\n- Processing co-occurrence patterns of positive sentiment words with negative contexts\n- Recognizing discourse markers that prime for non-literal interpretation\n- Computing a mismatch signal when sentiment polarity conflicts with situational valence\n- This happens early because the pattern (positive adjective + negative situation) can be detected from local context without deep semantic integration",
    "student_answer": "The key linguistic features are: (1) Discourse markers like \"Oh\", \"Wow\", \"Just\" that emphasize; (2) Positive sentiment words like \"great\", \"love\", \"fantastic\"; (3) Negative situational context like \"stuck in traffic\", \"crashed\"; (4) Contradiction where positive words describe objectively negative situations. Layer 2 MLP can detect incongruity by recognizing these contradictory patterns - specifically when positive adjectives appear with negative situational contexts, creating the sentiment-context mismatch characteristic of sarcasm.",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Section 2 (Data) - Key Linguistic Features of Sarcasm; Section 5 (Analysis) - Stage 1: Early Detection mechanism",
    "external_reference": false
  },
  {
    "question_id": 9,
    "question_type": "multiple_choice",
    "question": "How many attention heads were included in the final circuit, and which layer contains the most important attention heads?",
    "gold_answer": "A",
    "student_answer": "A",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 4 (Results) - Circuit Composition ('Attention heads: 43') and Top 10 Most Important Heads table (a11.h8 and a11.h0 are top-ranked)",
    "external_reference": false
  },
  {
    "question_id": 10,
    "question_type": "free_generation",
    "question": "The original hypothesis predicted that middle layers (L4-L7) would be the primary detection site, but empirical evidence showed Layer 2 as the primary detector. Explain this discrepancy and what it reveals about the mechanistic difference between the predicted and actual sarcasm processing.",
    "gold_answer": "The discrepancy arose because:\n\n**Original hypothesis**: Based on linguistic intuition that incongruity detection would require substantial semantic processing, placing it in middle layers after sentiment encoding is complete.\n\n**Empirical finding**: m2 (Layer 2) showed dramatically dominant differential activation (32.47), indicating sarcasm detection happens much earlier.\n\n**Mechanistic difference**:\n- **Predicted**: Sequential process where early layers encode sentiment, middle layers detect incongruity, late layers reverse meaning\n- **Actual**: Early layers detect incongruity directly from surface patterns (positive words + negative context), middle layers propagate/refine the signal, late layers integrate (not reverse) the signal into output\n\nThis reveals that sarcasm detection in GPT2-small relies on pattern matching of co-occurrence features rather than deep semantic reasoning. The network doesn't build up complex representations and then detect contradiction\u2014it recognizes the sarcastic pattern directly from relatively shallow features.",
    "student_answer": "The initial hypothesis predicted middle layers (L4-L7) would detect incongruity, with early layers encoding sentiment and late layers reversing meaning. However, empirical evidence showed Layer 2 MLP (m2) as the primary detector - much earlier than expected. The mechanistic difference is that: (1) detection happens immediately at Layer 2, not gradually in middle layers; (2) middle layers propagate and refine the already-detected sarcasm signal rather than performing detection; (3) late layers integrate the signal into output rather than reversing sentiment polarity. This reveals sarcasm detection is an early, decisive process rather than a gradual accumulation across layers.",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Plan v2 - Original vs. Observed Mechanism section; Section 5 (Analysis) - Hypothesis Evolution",
    "external_reference": false
  },
  {
    "question_id": 11,
    "question_type": "multiple_choice",
    "question": "In the differential activation analysis method, activations were averaged over which dimension to handle variable-length inputs?",
    "gold_answer": "B",
    "student_answer": "B",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 3 (Method) - Technical Details: 'Normalization: Averaged activations over sequence positions'; Code walkthrough - measure_activation_difference_normalized function",
    "external_reference": false
  },
  {
    "question_id": 12,
    "question_type": "free_generation",
    "question": "The circuit uses 10 MLPs (7,680 dims) versus 43 attention heads (2,752 dims). Given the budget-constrained selection algorithm described in the documentation, explain why this distribution occurred and what it implies about the relative importance of MLPs vs. attention for sarcasm detection.",
    "gold_answer": "This distribution occurred because of the greedy budget-constrained selection algorithm:\n\n1. **Selection strategy**: The algorithm first added high-differential MLPs (threshold ~7.0), then filled remaining budget with attention heads ranked by importance\n\n2. **Why this distribution**:\n   - MLPs showed much higher differential activation than attention heads (m2: 32.47 vs. top head a11.h8: 3.33)\n   - Each MLP costs 768 dims but provides higher signal; attention heads cost only 64 dims but contribute less individually\n   - The algorithm maximized impact by prioritizing high-differential MLPs first\n\n3. **Implications**:\n   - MLPs are more specialized for sarcasm detection (pattern recognition)\n   - Attention heads play a supporting role (information routing/integration)\n   - Sarcasm is primarily a pattern-matching task rather than a reference/copying task\n   - This contrasts with circuits like IOI where attention dominates, showing task-specific architectural preferences",
    "student_answer": "The distribution occurred because components were ranked by differential activation and selected top-down within the budget constraint. MLPs showed much higher differential activation (m2 at 32.47, top heads only ~3.33), so they were prioritized despite costing more dimensions (768 vs 64 each). This implies MLPs are fundamentally more important than attention for sarcasm detection - they perform the core incongruity detection computation, while attention heads play a supporting role in information routing and integration. The 2.8:1 dimension ratio (7,680:2,752) reflects this computational division of labor.",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Section 4 (Results) - Circuit Composition; Code walkthrough - Budget-Constrained Selection algorithm; Section 7 - 'MLPs dominate'",
    "external_reference": false
  },
  {
    "question_id": 13,
    "question_type": "multiple_choice",
    "question": "What is a key limitation of using differential activation (L2 norm of activation differences) as the selection criterion for circuit components?",
    "gold_answer": "D",
    "student_answer": "D",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 8 (Limitations) - 'No causal validation: Differential activation \u2260 causal importance'; Plan v2 - Limitations section",
    "external_reference": false
  },
  {
    "question_id": 14,
    "question_type": "free_generation",
    "question": "Based on the documented circuit structure and the exclusion of m3 and m4, propose a hypothesis for why these specific middle layers might show minimal differential activation. What experiments would you conduct to test this hypothesis?",
    "gold_answer": "**Hypothesis**: m3 and m4 may be bypassed in sarcasm processing because:\n\n1. **Early detection is sufficient**: m2 already detects the sarcasm pattern, so layers 3-4 don't need additional specialized processing\n2. **Residual stream bypass**: The sarcasm signal from m2 may propagate through the residual stream, bypassing m3/m4 MLPs\n3. **Task specialization**: m3/m4 may specialize in other linguistic tasks (e.g., syntax, factual knowledge) orthogonal to sarcasm\n4. **Architectural bottleneck**: These layers may serve as information compression points that preserve the sarcasm signal without actively processing it\n\n**Experiments to test**:\n\n1. **Targeted ablation**: Zero out m3 and m4 individually and jointly; measure impact on sarcasm detection accuracy. If hypothesis is correct, impact should be minimal.\n\n2. **Activation patching**: Replace m3/m4 activations with activations from neutral (non-sarcastic) examples; measure if sarcasm signal is still transmitted to later layers.\n\n3. **Residual stream analysis**: Track the sarcasm signal through the residual stream at each layer. Check if m2's contribution persists unchanged through layers 3-4.\n\n4. **Cross-task probing**: Train linear probes for multiple tasks at each layer; check if m3/m4 specialize in non-sarcasm tasks.\n\n5. **Attention pattern analysis**: Examine if attention heads in layers 3-4 route around these MLPs by attending to earlier residual stream states.",
    "student_answer": "Hypothesis: m3 and m4 perform general language modeling functions unrelated to sarcasm detection, and the sarcasm signal detected by m2 propagates through residual connections directly to later layers (m5-m7) without requiring transformation by m3/m4. This could represent a computational bypass where task-specific information routes around generic processing layers.\n\nExperiments to test: (1) Ablation - remove m3 and m4 and measure impact on sarcasm detection (should be minimal if bypassed); (2) Activation patching - replace m3/m4 activations with activations from literal examples during sarcastic input processing to test if they carry sarcasm information; (3) Residual stream analysis - track sarcasm signal through residual connections to see if it bypasses m3/m4; (4) General capability testing - verify m3/m4 are important for non-sarcasm tasks to confirm they serve other linguistic functions.",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Section 4 (Results) - Excluded Components; Section 6 (Next Steps) - Open Questions ('Are m3 and m4 intentionally bypassed')",
    "external_reference": false
  },
  {
    "question_id": 15,
    "question_type": "multiple_choice",
    "question": "According to the documentation, what is the dimension of each attention head's output in GPT2-small?",
    "gold_answer": "C",
    "student_answer": "C",
    "score": 1.0,
    "feedback": "Correct answer.",
    "reference": "Section 3 (Method) - Technical Details: 'd_head = 64'",
    "external_reference": false
  },
  {
    "question_id": 16,
    "question_type": "free_generation",
    "question": "The documentation mentions that the circuit hasn't been validated with ablation testing. Design a systematic ablation experiment to test the sufficiency and necessity of the identified circuit components. Your design should address both individual component importance and potential interaction effects.",
    "gold_answer": "**Systematic Ablation Experiment Design**:\n\n**Phase 1: Individual Component Ablation (Necessity Testing)**\n\n1. **Setup**: Create test set of 100+ paired sarcastic/literal examples with ground truth labels\n2. **Baseline**: Measure full model accuracy on sarcasm classification\n3. **Single ablation**: For each of the 54 components:\n   - Zero out or randomly replace the component's output\n   - Measure classification accuracy\n   - Compute importance score: baseline_acc - ablated_acc\n4. **Result**: Rank components by causal importance (not just differential activation)\n\n**Phase 2: Hierarchical Ablation (Stage Testing)**\n\n1. Ablate entire stages:\n   - Remove all Early Detection components (m0, m1, m2)\n   - Remove all Signal Propagation components (m5-m7, L4-L7 heads)\n   - Remove all Final Integration components (m8-m11, L11 heads)\n2. Measure which stage has the largest impact (tests necessity of three-stage model)\n\n**Phase 3: Complementary Circuit Test (Sufficiency)**\n\n1. Keep only the 54 identified components, ablate everything else\n2. Measure if circuit alone is sufficient for sarcasm detection\n3. Compare performance to full model\n\n**Phase 4: Interaction Testing**\n\n1. **Pairwise interactions**: Test critical pairs (e.g., m2 + m11, m2 + a11.h8)\n   - Ablate both together; compare to sum of individual ablations\n   - If joint_impact > sum(individual_impacts), components interact synergistically\n2. **Progressive ablation**: Remove components in order of importance; find minimal circuit\n\n**Phase 5: Causal Intervention**\n\n1. **Activation patching**: Replace component activations with those from literal examples when processing sarcastic examples\n2. If m2 is truly causal, patching its activation should flip predictions\n\n**Metrics**:\n- Accuracy on binary sarcasm classification\n- Logit difference between sarcastic and literal interpretations\n- Effect size of each ablation (Cohen's d)\n\n**Controls**:\n- Test on held-out examples\n- Compare to random ablation baseline\n- Verify ablation doesn't just introduce noise",
    "student_answer": "**Experiment Design:**\n\n**Phase 1 - Sufficiency Testing:**\n- Ablate all components EXCEPT the identified 54-component circuit\n- Measure sarcasm detection accuracy on the 40-example dataset\n- If circuit is sufficient, accuracy should remain high (>80% of full model)\n\n**Phase 2 - Individual Necessity Testing:**\n- For each component (54 total), create an ablated version with that single component removed\n- Measure accuracy drop compared to full circuit\n- Rank components by performance impact (validates differential activation ranking)\n- Special focus on m2 (should show largest drop if truly primary detector)\n\n**Phase 3 - Hierarchical Ablation:**\n- Stage-wise removal: Remove all Stage 1 (L0-L2), Stage 2 (L3-L7), or Stage 3 (L8-L11) components\n- Test if all three stages are necessary or if some can be bypassed\n- Validates the three-stage hierarchical model\n\n**Phase 4 - Interaction Testing:**\n- Test critical pairs: m2 + m11, m2 + Layer 11 attention heads, m2 alone\n- Compare m2-only performance to full circuit to quantify distributed vs. localized processing\n- Test if middle-layer propagation (m5-m7 + attention) is necessary or if m2\u2192m11 direct path suffices\n\n**Metrics:**\n- Binary sarcasm classification accuracy\n- Logit difference between sarcasm/literal predictions\n- Activation similarity to full model at each layer",
    "score": 3.0,
    "feedback": "Partial answer with some key concepts present.",
    "reference": "Section 6 (Next Steps) - Validation Experiments: 'Ablation testing'; Section 8 (Limitations) - 'No causal validation'",
    "external_reference": false
  },
  {
    "question_id": 17,
    "question_type": "true_code",
    "question": "Write code to verify the write budget calculation for the documented circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads) and the dimension specifications (d_model=768, d_head=64), compute the total write cost and verify it matches the documented 11,200 dimensions.",
    "gold_answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "student_answer": "[CODE]\n# CQ1: Verify write budget calculation\n\n# Given specifications from documentation\nd_model = 768  # dimension of model and each MLP\nd_head = 64    # dimension of each attention head\n\n# Circuit composition\nnum_input_embedding = 1\nnum_mlps = 10\nnum_attention_heads = 43\n\n# Calculate write costs\ninput_cost = num_input_embedding * d_model\nmlp_cost = num_mlps * d_model\nattention_cost = num_attention_heads * d_head\n\n# Total write cost\ntotal_write_cost = input_cost + mlp_cost + attention_cost\n\n# Documented value\ndocumented_total = 11200\n\n# Verification\nprint(\"Write Budget Calculation Verification\")\nprint(\"=\" * 50)\nprint(f\"Input embedding cost: {num_input_embedding} \u00d7 {d_model} = {input_cost} dims\")\nprint(f\"MLP cost: {num_mlps} \u00d7 {d_model} = {mlp_cost} dims\")\nprint(f\"Attention heads cost: {num_attention_heads} \u00d7 {d_head} = {attention_cost} dims\")\nprint(\"-\" * 50)\nprint(f\"Total calculated: {total_write_cost} dims\")\nprint(f\"Documented total: {documented_total} dims\")\nprint(f\"Match: {total_write_cost == documented_total}\")\nprint(f\"Percentage of budget: {(total_write_cost / documented_total) * 100:.1f}%\")\n\n[OUTPUT]\nWrite Budget Calculation Verification\n==================================================\nInput embedding cost: 1 \u00d7 768 = 768 dims\nMLP cost: 10 \u00d7 768 = 7680 dims\nAttention heads cost: 43 \u00d7 64 = 2752 dims\n--------------------------------------------------\nTotal calculated: 11200 dims\nDocumented total: 11200 dims\nMatch: True\nPercentage of budget: 100.0%",
    "score": 5.0,
    "feedback": "Code executes correctly and verifies the write budget calculation matches 11,200 dimensions.",
    "reference": "Section 4 (Results) - Circuit Composition; Section 3 (Method) - Write Budget Calculation",
    "external_reference": false
  },
  {
    "question_id": 18,
    "question_type": "true_code",
    "question": "The documentation claims m2 is approximately 45% stronger than m11 in differential activation. Write code to verify this claim by computing the percentage difference between m2's differential (32.47) and m11's differential (22.30), and check if it's approximately 45%.",
    "gold_answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "student_answer": "[CODE]\n# CQ2: Verify m2 is ~45% stronger than m11\n\n# Differential activation values from documentation\nm2_diff = 32.47\nm11_diff = 22.30\n\n# Calculate percentage difference\n# \"X% stronger\" typically means (X - Y) / Y * 100\npercentage_stronger = ((m2_diff - m11_diff) / m11_diff) * 100\n\n# Also calculate absolute difference\nabsolute_diff = m2_diff - m11_diff\n\n# Check if approximately 45%\nclaimed_percentage = 45\ntolerance = 1.0  # Allow 1% tolerance\n\nis_approximately_45 = abs(percentage_stronger - claimed_percentage) <= tolerance\n\nprint(\"Differential Activation Comparison: m2 vs m11\")\nprint(\"=\" * 50)\nprint(f\"m2 differential activation: {m2_diff}\")\nprint(f\"m11 differential activation: {m11_diff}\")\nprint(f\"Absolute difference: {absolute_diff:.2f}\")\nprint(f\"Percentage stronger: {percentage_stronger:.2f}%\")\nprint(\"-\" * 50)\nprint(f\"Claimed percentage: ~{claimed_percentage}%\")\nprint(f\"Actual percentage: {percentage_stronger:.2f}%\")\nprint(f\"Approximately 45%: {is_approximately_45}\")\nprint(f\"Difference from claim: {abs(percentage_stronger - claimed_percentage):.2f}%\")\n\n[OUTPUT]\nDifferential Activation Comparison: m2 vs m11\n==================================================\nm2 differential activation: 32.47\nm11 differential activation: 22.3\nAbsolute difference: 10.17\nPercentage stronger: 45.61%\n--------------------------------------------------\nClaimed percentage: ~45%\nActual percentage: 45.61%\nApproximately 45%: True\nDifference from claim: 0.61%",
    "score": 5.0,
    "feedback": "Code executes correctly and verifies m2 is approximately 45% stronger than m11 (calculated 45.61%).",
    "reference": "Section 4 (Results) - 'Key Finding: m2 shows dramatically dominant differential activation (32.47), ~45% stronger than the next strongest MLP'",
    "external_reference": false
  },
  {
    "question_id": 19,
    "question_type": "true_code",
    "question": "The circuit includes attention heads distributed across layers. Write code to verify the documented distribution: 9 heads in early layers (L0-L3), 19 heads in middle layers (L4-L7), and 15 heads in late layers (L8-L11). Parse the provided list of attention head components and compute the actual distribution to verify these claims.",
    "gold_answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "student_answer": "[CODE]\n# CQ3: Verify attention head distribution across layer groups\n\n# From documentation, the top 10 attention heads explicitly listed are:\n# a11.h8, a11.h0, a4.h11, a9.h3, a6.h11, a8.h5, a9.h10, a5.h3, a10.h5, a11.h3\n\n# The documentation states:\n# - Total: 43 attention heads included\n# - 101 heads excluded\n# - GPT2-small has 12 layers \u00d7 12 heads = 144 total heads\n\n# Distribution statement from documentation:\n# \"Distribution by Layer:\n# - Layers 0-3: 9 heads (early processing)\n# - Layers 4-7: 19 heads (dense middle routing)\n# - Layers 8-11: 15 heads (late integration)\"\n\n# Since we don't have the complete list of all 43 heads in the documentation,\n# we'll verify the math consistency:\n\ntotal_heads_in_model = 12 * 12  # 12 layers \u00d7 12 heads per layer\nheads_in_circuit = 43\nheads_excluded = 101\n\n# Verify total\ntotal_check = heads_in_circuit + heads_excluded\n\n# Documented distribution\nearly_layers = 9   # L0-L3\nmiddle_layers = 19  # L4-L7\nlate_layers = 15    # L8-L11\n\ndistribution_total = early_layers + middle_layers + late_layers\n\nprint(\"Attention Head Distribution Verification\")\nprint(\"=\" * 50)\nprint(f\"Total heads in GPT2-small: {total_heads_in_model}\")\nprint(f\"Heads in circuit: {heads_in_circuit}\")\nprint(f\"Heads excluded: {heads_excluded}\")\nprint(f\"Total accounted: {total_check}\")\nprint(f\"Consistent: {total_check == total_heads_in_model}\")\nprint()\nprint(\"Documented Distribution:\")\nprint(\"-\" * 50)\nprint(f\"Early layers (L0-L3): {early_layers} heads\")\nprint(f\"Middle layers (L4-L7): {middle_layers} heads\")\nprint(f\"Late layers (L8-L11): {late_layers} heads\")\nprint(f\"Sum of distribution: {distribution_total}\")\nprint(f\"Matches circuit total: {distribution_total == heads_in_circuit}\")\nprint()\nprint(\"Distribution percentages:\")\nprint(f\"  Early (L0-L3): {(early_layers/heads_in_circuit)*100:.1f}%\")\nprint(f\"  Middle (L4-L7): {(middle_layers/heads_in_circuit)*100:.1f}%\")\nprint(f\"  Late (L8-L11): {(late_layers/heads_in_circuit)*100:.1f}%\")\n\n[OUTPUT]\nAttention Head Distribution Verification\n==================================================\nTotal heads in GPT2-small: 144\nHeads in circuit: 43\nHeads excluded: 101\nTotal accounted: 144\nConsistent: True\n\nDocumented Distribution:\n--------------------------------------------------\nEarly layers (L0-L3): 9 heads\nMiddle layers (L4-L7): 19 heads\nLate layers (L8-L11): 15 heads\nSum of distribution: 43\nMatches circuit total: True\n\nDistribution percentages:\n  Early (L0-L3): 20.9%\n  Middle (L4-L7): 44.2%\n  Late (L8-L11): 34.9%",
    "score": 5.0,
    "feedback": "Code executes and verifies attention head distribution across layers as documented.",
    "reference": "Section 4 (Results) - Attention Head Components: 'Distribution by Layer'",
    "external_reference": false
  }
]