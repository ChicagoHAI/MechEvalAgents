[
  {
    "question_type": "multiple_choice",
    "question": "What is the primary computational mechanism used by GPT2-small for sarcasm detection, according to the documented circuit?",
    "answer": "C",
    "choice": ["A) Attention-based information routing across sequence positions", "B) Late-layer sentiment polarity reversal", "C) Early-layer MLP-based incongruity detection", "D) Distributed gradient computation across all 12 layers"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 5 (Analysis) - Mechanistic Interpretation; Section 7 (Main Takeaways) - 'MLPs dominate' and 'Sarcasm detection is early'"
  },
  {
    "question_type": "multiple_choice",
    "question": "Which component shows the most dominant differential activation in the sarcasm circuit, and what is its approximate differential activation value?",
    "answer": "B",
    "choice": ["A) a11.h8 (Layer 11, Head 8) with differential ~3.33", "B) m2 (Layer 2 MLP) with differential ~32.47", "C) m11 (Layer 11 MLP) with differential ~22.30", "D) a4.h11 (Layer 4, Head 11) with differential ~1.40"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - MLP Components table; Section 5 (Analysis) - 'Layer 2 MLP (m2) is primary detector'"
  },
  {
    "question_type": "multiple_choice",
    "question": "What is the total write budget utilized by the documented sarcasm circuit?",
    "answer": "D",
    "choice": ["A) 8,448 dimensions (75% of budget)", "B) 9,600 dimensions (86% of budget)", "C) 10,240 dimensions (91% of budget)", "D) 11,200 dimensions (100% of budget)"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - Circuit Composition: 'Total write cost: 11,200 / 11,200 (100%)'"
  },
  {
    "question_type": "free_generation",
    "question": "The documentation describes a three-stage hierarchical process for sarcasm detection. Describe each stage, identify the key components involved, and explain the computational function performed at each stage.",
    "answer": "The three-stage process is:\n\n**Stage 1: Early Detection (L0-L2)** - The primary component is m2 (Layer 2 MLP), supported by m0 and m1. Function: Detect incongruity between sentiment words and contextual markers by identifying mismatch patterns like positive words combined with negative situations.\n\n**Stage 2: Signal Propagation and Refinement (L3-L7)** - Key components include m5, m6, m7 MLPs and 19 attention heads in layers 4-7. Function: Propagate and refine the sarcasm signal detected by m2, distributing information across sequence positions for context-aware processing.\n\n**Stage 3: Final Integration (L8-L11)** - Critical components are m8, m9, m10, m11 MLPs and Layer 11 attention heads (especially a11.h8 and a11.h0). Function: Perform final processing and integration of the sarcasm signal into the output representation, determining how sarcasm affects final token predictions.",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 5 (Analysis) - Mechanistic Interpretation (all three stages); Plan v2 - Revised Mechanistic Model"
  },
  {
    "question_type": "multiple_choice",
    "question": "Which two MLP layers were excluded from the circuit due to minimal differential activation?",
    "answer": "B",
    "choice": ["A) m0 and m1", "B) m3 and m4", "C) m5 and m6", "D) m10 and m11"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - Excluded Components: 'MLPs excluded: m3, m4 - Showed minimal differential activation (<6.5)'"
  },
  {
    "question_type": "free_generation",
    "question": "Compare the sarcasm circuit to the Indirect Object Identification (IOI) circuit along four dimensions: primary mechanism, key layer, circuit size, and relative importance of attention vs. MLPs. What does this comparison suggest about linguistic task processing in transformers?",
    "answer": "The comparison reveals:\n\n1. **Primary mechanism**: IOI uses name copying via attention; sarcasm uses incongruity detection via MLP\n2. **Key layer**: IOI relies on later layers (9-11); sarcasm relies on early layer (2)\n3. **Circuit size**: IOI is sparse (~10 components); sarcasm is dense (54 components)\n4. **Attention vs. MLPs**: IOI has dominant attention with supporting MLPs; sarcasm has dominant MLPs with supporting attention\n\nThis suggests that different linguistic tasks use fundamentally different computational strategies within the same transformer architecture. Simple reference-based tasks may use attention-heavy circuits, while complex semantic/pragmatic tasks requiring pattern recognition may rely more heavily on MLPs.",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 5 (Analysis) - Comparison to IOI Circuit table"
  },
  {
    "question_type": "multiple_choice",
    "question": "What method was used to identify components causally important for sarcasm detection?",
    "answer": "C",
    "choice": ["A) Gradient-based attribution analysis", "B) Systematic ablation testing with behavioral metrics", "C) Differential activation analysis on paired examples", "D) Linear probing with supervised sarcasm classifiers"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 3 (Method) - Experimental Approach: 'We used differential activation analysis'"
  },
  {
    "question_type": "free_generation",
    "question": "Explain the key linguistic features that distinguish sarcastic from literal sentences in the dataset. How might these features enable Layer 2 MLP to detect incongruity?",
    "answer": "Key linguistic features of sarcasm in the dataset:\n\n1. **Discourse markers**: Emphasis particles like 'Oh', 'Wow', 'Just' that signal pragmatic meaning\n2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\n3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\n4. **Contradiction**: Positive words describing objectively negative situations\n\nLayer 2 MLP likely detects incongruity by:\n- Processing co-occurrence patterns of positive sentiment words with negative contexts\n- Recognizing discourse markers that prime for non-literal interpretation\n- Computing a mismatch signal when sentiment polarity conflicts with situational valence\n- This happens early because the pattern (positive adjective + negative situation) can be detected from local context without deep semantic integration",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 2 (Data) - Key Linguistic Features of Sarcasm; Section 5 (Analysis) - Stage 1: Early Detection mechanism"
  },
  {
    "question_type": "multiple_choice",
    "question": "How many attention heads were included in the final circuit, and which layer contains the most important attention heads?",
    "answer": "A",
    "choice": ["A) 43 heads total, with the most important in Layer 11", "B) 101 heads total, with the most important in Layer 4", "C) 54 heads total, with the most important in Layer 2", "D) 19 heads total, with the most important in Layer 6"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - Circuit Composition ('Attention heads: 43') and Top 10 Most Important Heads table (a11.h8 and a11.h0 are top-ranked)"
  },
  {
    "question_type": "free_generation",
    "question": "The original hypothesis predicted that middle layers (L4-L7) would be the primary detection site, but empirical evidence showed Layer 2 as the primary detector. Explain this discrepancy and what it reveals about the mechanistic difference between the predicted and actual sarcasm processing.",
    "answer": "The discrepancy arose because:\n\n**Original hypothesis**: Based on linguistic intuition that incongruity detection would require substantial semantic processing, placing it in middle layers after sentiment encoding is complete.\n\n**Empirical finding**: m2 (Layer 2) showed dramatically dominant differential activation (32.47), indicating sarcasm detection happens much earlier.\n\n**Mechanistic difference**:\n- **Predicted**: Sequential process where early layers encode sentiment, middle layers detect incongruity, late layers reverse meaning\n- **Actual**: Early layers detect incongruity directly from surface patterns (positive words + negative context), middle layers propagate/refine the signal, late layers integrate (not reverse) the signal into output\n\nThis reveals that sarcasm detection in GPT2-small relies on pattern matching of co-occurrence features rather than deep semantic reasoning. The network doesn't build up complex representations and then detect contradiction—it recognizes the sarcastic pattern directly from relatively shallow features.",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Plan v2 - Original vs. Observed Mechanism section; Section 5 (Analysis) - Hypothesis Evolution"
  },
  {
    "question_type": "multiple_choice",
    "question": "In the differential activation analysis method, activations were averaged over which dimension to handle variable-length inputs?",
    "answer": "B",
    "choice": ["A) Batch dimension", "B) Sequence position dimension", "C) Model dimension (d_model)", "D) Head dimension (d_head)"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 3 (Method) - Technical Details: 'Normalization: Averaged activations over sequence positions'; Code walkthrough - measure_activation_difference_normalized function"
  },
  {
    "question_type": "free_generation",
    "question": "The circuit uses 10 MLPs (7,680 dims) versus 43 attention heads (2,752 dims). Given the budget-constrained selection algorithm described in the documentation, explain why this distribution occurred and what it implies about the relative importance of MLPs vs. attention for sarcasm detection.",
    "answer": "This distribution occurred because of the greedy budget-constrained selection algorithm:\n\n1. **Selection strategy**: The algorithm first added high-differential MLPs (threshold ~7.0), then filled remaining budget with attention heads ranked by importance\n\n2. **Why this distribution**:\n   - MLPs showed much higher differential activation than attention heads (m2: 32.47 vs. top head a11.h8: 3.33)\n   - Each MLP costs 768 dims but provides higher signal; attention heads cost only 64 dims but contribute less individually\n   - The algorithm maximized impact by prioritizing high-differential MLPs first\n\n3. **Implications**:\n   - MLPs are more specialized for sarcasm detection (pattern recognition)\n   - Attention heads play a supporting role (information routing/integration)\n   - Sarcasm is primarily a pattern-matching task rather than a reference/copying task\n   - This contrasts with circuits like IOI where attention dominates, showing task-specific architectural preferences",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - Circuit Composition; Code walkthrough - Budget-Constrained Selection algorithm; Section 7 - 'MLPs dominate'"
  },
  {
    "question_type": "multiple_choice",
    "question": "What is a key limitation of using differential activation (L2 norm of activation differences) as the selection criterion for circuit components?",
    "answer": "D",
    "choice": ["A) It cannot handle variable-length sequences", "B) It requires expensive gradient computation", "C) It only works for attention mechanisms, not MLPs", "D) High differential activation does not guarantee causal importance"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 8 (Limitations) - 'No causal validation: Differential activation ≠ causal importance'; Plan v2 - Limitations section"
  },
  {
    "question_type": "free_generation",
    "question": "Based on the documented circuit structure and the exclusion of m3 and m4, propose a hypothesis for why these specific middle layers might show minimal differential activation. What experiments would you conduct to test this hypothesis?",
    "answer": "**Hypothesis**: m3 and m4 may be bypassed in sarcasm processing because:\n\n1. **Early detection is sufficient**: m2 already detects the sarcasm pattern, so layers 3-4 don't need additional specialized processing\n2. **Residual stream bypass**: The sarcasm signal from m2 may propagate through the residual stream, bypassing m3/m4 MLPs\n3. **Task specialization**: m3/m4 may specialize in other linguistic tasks (e.g., syntax, factual knowledge) orthogonal to sarcasm\n4. **Architectural bottleneck**: These layers may serve as information compression points that preserve the sarcasm signal without actively processing it\n\n**Experiments to test**:\n\n1. **Targeted ablation**: Zero out m3 and m4 individually and jointly; measure impact on sarcasm detection accuracy. If hypothesis is correct, impact should be minimal.\n\n2. **Activation patching**: Replace m3/m4 activations with activations from neutral (non-sarcastic) examples; measure if sarcasm signal is still transmitted to later layers.\n\n3. **Residual stream analysis**: Track the sarcasm signal through the residual stream at each layer. Check if m2's contribution persists unchanged through layers 3-4.\n\n4. **Cross-task probing**: Train linear probes for multiple tasks at each layer; check if m3/m4 specialize in non-sarcasm tasks.\n\n5. **Attention pattern analysis**: Examine if attention heads in layers 3-4 route around these MLPs by attending to earlier residual stream states.",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 4 (Results) - Excluded Components; Section 6 (Next Steps) - Open Questions ('Are m3 and m4 intentionally bypassed')"
  },
  {
    "question_type": "multiple_choice",
    "question": "According to the documentation, what is the dimension of each attention head's output in GPT2-small?",
    "answer": "C",
    "choice": ["A) 768 dimensions", "B) 128 dimensions", "C) 64 dimensions", "D) 32 dimensions"],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 3 (Method) - Technical Details: 'd_head = 64'"
  },
  {
    "question_type": "free_generation",
    "question": "The documentation mentions that the circuit hasn't been validated with ablation testing. Design a systematic ablation experiment to test the sufficiency and necessity of the identified circuit components. Your design should address both individual component importance and potential interaction effects.",
    "answer": "**Systematic Ablation Experiment Design**:\n\n**Phase 1: Individual Component Ablation (Necessity Testing)**\n\n1. **Setup**: Create test set of 100+ paired sarcastic/literal examples with ground truth labels\n2. **Baseline**: Measure full model accuracy on sarcasm classification\n3. **Single ablation**: For each of the 54 components:\n   - Zero out or randomly replace the component's output\n   - Measure classification accuracy\n   - Compute importance score: baseline_acc - ablated_acc\n4. **Result**: Rank components by causal importance (not just differential activation)\n\n**Phase 2: Hierarchical Ablation (Stage Testing)**\n\n1. Ablate entire stages:\n   - Remove all Early Detection components (m0, m1, m2)\n   - Remove all Signal Propagation components (m5-m7, L4-L7 heads)\n   - Remove all Final Integration components (m8-m11, L11 heads)\n2. Measure which stage has the largest impact (tests necessity of three-stage model)\n\n**Phase 3: Complementary Circuit Test (Sufficiency)**\n\n1. Keep only the 54 identified components, ablate everything else\n2. Measure if circuit alone is sufficient for sarcasm detection\n3. Compare performance to full model\n\n**Phase 4: Interaction Testing**\n\n1. **Pairwise interactions**: Test critical pairs (e.g., m2 + m11, m2 + a11.h8)\n   - Ablate both together; compare to sum of individual ablations\n   - If joint_impact > sum(individual_impacts), components interact synergistically\n2. **Progressive ablation**: Remove components in order of importance; find minimal circuit\n\n**Phase 5: Causal Intervention**\n\n1. **Activation patching**: Replace component activations with those from literal examples when processing sarcastic examples\n2. If m2 is truly causal, patching its activation should flip predictions\n\n**Metrics**:\n- Accuracy on binary sarcasm classification\n- Logit difference between sarcastic and literal interpretations\n- Effect size of each ablation (Cohen's d)\n\n**Controls**:\n- Test on held-out examples\n- Compare to random ablation baseline\n- Verify ablation doesn't just introduce noise",
    "choice": [],
    "requires_code": false,
    "code_id": null,
    "reference": "Section 6 (Next Steps) - Validation Experiments: 'Ablation testing'; Section 8 (Limitations) - 'No causal validation'"
  },
  {
    "question_type": "true_code",
    "question": "Write code to verify the write budget calculation for the documented circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads) and the dimension specifications (d_model=768, d_head=64), compute the total write cost and verify it matches the documented 11,200 dimensions.",
    "answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ1",
    "reference": "Section 4 (Results) - Circuit Composition; Section 3 (Method) - Write Budget Calculation"
  },
  {
    "question_type": "true_code",
    "question": "The documentation claims m2 is approximately 45% stronger than m11 in differential activation. Write code to verify this claim by computing the percentage difference between m2's differential (32.47) and m11's differential (22.30), and check if it's approximately 45%.",
    "answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ2",
    "reference": "Section 4 (Results) - 'Key Finding: m2 shows dramatically dominant differential activation (32.47), ~45% stronger than the next strongest MLP'"
  },
  {
    "question_type": "true_code",
    "question": "The circuit includes attention heads distributed across layers. Write code to verify the documented distribution: 9 heads in early layers (L0-L3), 19 heads in middle layers (L4-L7), and 15 heads in late layers (L8-L11). Parse the provided list of attention head components and compute the actual distribution to verify these claims.",
    "answer": "See exam_code_solutions.ipynb for full solution with auto-check",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ3",
    "reference": "Section 4 (Results) - Attention Head Components: 'Distribution by Layer'"
  }
]
