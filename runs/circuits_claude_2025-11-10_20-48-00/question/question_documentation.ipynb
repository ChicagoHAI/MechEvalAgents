{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f1b07b",
   "metadata": {},
   "source": [
    "# Sarcasm Circuit Analysis - Assessment Questions\n",
    "\n",
    "This assessment evaluates your understanding of the sarcasm circuit analysis documentation for GPT2-small.\n",
    "\n",
    "**Instructions**: \n",
    "- Answer all questions based on the documentation provided\n",
    "- For multiple choice questions, select the correct answer\n",
    "- For free generation questions, provide detailed explanations\n",
    "- For code questions, complete the code to produce the expected output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ab6bc",
   "metadata": {},
   "source": [
    "## Key Knowledge Points\n",
    "\n",
    "The following key concepts from the documentation are tested in this assessment:\n",
    "\n",
    "### 1. Goal and Setup\n",
    "- Task: Identify sarcasm detection circuit in GPT2-small\n",
    "- Model specifications: 12 layers, 12 attention heads per layer\n",
    "- Dimensions: d_model = 768, d_head = 64\n",
    "- Write budget constraint: ≤ 11,200 dimensions\n",
    "\n",
    "### 2. Dataset Characteristics\n",
    "- 40 total examples (20 sarcastic, 20 literal)\n",
    "- Synthetic paired data structure\n",
    "- Key linguistic features: discourse markers, positive sentiment words, negative situational context\n",
    "\n",
    "### 3. Methodology\n",
    "- Differential activation analysis\n",
    "- L2 norm measurement: ||mean_sarc - mean_lit||_2\n",
    "- Normalization over sequence positions\n",
    "\n",
    "### 4. Circuit Composition\n",
    "- 54 total components (1 input + 10 MLPs + 43 attention heads)\n",
    "- 100% budget utilization (11,200 dimensions)\n",
    "- Key finding: m2 as primary sarcasm detector\n",
    "\n",
    "### 5. Three-Stage Mechanistic Model\n",
    "- Stage 1 (L0-L2): Early detection\n",
    "- Stage 2 (L3-L7): Distributed propagation\n",
    "- Stage 3 (L8-L11): Final integration\n",
    "\n",
    "### 6. Key Insights\n",
    "- Sarcasm detection happens early (L2), not gradually\n",
    "- MLPs dominate over attention heads\n",
    "- Different from IOI circuit (MLP-dominant vs attention-dominant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a9931",
   "metadata": {},
   "source": [
    "---\n",
    "## Assessment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65011a55",
   "metadata": {},
   "source": [
    "### Question 1 (Multiple Choice)\n",
    "\n",
    "What is the dimension of a single attention head (d_head) in GPT2-small as used in this sarcasm circuit analysis?\n",
    "\n",
    "A. 32 dimensions\n",
    "B. 128 dimensions\n",
    "C. 768 dimensions\n",
    "D. 64 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71495555",
   "metadata": {},
   "source": [
    "### Question 2 (Multiple Choice)\n",
    "\n",
    "How many total components are included in the identified sarcasm detection circuit?\n",
    "\n",
    "A. 44 components (1 input + 12 MLPs + 31 attention heads)\n",
    "B. 54 components (1 input + 10 MLPs + 43 attention heads)\n",
    "C. 64 components (1 input + 12 MLPs + 51 attention heads)\n",
    "D. 24 components (1 input + 10 MLPs + 13 attention heads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af95236",
   "metadata": {},
   "source": [
    "### Question 3 (Multiple Choice)\n",
    "\n",
    "Which MLP layer is identified as the primary sarcasm detector with the highest differential activation?\n",
    "\n",
    "A. m0 (Layer 0 MLP) with 7.33 average differential activation\n",
    "B. m2 (Layer 2 MLP) with 32.47 average differential activation\n",
    "C. m11 (Layer 11 MLP) with 22.30 average differential activation\n",
    "D. m5 (Layer 5 MLP) with 7.79 average differential activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff7f07",
   "metadata": {},
   "source": [
    "### Question 4 (Free Generation)\n",
    "\n",
    "Which two MLP layers were excluded from the sarcasm circuit? Explain why they were excluded based on the documentation.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cde7f5",
   "metadata": {},
   "source": [
    "### Question 5 (Free Generation)\n",
    "\n",
    "The initial hypothesis suggested that sarcasm detection follows a three-stage process: sentiment encoding → incongruity detection → meaning reversal. How did the empirical findings revise this understanding? Explain the key differences between the original hypothesis and the observed mechanism.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a56a76",
   "metadata": {},
   "source": [
    "### Question 6 (Multiple Choice)\n",
    "\n",
    "How does the sarcasm circuit differ from the Indirect Object Identification (IOI) circuit in terms of the dominant component type?\n",
    "\n",
    "A. Both circuits are equally balanced between MLPs and attention\n",
    "B. IOI circuit is MLP-dominant while sarcasm circuit is attention-dominant\n",
    "C. Both circuits are primarily attention-dominant\n",
    "D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c866e7",
   "metadata": {},
   "source": [
    "### Question 7 (Free Generation)\n",
    "\n",
    "Identify the two most important attention heads in the sarcasm circuit based on differential activation. What is their interpreted function according to the documentation?\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5add3",
   "metadata": {},
   "source": [
    "### Question 8 (Free Generation)\n",
    "\n",
    "Based on the documentation, explain the key linguistic features that characterize sarcastic sentences in the dataset. How does the combination of these features create the contradiction that the circuit must detect?\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8944e",
   "metadata": {},
   "source": [
    "### Question 9 (Free Generation)\n",
    "\n",
    "If you wanted to include all 12 MLPs and all 144 attention heads (12 layers × 12 heads) in a circuit for GPT2-small, calculate the total write cost. Would this exceed the 11,200 dimension budget? Show your calculation.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0606",
   "metadata": {},
   "source": [
    "### Question 10 (Multiple Choice)\n",
    "\n",
    "According to the revised mechanistic model, what is the primary function of the middle layers (L3-L7) in the sarcasm circuit?\n",
    "\n",
    "A. Primary incongruity detection - identifying contradictions between sentiment and context\n",
    "B. Sentiment encoding - detecting and encoding literal sentiment words\n",
    "C. Distributed propagation - refining and routing the sarcasm signal across sequence positions\n",
    "D. Meaning reversal - flipping sentiment polarity when sarcasm is detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa523fa",
   "metadata": {},
   "source": [
    "### Question 11 (Multiple Choice)\n",
    "\n",
    "What normalization technique was used to handle variable-length inputs when computing differential activations?\n",
    "\n",
    "A. Max pooling over sequence positions\n",
    "B. Averaged activations over sequence positions (mean over sequence dimension)\n",
    "C. Used only the last token's activation\n",
    "D. Padded all sequences to the same length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fcde8",
   "metadata": {},
   "source": [
    "### Question 12 (Free Generation)\n",
    "\n",
    "The documentation lists several limitations of the study. Why is the distinction between 'differential activation' and 'causal importance' considered a significant limitation? Explain what additional experiments would be needed to establish causal importance.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aedd83",
   "metadata": {},
   "source": [
    "### Code Question CQ1\n",
    "\n",
    "Write code to verify the write budget calculation for the sarcasm circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads), compute the total write cost and verify it matches the documented 11,200 dimension budget.\n",
    "\n",
    "Your code should:\n",
    "1. Define the dimension sizes (d_model=768 for input/MLPs, d_head=64 for attention heads)\n",
    "2. Calculate the individual costs for each component type\n",
    "3. Calculate the total write cost\n",
    "4. Print the breakdown and verify it equals 11,200\n",
    "\n",
    "Expected output should show the individual costs and confirm the total equals 11,200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7563bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify the write budget calculation\n",
    "# Define dimension sizes\n",
    "d_model = None  # TODO: Set to 768\n",
    "d_head = None   # TODO: Set to 64\n",
    "\n",
    "# Circuit composition\n",
    "num_input = None        # TODO: Set to 1\n",
    "num_mlps = None         # TODO: Set to 10\n",
    "num_attention_heads = None  # TODO: Set to 43\n",
    "\n",
    "# Calculate individual costs\n",
    "input_cost = None       # TODO: Calculate\n",
    "mlp_cost = None         # TODO: Calculate\n",
    "attn_cost = None        # TODO: Calculate\n",
    "\n",
    "# Calculate total\n",
    "total_cost = None       # TODO: Calculate\n",
    "\n",
    "# Print results\n",
    "print(f\"Input embedding cost: {input_cost} dimensions\")\n",
    "print(f\"MLP cost ({num_mlps} MLPs): {mlp_cost} dimensions\")\n",
    "print(f\"Attention head cost ({num_attention_heads} heads): {attn_cost} dimensions\")\n",
    "print(f\"Total write cost: {total_cost} dimensions\")\n",
    "print(f\"Budget verification: {total_cost == 11200}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559dbbe",
   "metadata": {},
   "source": [
    "### Code Question CQ2\n",
    "\n",
    "Write code to analyze the distribution of the 43 attention heads in the sarcasm circuit across the 12 layers (0-11). \n",
    "\n",
    "Given the list of attention heads in the circuit (from the JSON file or as provided below), your code should:\n",
    "1. Parse the attention head names to extract layer numbers\n",
    "2. Count the number of heads per layer\n",
    "3. Group layers into three stages:\n",
    "   - Early (L0-L3)\n",
    "   - Middle (L4-L7)  \n",
    "   - Late (L8-L11)\n",
    "4. Print the count per layer and the total per stage\n",
    "5. Verify the stage totals match the documentation: Early=9, Middle=19, Late=15\n",
    "\n",
    "Attention heads in circuit: ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze attention head distribution by layer\n",
    "attention_heads = ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', \n",
    "                   'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', \n",
    "                   'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', \n",
    "                   'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', \n",
    "                   'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', \n",
    "                   'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', \n",
    "                   'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n",
    "\n",
    "# TODO: Initialize a counter for each layer (0-11)\n",
    "layer_counts = None  # Use a dictionary or list\n",
    "\n",
    "# TODO: Parse each attention head name to extract layer number\n",
    "# Example: 'a11.h8' -> layer 11\n",
    "for head in attention_heads:\n",
    "    # TODO: Extract layer number from head name\n",
    "    layer = None  # Parse the layer number\n",
    "    # TODO: Increment the count for this layer\n",
    "    pass\n",
    "\n",
    "# TODO: Print count per layer\n",
    "print(\"Heads per layer:\")\n",
    "for layer in range(12):\n",
    "    # TODO: Print layer and count\n",
    "    pass\n",
    "\n",
    "# TODO: Calculate stage totals\n",
    "early_total = None    # Layers 0-3\n",
    "middle_total = None   # Layers 4-7\n",
    "late_total = None     # Layers 8-11\n",
    "\n",
    "print(f\"\\nStage totals:\")\n",
    "print(f\"Early (L0-L3): {early_total} heads\")\n",
    "print(f\"Middle (L4-L7): {middle_total} heads\")\n",
    "print(f\"Late (L8-L11): {late_total} heads\")\n",
    "\n",
    "# TODO: Verify against documentation\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Early matches doc (9): {early_total == 9}\")\n",
    "print(f\"Middle matches doc (19): {middle_total == 19}\")\n",
    "print(f\"Late matches doc (15): {late_total == 15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137975b8",
   "metadata": {},
   "source": [
    "### Code Question CQ3\n",
    "\n",
    "Write code to analyze the relative contribution of MLPs versus attention heads to the sarcasm circuit in terms of dimensions.\n",
    "\n",
    "Your code should:\n",
    "1. Calculate the total dimensions from MLPs (10 MLPs × 768 dims)\n",
    "2. Calculate the total dimensions from attention heads (43 heads × 64 dims)\n",
    "3. Calculate the percentage of total circuit dimensions (excluding input) contributed by:\n",
    "   - MLPs\n",
    "   - Attention heads\n",
    "4. Print the results and explain what this ratio tells us about whether the circuit is \"MLP-dominant\" or \"attention-dominant\"\n",
    "\n",
    "Note: Exclude the input embedding (768 dims) from the percentage calculation since it's always required.\n",
    "\n",
    "Expected conclusion: The circuit should be MLP-dominant with MLPs contributing ~74% of non-input dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze MLP vs attention head contribution\n",
    "# Define dimensions and counts\n",
    "d_model = 768\n",
    "d_head = 64\n",
    "num_mlps = 10\n",
    "num_attention_heads = 43\n",
    "\n",
    "# TODO: Calculate total dimensions from each component type\n",
    "mlp_dims = None         # TODO: Calculate\n",
    "attn_dims = None        # TODO: Calculate\n",
    "\n",
    "# TODO: Calculate total non-input dimensions\n",
    "total_non_input = None  # TODO: Calculate (exclude input embedding)\n",
    "\n",
    "# TODO: Calculate percentages\n",
    "mlp_percentage = None   # TODO: Calculate\n",
    "attn_percentage = None  # TODO: Calculate\n",
    "\n",
    "# Print results\n",
    "print(f\"MLP dimensions: {mlp_dims}\")\n",
    "print(f\"Attention head dimensions: {attn_dims}\")\n",
    "print(f\"Total non-input dimensions: {total_non_input}\")\n",
    "print(f\"\\nPercentage contribution:\")\n",
    "print(f\"MLPs: {mlp_percentage:.1f}%\")\n",
    "print(f\"Attention heads: {attn_percentage:.1f}%\")\n",
    "\n",
    "# TODO: Determine if circuit is MLP-dominant or attention-dominant\n",
    "if mlp_percentage > attn_percentage:\n",
    "    print(\"\\nConclusion: Circuit is MLP-dominant\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Circuit is attention-dominant\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
