{
  "grading_results": [
    {
      "question_id": 1,
      "question_type": "multiple_choice",
      "question": "What is the dimension of a single attention head (d_head) in GPT2-small as used in this sarcasm circuit analysis?",
      "gold_answer": "D. 64 dimensions",
      "student_answer": "Question 1: d_head dimension\n\nFrom documentation section 'Technical Details':\n- d_model = 768\n- d_head = 64\n\nAnswer: D. 64 dimensions\n\n**Reasoning:** The documentation explicitly states in the \"Technical Details\" section that GPT2-small has d_head = 64 dimensions. It also mentions in the \"Write Budget Calculation\" that \"Each attention head: 64 dimensions\".\n\n**Answer:** D",
      "student_code": "# Question 1: Find d_head from documentation\n# From documentation: \"d_head = 64\"\n\nprint(\"Question 1: d_head dimension\")\nprint(\"\\nFrom documentation section 'Technical Details':\")\nprint(\"- d_model = 768\")\nprint(\"- d_head = 64\")\nprint(\"\\nAnswer: D. 64 dimensions\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified d_head = 64 dimensions from documentation's Technical Details section.",
      "reference": "Section 3 Method - Technical Details",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 2,
      "question_type": "multiple_choice",
      "question": "How many total components are included in the identified sarcasm detection circuit?",
      "gold_answer": "B. 54 components (1 input + 10 MLPs + 43 attention heads)",
      "student_answer": "Question 2: Total components in circuit\n\nFrom documentation 'Circuit Composition' section:\nTotal Components: 54 (maximizing budget utilization)\n- Input: 1 (768 dims)\n- MLPs: 10 (7,680 dims)\n- Attention heads: 43 (2,752 dims)\n\nAnswer: B. 54 components (1 input + 10 MLPs + 43 attention heads)\n\n**Reasoning:** The documentation clearly states in the \"Circuit Composition\" section that the total components are 54, broken down as: Input (1) + MLPs (10) + Attention heads (43). This equals 1 + 10 + 43 = 54 components.\n\n**Answer:** B",
      "student_code": "# Question 2: Total components\nprint(\"Question 2: Total components in circuit\")\nprint(\"\\nFrom documentation 'Circuit Composition' section:\")\nprint(\"Total Components: 54 (maximizing budget utilization)\")\nprint(\"- Input: 1 (768 dims)\")\nprint(\"- MLPs: 10 (7,680 dims)\")\nprint(\"- Attention heads: 43 (2,752 dims)\")\nprint(\"\\nAnswer: B. 54 components (1 input + 10 MLPs + 43 attention heads)\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified 54 total components (1 input + 10 MLPs + 43 attention heads).",
      "reference": "Section 4 Results - Circuit Composition",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 3,
      "question_type": "multiple_choice",
      "question": "Which MLP layer is identified as the primary sarcasm detector with the highest differential activation?",
      "gold_answer": "B. m2 (Layer 2 MLP) with 32.47 average differential activation",
      "student_answer": "Question 3: Primary sarcasm detector\n\nFrom documentation 'MLP Components (Ranked by Importance)':\nm2 | 32.47 | 2 | **Primary sarcasm detector**\n\nKey Finding: m2 shows **dramatically dominant** differential activation (32.47),\n~45% stronger than the next strongest MLP.\n\nAnswer: B. m2 (Layer 2 MLP) with 32.47 average differential activation\n\n**Reasoning:** The documentation explicitly identifies m2 as the \"Primary sarcasm detector\" in the MLP Components table, with the highest differential activation of 32.47. The documentation notes that this is \"dramatically dominant\" and \"~45% stronger than the next strongest MLP\" (m11 with 22.30).\n\n**Answer:** B",
      "student_code": "# Question 3: Primary sarcasm detector\nprint(\"Question 3: Primary sarcasm detector\")\nprint(\"\\nFrom documentation 'MLP Components (Ranked by Importance)':\")\nprint(\"m2 | 32.47 | 2 | **Primary sarcasm detector**\")\nprint(\"\\nKey Finding: m2 shows **dramatically dominant** differential activation (32.47),\")\nprint(\"~45% stronger than the next strongest MLP.\")\nprint(\"\\nAnswer: B. m2 (Layer 2 MLP) with 32.47 average differential activation\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified m2 as the primary detector with 32.47 differential activation.",
      "reference": "Section 4 Results - MLP Components",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 4,
      "question_type": "free_generation",
      "question": "Which two MLP layers were excluded from the sarcasm circuit? Explain why they were excluded based on the documentation.",
      "gold_answer": "MLP layers m3 and m4 were excluded from the sarcasm circuit. According to the documentation, they showed minimal differential activation (less than 6.5), suggesting that these layers are less involved in sarcasm-specific processing compared to other MLP layers. This indicates that not all layers contribute equally to sarcasm detection, with m3 and m4 potentially performing more general language modeling tasks rather than sarcasm-specific computations.",
      "student_answer": "Question 4: Excluded MLP layers\n\nFrom documentation 'Excluded Components' section:\n\n**MLPs excluded**: m3, m4\n- Showed minimal differential activation (<6.5)\n- Suggests these layers less involved in sarcasm processing\n\nTotal MLPs in GPT2-small: 12 (m0 through m11)\nMLPs included: 10\nMLPs excluded: 2 (m3 and m4)\n\n**Reasoning:** According to the documentation's \"Excluded Components\" section, m3 and m4 are the two MLP layers that were excluded from the sarcasm circuit. The reason given is that they \"showed minimal differential activation (<6.5)\", which suggests these layers are less involved in sarcasm processing compared to the other MLPs that were included in the circuit.\n\n**Answer:** The two excluded MLP layers are m3 and m4. They were excluded because they showed minimal differential activation (less than 6.5), suggesting these layers are less involved in sarcasm processing compared to the 10 MLPs that were included in the circuit.",
      "student_code": "# Question 4: Excluded MLPs\nprint(\"Question 4: Excluded MLP layers\")\nprint(\"\\nFrom documentation 'Excluded Components' section:\")\nprint(\"\\n**MLPs excluded**: m3, m4\")\nprint(\"- Showed minimal differential activation (<6.5)\")\nprint(\"- Suggests these layers less involved in sarcasm processing\")\nprint(\"\\nTotal MLPs in GPT2-small: 12 (m0 through m11)\")\nprint(\"MLPs included: 10\")\nprint(\"MLPs excluded: 2 (m3 and m4)\")",
      "score": 5.0,
      "feedback": "Excellent. Correctly identified m3 and m4 as excluded MLPs with accurate explanation of minimal differential activation.",
      "reference": "Section 4 Results - Excluded Components",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 5,
      "question_type": "free_generation",
      "question": "The initial hypothesis suggested that sarcasm detection follows a three-stage process: sentiment encoding \u2192 incongruity detection \u2192 meaning reversal. How did the empirical findings revise this understanding? Explain the key differences between the original hypothesis and the observed mechanism.",
      "gold_answer": "The empirical findings significantly revised the initial hypothesis in two key ways:\n\n1. **Timing of Detection**: The original hypothesis predicted that incongruity detection would occur in middle layers (L4-L7). However, the empirical evidence revealed that the primary sarcasm detection happens much earlier, at Layer 2 MLP (m2), which showed dramatically dominant differential activation (32.47). This means the network 'decides' whether text is sarcastic at L2, not gradually through middle layers.\n\n2. **Mechanism in Late Layers**: The original hypothesis predicted that late layers (L8-L11) would perform 'meaning reversal' - flipping sentiment polarity when sarcasm is detected. The revised understanding shows that late layers actually perform 'integration' rather than reversal - they don't flip sentiment but instead integrate the early detection signal into the output representation. The late MLPs and Layer 11 attention heads work to incorporate the sarcasm signal detected by m2 into the final prediction.\n\nIn summary, the detection is earlier than expected (L2 vs middle layers) and the process is integration rather than reversal.",
      "student_answer": "Question 5: Revision of initial hypothesis\n\n=== Phase 1: Initial Hypothesis ===\n1. Early layers encode sentiment\n2. Middle layers detect incongruity\n3. Late layers reverse meaning\n\n=== Phase 2: Revised Understanding (Empirical Evidence) ===\n1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\n2. Middle layers **propagate** rather than detect sarcasm signal\n3. Late layers **integrate** rather than reverse sentiment\n\n=== Key Differences ===\n- Timing: Detection happens at Layer 2 (early), not middle layers\n- Middle layer function: Propagation, not detection\n- Late layer function: Integration, not reversal\n\n**Reasoning:** According to the documentation's \"Hypothesis Evolution\" section, the initial hypothesis proposed a three-stage process where early layers encode sentiment, middle layers detect incongruity, and late layers reverse meaning. However, empirical evidence revealed three key revisions:\n\n1. **Timing of detection**: The primary incongruity detection happens at Laye",
      "student_code": "# Question 5: Hypothesis evolution\nprint(\"Question 5: Revision of initial hypothesis\")\nprint(\"\\n=== Phase 1: Initial Hypothesis ===\")\nprint(\"1. Early layers encode sentiment\")\nprint(\"2. Middle layers detect incongruity\")\nprint(\"3. Late layers reverse meaning\")\nprint(\"\\n=== Phase 2: Revised Understanding (Empirical Evidence) ===\")\nprint(\"1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\")\nprint(\"2. Middle layers **propagate** rather than detect sarcasm signal\")\nprint(\"3. Late layers **integrate** rather than reverse sentiment\")\nprint(\"\\n=== Key Differences ===\")\nprint(\"- Timing: Detection happens at Layer 2 (early), not middle layers\")\nprint(\"- Middle layer function: Propagation, not detection\")\nprint(\"- Late layer function: Integration, not reversal\")",
      "score": 5.0,
      "feedback": "Excellent. Comprehensive explanation of hypothesis revision with all three key differences correctly identified.",
      "reference": "Section 5 Analysis - Hypothesis Evolution",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 6,
      "question_type": "multiple_choice",
      "question": "How does the sarcasm circuit differ from the Indirect Object Identification (IOI) circuit in terms of the dominant component type?",
      "gold_answer": "D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant",
      "student_answer": "Question 6: Sarcasm vs IOI circuit comparison\n\nFrom documentation 'Comparison to IOI Circuit' table:\n\n| Aspect | IOI Circuit | Sarcasm Circuit |\n|--------|-------------|-----------------|\n| Attention importance | Dominant | Supporting |\n| MLP importance | Supporting | Dominant |\n\nConclusion: Sarcasm circuit is MLP-dominant, IOI circuit is attention-dominant\n\nAnswer: D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\n\n**Reasoning:** According to the documentation's \"Comparison to IOI Circuit\" table, the sarcasm circuit has MLP importance as \"Dominant\" and attention importance as \"Supporting\", while the IOI circuit has the opposite pattern with attention importance as \"Dominant\" and MLP importance as \"Supporting\". This indicates that the sarcasm circuit is MLP-dominant while the IOI circuit is attention-dominant.\n\n**Answer:** D",
      "student_code": "# Question 6: Comparison to IOI circuit\nprint(\"Question 6: Sarcasm vs IOI circuit comparison\")\nprint(\"\\nFrom documentation 'Comparison to IOI Circuit' table:\")\nprint(\"\\n| Aspect | IOI Circuit | Sarcasm Circuit |\")\nprint(\"|--------|-------------|-----------------|\")\nprint(\"| Attention importance | Dominant | Supporting |\")\nprint(\"| MLP importance | Supporting | Dominant |\")\nprint(\"\\nConclusion: Sarcasm circuit is MLP-dominant, IOI circuit is attention-dominant\")\nprint(\"\\nAnswer: D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified that sarcasm circuit is MLP-dominant while IOI is attention-dominant.",
      "reference": "Section 5 Analysis - Comparison to IOI Circuit",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 7,
      "question_type": "free_generation",
      "question": "Identify the two most important attention heads in the sarcasm circuit based on differential activation. What is their interpreted function according to the documentation?",
      "gold_answer": "The two most important attention heads are:\n\n1. **a11.h8** with 3.33 average differential activation\n2. **a11.h0** with 2.74 average differential activation\n\nBoth heads are located in Layer 11 and are interpreted as 'output integration heads.' Their function is to integrate the processed sarcasm signal into the final representation, determining how the detected sarcasm affects final token predictions. They are critical for the third stage (Final Integration) of the three-stage hierarchical processing model.",
      "student_answer": "Question 7: Two most important attention heads\n\nFrom documentation 'Top 10 Most Important Heads' table:\n\n1. a11.h8 - Avg Diff: 3.33 - Interpretation: Output integration head\n2. a11.h0 - Avg Diff: 2.74 - Interpretation: Output integration head\n\nBoth are from Layer 11 and perform output integration functions.\n\n**Reasoning:** According to the documentation's \"Top 10 Most Important Heads\" table, the two attention heads with the highest differential activation are a11.h8 (3.33 average differential activation) and a11.h0 (2.74 average differential activation). Both heads are from Layer 11, and the documentation interprets their function as \"Output integration head\".\n\n**Answer:** The two most important attention heads are a11.h8 (differential activation: 3.33) and a11.h0 (differential activation: 2.74). Their interpreted function is \"output integration\" - they integrate the sarcasm information into the final output at Layer 11.",
      "student_code": "# Question 7: Top attention heads\nprint(\"Question 7: Two most important attention heads\")\nprint(\"\\nFrom documentation 'Top 10 Most Important Heads' table:\")\nprint(\"\\n1. a11.h8 - Avg Diff: 3.33 - Interpretation: Output integration head\")\nprint(\"2. a11.h0 - Avg Diff: 2.74 - Interpretation: Output integration head\")\nprint(\"\\nBoth are from Layer 11 and perform output integration functions.\")",
      "score": 5.0,
      "feedback": "Excellent. Correctly identified a11.h8 and a11.h0 as top heads with accurate differential activations and functions.",
      "reference": "Section 4 Results - Attention Head Components",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 8,
      "question_type": "free_generation",
      "question": "Based on the documentation, explain the key linguistic features that characterize sarcastic sentences in the dataset. How does the combination of these features create the contradiction that the circuit must detect?",
      "gold_answer": "The key linguistic features that characterize sarcastic sentences are:\n\n1. **Discourse markers**: Words like 'Oh', 'Wow', 'Just' (emphasis particles) that signal non-literal intent\n\n2. **Positive sentiment words**: Words like 'great', 'love', 'fantastic', 'wonderful', 'perfect' that express positive emotion\n\n3. **Negative situational context**: Phrases describing objectively negative situations like 'another meeting', 'stuck in traffic', 'crashed'\n\nThe contradiction arises from the combination of these features: positive sentiment words are used to describe objectively negative situations. For example, 'Oh great, another meeting at 7 AM' uses the positive word 'great' combined with the discourse marker 'Oh' to describe the negative situation of an early morning meeting. The circuit must detect this incongruity between the positive literal sentiment and the negative contextual reality to correctly identify the text as sarcastic. This is why m2 (Layer 2 MLP) functions as an incongruity detector - it processes patterns where positive adjectives are combined with negative situational contexts.",
      "student_answer": "Question 8: Key linguistic features of sarcasm\n\nFrom documentation 'Key Linguistic Features of Sarcasm' section:\n\n1. **Discourse markers**: 'Oh', 'Wow', 'Just' (emphasis particles)\n2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\n3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\n4. **Contradiction**: Positive words describe objectively negative situations\n\nExample: 'Oh great, another meeting at 7 AM.'\n- Discourse marker: 'Oh'\n- Positive word: 'great'\n- Negative context: 'another meeting at 7 AM' (undesirable)\n- Contradiction: Positive 'great' describes negative situation\n\n**Reasoning:** The documentation lists four key linguistic features that characterize sarcastic sentences in the \"Key Linguistic Features of Sarcasm\" section:\n\n1. Discourse markers (emphasis particles like \"Oh\", \"Wow\", \"Just\")\n2. Positive sentiment words (\"great\", \"love\", \"fantastic\", \"wonderful\", \"perfect\")\n3. Negative situational context (\"anot",
      "student_code": "# Question 8: Linguistic features of sarcasm\nprint(\"Question 8: Key linguistic features of sarcasm\")\nprint(\"\\nFrom documentation 'Key Linguistic Features of Sarcasm' section:\")\nprint(\"\\n1. **Discourse markers**: 'Oh', 'Wow', 'Just' (emphasis particles)\")\nprint(\"2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\")\nprint(\"3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\")\nprint(\"4. **Contradiction**: Positive words describe objectively negative situations\")\nprint(\"\\nExample: 'Oh great, another meeting at 7 AM.'\")\nprint(\"- Discourse marker: 'Oh'\")\nprint(\"- Positive word: 'great'\")\nprint(\"- Negative context: 'another meeting at 7 AM' (undesirable)\")\nprint(\"- Contradiction: Positive 'great' describes negative situation\")",
      "score": 5.0,
      "feedback": "Excellent. Correctly identified all four key linguistic features with clear examples and understanding.",
      "reference": "Section 2 Data - Key Linguistic Features of Sarcasm",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 9,
      "question_type": "free_generation",
      "question": "If you wanted to include all 12 MLPs and all 144 attention heads (12 layers \u00d7 12 heads) in a circuit for GPT2-small, calculate the total write cost. Would this exceed the 11,200 dimension budget? Show your calculation.",
      "gold_answer": "Let's calculate the total write cost:\n\n**Input embedding**: 768 dimensions\n\n**All 12 MLPs**: 12 \u00d7 768 = 9,216 dimensions\n\n**All 144 attention heads**: 144 \u00d7 64 = 9,216 dimensions\n\n**Total write cost**: 768 + 9,216 + 9,216 = **19,200 dimensions**\n\n**Comparison to budget**: 19,200 > 11,200\n\nYes, this would exceed the 11,200 dimension budget by 8,000 dimensions (71% over budget). This is why the circuit discovery process must select a subset of components - the budget constraint forces identification of the most important components for sarcasm detection rather than including everything. The actual circuit uses 54 components with exactly 11,200 dimensions (100% budget utilization).",
      "student_answer": "Question 9: Total write cost for all components\n\nGiven information from documentation:\n- GPT2-small has 12 layers\n- Each layer has 12 attention heads\n- Each MLP: 768 dimensions\n- Each attention head: 64 dimensions\n- Input embedding: 768 dimensions\n- Budget: 11,200 dimensions\n\nCalculation:\nInput embedding: 768 dims\nAll 12 MLPs: 12 \u00d7 768 = 9216 dims\nAll 144 attention heads: 12 \u00d7 12 \u00d7 64 = 9216 dims\n\nTotal write cost: 768 + 9216 + 9216 = 19200 dims\nBudget: 11,200 dims\n\nExceeds budget? True\nDifference: 8000 dims over budget\n\n**Reasoning:** Based on the documentation's technical details, GPT2-small has 12 layers with 12 attention heads each (144 total heads). Each MLP costs 768 dimensions, each attention head costs 64 dimensions, and the input embedding costs 768 dimensions.\n\nCalculation:\n- Input embedding: 768 dims\n- All 12 MLPs: 12 \u00d7 768 = 9,216 dims\n- All 144 attention heads: 12 \u00d7 12 \u00d7 64 = 9,216 dims\n- Total: 768 + 9,216 + 9,216 = 19,200 dims\n\nThe budget is 11,200 dimensions.\n\n**Answer:",
      "student_code": "# Question 9: Calculate full circuit cost\nprint(\"Question 9: Total write cost for all components\")\nprint(\"\\nGiven information from documentation:\")\nprint(\"- GPT2-small has 12 layers\")\nprint(\"- Each layer has 12 attention heads\")\nprint(\"- Each MLP: 768 dimensions\")\nprint(\"- Each attention head: 64 dimensions\")\nprint(\"- Input embedding: 768 dimensions\")\nprint(\"- Budget: 11,200 dimensions\")\n\nprint(\"\\nCalculation:\")\ninput_cost = 768\nmlp_cost = 12 * 768\nattn_cost = 144 * 64  # 12 layers \u00d7 12 heads \u00d7 64 dims\ntotal_cost = input_cost + mlp_cost + attn_cost\n\nprint(f\"Input embedding: {input_cost} dims\")\nprint(f\"All 12 MLPs: 12 \u00d7 768 = {mlp_cost} dims\")\nprint(f\"All 144 attention heads: 12 \u00d7 12 \u00d7 64 = {attn_cost} dims\")\nprint(f\"\\nTotal write cost: {input_cost} + {mlp_cost} + {attn_cost} = {total_cost} dims\")\nprint(f\"Budget: 11,200 dims\")\nprint(f\"\\nExceeds budget? {total_cost > 11200}\")\nprint(f\"Difference: {total_cost - 11200} dims over budget\")",
      "score": 5.0,
      "feedback": "Excellent. Correctly calculated total write cost (19,200 dims) and identified budget excess by 8,000 dims.",
      "reference": "Section 3 Method - Write Budget Calculation",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 10,
      "question_type": "multiple_choice",
      "question": "According to the revised mechanistic model, what is the primary function of the middle layers (L3-L7) in the sarcasm circuit?",
      "gold_answer": "C. Distributed propagation - refining and routing the sarcasm signal across sequence positions",
      "student_answer": "Question 10: Function of middle layers (L3-L7)\n\nFrom documentation 'Mechanistic Interpretation' section:\n\n**Stage 2: Distributed Propagation (L3-L7)**\n- Mid-layer MLPs refine the sarcasm signal\n- 19 attention heads route information across sequence positions\n- Enables context-aware processing throughout the sentence\n\nFrom 'Hypothesis Evolution - Phase 2: Revised Understanding':\n2. Middle layers **propagate** rather than detect sarcasm signal\n\nAnswer: C. Distributed propagation - refining and routing the sarcasm signal\n\n**Reasoning:** According to the documentation's \"Mechanistic Interpretation\" section, Stage 2 (L3-L7) is described as \"Distributed Propagation\" where mid-layer MLPs refine the sarcasm signal and attention heads route information across sequence positions. The \"Hypothesis Evolution\" section explicitly states that empirical evidence showed middle layers \"propagate rather than detect sarcasm signal.\"\n\n**Answer:** C",
      "student_code": "# Question 10: Middle layer function\nprint(\"Question 10: Function of middle layers (L3-L7)\")\nprint(\"\\nFrom documentation 'Mechanistic Interpretation' section:\")\nprint(\"\\n**Stage 2: Distributed Propagation (L3-L7)**\")\nprint(\"- Mid-layer MLPs refine the sarcasm signal\")\nprint(\"- 19 attention heads route information across sequence positions\")\nprint(\"- Enables context-aware processing throughout the sentence\")\nprint(\"\\nFrom 'Hypothesis Evolution - Phase 2: Revised Understanding':\")\nprint(\"2. Middle layers **propagate** rather than detect sarcasm signal\")\nprint(\"\\nAnswer: C. Distributed propagation - refining and routing the sarcasm signal\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified middle layers perform distributed propagation.",
      "reference": "Section 5 Analysis - Mechanistic Interpretation",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 11,
      "question_type": "multiple_choice",
      "question": "What normalization technique was used to handle variable-length inputs when computing differential activations?",
      "gold_answer": "B. Averaged activations over sequence positions (mean over sequence dimension)",
      "student_answer": "Question 11: Normalization technique for variable-length inputs\n\nFrom documentation 'Technical Details' section:\n\n**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\n\nAnswer: B. Averaged activations over sequence positions (mean over sequence dimension)\n\n**Reasoning:** The documentation's \"Technical Details\" section explicitly states: \"Normalization: Averaged activations over sequence positions to handle variable-length inputs.\" This indicates that the mean over the sequence dimension was used.\n\n**Answer:** B",
      "student_code": "# Question 11: Normalization technique\nprint(\"Question 11: Normalization technique for variable-length inputs\")\nprint(\"\\nFrom documentation 'Technical Details' section:\")\nprint(\"\\n**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\")\nprint(\"\\nAnswer: B. Averaged activations over sequence positions (mean over sequence dimension)\")",
      "score": 1.0,
      "feedback": "Correct. Student correctly identified averaging over sequence positions as the normalization technique.",
      "reference": "Section 3 Method - Technical Details",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 12,
      "question_type": "free_generation",
      "question": "The documentation lists several limitations of the study. Why is the distinction between 'differential activation' and 'causal importance' considered a significant limitation? Explain what additional experiments would be needed to establish causal importance.",
      "gold_answer": "The distinction between differential activation and causal importance is significant because:\n\n**Differential activation** only measures how differently a component activates on sarcastic vs. literal text. A high differential does not necessarily mean the component is causally responsible for the sarcasm detection behavior - it could be epiphenomenal (correlated but not causal).\n\n**Causal importance** requires demonstrating that the component's contribution actually changes the model's behavior.\n\nTo establish causal importance, the following additional experiments would be needed:\n\n1. **Ablation testing**: Systematically remove each component (by zeroing or mean-ablating its output) and measure the impact on sarcasm detection accuracy. If accuracy drops significantly when a component is removed, it has causal importance.\n\n2. **Intervention experiments**: Patch activations from one example to another. For instance, take the activation from a literal sentence and patch it into a sarcastic sentence - if this changes the model's prediction, the component is causally important.\n\n3. **Activation patching**: Test if restoring just the circuit components (while ablating everything else) is sufficient to reproduce the sarcasm detection behavior.\n\n4. **Probing**: Train linear classifiers to detect sarcasm at each layer to verify when the relevant information becomes available.\n\nWithout these experiments, we cannot be certain that the 54 components identified are all necessary and sufficient for sarcasm detection.",
      "student_answer": "Question 12: Limitation of differential activation analysis\n\nFrom documentation 'Limitations' section:\n3. **No causal validation**: Differential activation \u2260 causal importance\n\nFrom 'Next Steps - Validation Experiments' section:\n1. **Ablation testing**: Systematically remove components, measure impact\n2. **Intervention experiments**: Patch activations to test causality\n\nInterpretation:\n- Differential activation shows CORRELATION (components that activate differently)\n- Causal importance shows CAUSATION (components that are necessary for the behavior)\n- A component could activate differently but not be causally important\n- Need ablation/intervention to establish causality\n\n**Reasoning:** The documentation lists \"No causal validation: Differential activation \u2260 causal importance\" as a limitation. This is significant because differential activation only measures correlation - it shows which components activate differently on sarcastic vs literal examples, but doesn't prove these components",
      "student_code": "# Question 12: Differential activation vs causal importance\nprint(\"Question 12: Limitation of differential activation analysis\")\nprint(\"\\nFrom documentation 'Limitations' section:\")\nprint(\"3. **No causal validation**: Differential activation \u2260 causal importance\")\nprint(\"\\nFrom 'Next Steps - Validation Experiments' section:\")\nprint(\"1. **Ablation testing**: Systematically remove components, measure impact\")\nprint(\"2. **Intervention experiments**: Patch activations to test causality\")\nprint(\"\\nInterpretation:\")\nprint(\"- Differential activation shows CORRELATION (components that activate differently)\")\nprint(\"- Causal importance shows CAUSATION (components that are necessary for the behavior)\")\nprint(\"- A component could activate differently but not be causally important\")\nprint(\"- Need ablation/intervention to establish causality\")",
      "score": 5.0,
      "feedback": "Excellent. Deep understanding of correlation vs causation distinction with proper validation methods identified.",
      "reference": "Section 8 Limitations and Section 6 Next Steps",
      "external_reference": false,
      "requires_code": false
    },
    {
      "question_id": 13,
      "question_type": "free_generation",
      "question": "Write code to verify the write budget calculation for the sarcasm circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads), compute the total write cost and verify it matches the documented 11,200 dimension budget.\n\nYour code should:\n1. Define the dimension sizes (d_model=768 for input/MLPs, d_head=64 for attention heads)\n2. Calculate the individual costs for each component type\n3. Calculate the total write cost\n4. Print the breakdown and verify it equals 11,200\n\nExpected output should show the individual costs and confirm the total equals 11,200.",
      "gold_answer": "The code should compute:\n- Input embedding: 1 \u00d7 768 = 768 dimensions\n- MLPs: 10 \u00d7 768 = 7,680 dimensions  \n- Attention heads: 43 \u00d7 64 = 2,752 dimensions\n- Total: 768 + 7,680 + 2,752 = 11,200 dimensions\n\nThe total matches the documented budget of 11,200 dimensions (100% utilization).",
      "student_answer": "============================================================\nWRITE BUDGET VERIFICATION\n============================================================\n\nComponent Dimensions:\n  d_model (input/MLP): 768 dimensions\n  d_head (attention):  64 dimensions\n\nCircuit Composition:\n  Input embeddings:    1\n  MLP layers:          10\n  Attention heads:     43\n\nCost Breakdown:\n  Input:      1 \u00d7 768 = 768 dims\n  MLPs:       10 \u00d7 768 = 7,680 dims\n  Attn heads: 43 \u00d7 64 = 2,752 dims\n  ---------------------------------------------\n  TOTAL:                      11,200 dims\n\nVerification:\n  Expected budget:  11,200 dims\n  Calculated total: 11,200 dims\n  Match: True \u2713\n  Budget utilization: 100.0%\n============================================================\n\n**Reasoning:** Based on the documentation, the circuit has 1 input embedding (768 dims), 10 MLPs (768 dims each), and 43 attention heads (64 dims each). The calculation shows: 768 + (10 \u00d7 768) + (43 \u00d7 64) = 768 + 7,680 + 2,752 = 11,200 dimensions, which exac",
      "student_code": "# Question 13: Verify write budget calculation\n\n# Define dimension sizes from documentation\nd_model = 768  # for input embedding and MLPs\nd_head = 64    # for attention heads\n\n# Circuit composition from documentation\nnum_input = 1\nnum_mlps = 10\nnum_attn_heads = 43\n\n# Calculate individual costs\ninput_cost = num_input * d_model\nmlp_cost = num_mlps * d_model\nattn_cost = num_attn_heads * d_head\n\n# Calculate total\ntotal_write_cost = input_cost + mlp_cost + attn_cost\n\n# Expected budget\nexpected_budget = 11200\n\n# Print breakdown\nprint(\"=\" * 60)\nprint(\"WRITE BUDGET VERIFICATION\")\nprint(\"=\" * 60)\nprint(\"\\nComponent Dimensions:\")\nprint(f\"  d_model (input/MLP): {d_model} dimensions\")\nprint(f\"  d_head (attention):  {d_head} dimensions\")\n\nprint(\"\\nCircuit Composition:\")\nprint(f\"  Input embeddings:    {num_input}\")\nprint(f\"  MLP layers:          {num_mlps}\")\nprint(f\"  Attention heads:     {num_attn_heads}\")\n\nprint(\"\\nCost Breakdown:\")\nprint(f\"  Input:      {num_input} \u00d7 {d_model} = {input_cost:,} di",
      "score": 5.0,
      "feedback": "Excellent. Code correctly implements and verifies write budget calculation with professional output.",
      "reference": "Section 3 Method - Write Budget Calculation and Section 4 Results - Circuit Composition",
      "external_reference": false,
      "requires_code": true
    },
    {
      "question_id": 14,
      "question_type": "free_generation",
      "question": "Write code to analyze the distribution of the 43 attention heads in the sarcasm circuit across the 12 layers (0-11). \n\nGiven the list of attention heads in the circuit (from the JSON file or as provided below), your code should:\n1. Parse the attention head names to extract layer numbers\n2. Count the number of heads per layer\n3. Group layers into three stages:\n   - Early (L0-L3)\n   - Middle (L4-L7)  \n   - Late (L8-L11)\n4. Print the count per layer and the total per stage\n5. Verify the stage totals match the documentation: Early=9, Middle=19, Late=15\n\nAttention heads in circuit: ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']",
      "gold_answer": "The code should output:\n- Layer distribution showing counts per layer (0-11)\n- Early stage (L0-L3): 9 heads total\n- Middle stage (L4-L7): 19 heads total\n- Late stage (L8-L11): 15 heads total\n\nThis confirms the documentation's statement about the distribution pattern showing dense middle-layer routing (19 heads in L4-L7) and concentrated output integration in late layers (15 heads in L8-L11).",
      "student_answer": "============================================================\nATTENTION HEAD DISTRIBUTION BY LAYER\n============================================================\n\nHeads per layer:\n  Layer  0:  0 heads\n  Layer  1:  1 heads\n  Layer  2:  4 heads\n  Layer  3:  4 heads\n  Layer  4:  6 heads\n  Layer  5:  4 heads\n  Layer  6:  6 heads\n  Layer  7:  3 heads\n  Layer  8:  6 heads\n  Layer  9:  2 heads\n  Layer 10:  2 heads\n  Layer 11:  5 heads\n\n------------------------------------------------------------\nStage Distribution:\n  Early (L0-L3):   9 heads\n  Middle (L4-L7):  19 heads\n  Late (L8-L11):   15 heads\n  Total:           43 heads\n\n------------------------------------------------------------\nVerification against documentation:\n  Early stage:  Expected=9, Actual=9, Match=True \u2713\n  Middle stage: Expected=19, Actual=19, Match=True \u2713\n  Late stage:   Expected=15, Actual=15, Match=True \u2713\n  Total heads:  Expected=43, Actual=43 \u2713\n============================================================\n\n**Reasoning:** The c",
      "student_code": "# Question 14: Analyze attention head distribution across layers\n\nattention_heads = ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', \n                   'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', \n                   'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', \n                   'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', \n                   'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', \n                   'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n\n# Step 1: Parse attention head names and extract layer numbers\nlayer_counts = {i: 0 for i in range(12)}  # Initialize counts for layers 0-11\n\nfor head in attention_heads:\n    # Format: a{layer}.h{head_num}\n    layer_num = int(head.split('.')[0][1:])  # Extract layer number after 'a'\n    layer_counts[layer_num] += 1\n\n# Step 2: Print count per layer\nprint(\"=\" * 60)\nprint(\"ATTENTION HEAD DISTRIBUTION BY LAYER\")\npr",
      "score": 5.0,
      "feedback": "Excellent. Code correctly analyzes head distribution and matches documentation's stated distribution.",
      "reference": "Section 4 Results - Attention Head Components - Distribution by Layer",
      "external_reference": false,
      "requires_code": true
    },
    {
      "question_id": 15,
      "question_type": "free_generation",
      "question": "Write code to analyze the relative contribution of MLPs versus attention heads to the sarcasm circuit in terms of dimensions.\n\nYour code should:\n1. Calculate the total dimensions from MLPs (10 MLPs \u00d7 768 dims)\n2. Calculate the total dimensions from attention heads (43 heads \u00d7 64 dims)\n3. Calculate the percentage of total circuit dimensions (excluding input) contributed by:\n   - MLPs\n   - Attention heads\n4. Print the results and explain what this ratio tells us about whether the circuit is \"MLP-dominant\" or \"attention-dominant\"\n\nNote: Exclude the input embedding (768 dims) from the percentage calculation since it's always required.\n\nExpected conclusion: The circuit should be MLP-dominant with MLPs contributing ~74% of non-input dimensions.",
      "gold_answer": "The code should compute:\n- MLP dimensions: 10 \u00d7 768 = 7,680\n- Attention head dimensions: 43 \u00d7 64 = 2,752\n- Total non-input dimensions: 7,680 + 2,752 = 10,432\n- MLP percentage: 7,680 / 10,432 = 73.6%\n- Attention percentage: 2,752 / 10,432 = 26.4%\n\nThis demonstrates the circuit is MLP-dominant, with MLPs contributing nearly 3\u00d7 the dimensions of attention heads. This supports the documentation's finding that the sarcasm circuit differs from the IOI circuit, which is attention-dominant.",
      "student_answer": "============================================================\nMLP vs ATTENTION HEAD CONTRIBUTION ANALYSIS\n============================================================\n\nDimensional Breakdown:\n  MLPs:           10 \u00d7 768 = 7,680 dims\n  Attention heads: 43 \u00d7 64 = 2,752 dims\n  ---------------------------------------------\n  Total (non-input):        10,432 dims\n\nPercentage Contribution (excluding input):\n  MLPs:            73.6%\n  Attention heads: 26.4%\n\nRatio Analysis:\n  MLP:Attention ratio = 2.79:1\n  MLPs contribute 2.79x more dimensions than attention\n\nConclusion:\n  The circuit is **MLP-DOMINANT**\n  - MLPs contribute 73.6% of non-input dimensions\n  - This indicates sarcasm detection relies primarily on MLP-based\n    pattern detection rather than attention-based information routing\n\nAlignment with Documentation:\n  Expected: MLPs contribute ~74% (from documentation)\n  Actual:   MLPs contribute 73.6%\n  Match: \u2713\n\nInterpretation:\n  From the documentation's 'Comparison to IOI Circuit' table:\n  ",
      "student_code": "# Question 15: Analyze MLP vs Attention contribution\n\n# Circuit parameters from documentation\nnum_mlps = 10\nnum_attn_heads = 43\nd_model = 768  # MLP dimension\nd_head = 64    # Attention head dimension\n\n# Step 1: Calculate total dimensions from MLPs\nmlp_dims = num_mlps * d_model\n\n# Step 2: Calculate total dimensions from attention heads\nattn_dims = num_attn_heads * d_head\n\n# Total non-input dimensions\ntotal_non_input_dims = mlp_dims + attn_dims\n\n# Step 3: Calculate percentages\nmlp_percentage = (mlp_dims / total_non_input_dims) * 100\nattn_percentage = (attn_dims / total_non_input_dims) * 100\n\n# Print results\nprint(\"=\" * 60)\nprint(\"MLP vs ATTENTION HEAD CONTRIBUTION ANALYSIS\")\nprint(\"=\" * 60)\n\nprint(\"\\nDimensional Breakdown:\")\nprint(f\"  MLPs:           {num_mlps} \u00d7 {d_model} = {mlp_dims:,} dims\")\nprint(f\"  Attention heads: {num_attn_heads} \u00d7 {d_head} = {attn_dims:,} dims\")\nprint(f\"  \" + \"-\" * 45)\nprint(f\"  Total (non-input):        {total_non_input_dims:,} dims\")\n\nprint(f\"\\nPercentage Con",
      "score": 5.0,
      "feedback": "Excellent. Code correctly calculates MLP vs attention contribution with insightful ratio analysis.",
      "reference": "Section 5 Analysis - Comparison to IOI Circuit and Section 7 Main Takeaways",
      "external_reference": false,
      "requires_code": true
    }
  ],
  "summary": {
    "overall_score": 1.0,
    "total_questions": 15,
    "comments": "Outstanding performance. Student demonstrated comprehensive understanding of the sarcasm circuit documentation with perfect scores across all questions. All answers were well-reasoned, accurately referenced the documentation, and showed no evidence of external knowledge. Code-required questions included working, well-structured implementations.",
    "grade_level": "Excellent",
    "external_reference_count": 0
  }
}