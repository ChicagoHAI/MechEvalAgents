{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eda0c7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/smallyan/critic_model_mechinterp\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Change to correct working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4caa14cf",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation length: 10312 characters\n",
      "\n",
      "=== First 500 characters of documentation ===\n",
      "# Sarcasm Circuit Documentation - GPT2-Small\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "Identify a precise circuit in GPT2-small that enables sarcasm recognition by detecting contradictions between literal sentiment and contextual tone. The circuit must:\n",
      "- Reproduce sarcasm detection behavior with high fidelity\n",
      "- Remain within 11,200 dimension write budget\n",
      "- Contain interpretable, minimal components\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset Description\n",
      "- **Source**: Synthetic sarcasm dataset (real mib-bench/sarcasm unavailable)\n",
      "- **Size**: 4\n"
     ]
    }
   ],
   "source": [
    "# Read the documentation\n",
    "doc_path = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/logs/documentation.md'\n",
    "with open(doc_path, 'r') as f:\n",
    "    documentation = f.read()\n",
    "\n",
    "print(f\"Documentation length: {len(documentation)} characters\")\n",
    "print(\"\\n=== First 500 characters of documentation ===\")\n",
    "print(documentation[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806482d5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam questions loaded successfully\n",
      "Number of questions: 15\n",
      "\n",
      "=== Questions overview ===\n",
      "\n",
      "Question 1:\n",
      "  Type: unknown\n",
      "  Question: What is the dimension of a single attention head (d_head) in GPT2-small as used in this sarcasm circ...\n",
      "\n",
      "Question 2:\n",
      "  Type: unknown\n",
      "  Question: How many total components are included in the identified sarcasm detection circuit?...\n",
      "\n",
      "Question 3:\n",
      "  Type: unknown\n",
      "  Question: Which MLP layer is identified as the primary sarcasm detector with the highest differential activati...\n",
      "\n",
      "Question 4:\n",
      "  Type: unknown\n",
      "  Question: Which two MLP layers were excluded from the sarcasm circuit? Explain why they were excluded based on...\n",
      "\n",
      "Question 5:\n",
      "  Type: unknown\n",
      "  Question: The initial hypothesis suggested that sarcasm detection follows a three-stage process: sentiment enc...\n",
      "\n",
      "Question 6:\n",
      "  Type: unknown\n",
      "  Question: How does the sarcasm circuit differ from the Indirect Object Identification (IOI) circuit in terms o...\n",
      "\n",
      "Question 7:\n",
      "  Type: unknown\n",
      "  Question: Identify the two most important attention heads in the sarcasm circuit based on differential activat...\n",
      "\n",
      "Question 8:\n",
      "  Type: unknown\n",
      "  Question: Based on the documentation, explain the key linguistic features that characterize sarcastic sentence...\n",
      "\n",
      "Question 9:\n",
      "  Type: unknown\n",
      "  Question: If you wanted to include all 12 MLPs and all 144 attention heads (12 layers × 12 heads) in a circuit...\n",
      "\n",
      "Question 10:\n",
      "  Type: unknown\n",
      "  Question: According to the revised mechanistic model, what is the primary function of the middle layers (L3-L7...\n",
      "\n",
      "Question 11:\n",
      "  Type: unknown\n",
      "  Question: What normalization technique was used to handle variable-length inputs when computing differential a...\n",
      "\n",
      "Question 12:\n",
      "  Type: unknown\n",
      "  Question: The documentation lists several limitations of the study. Why is the distinction between 'differenti...\n",
      "\n",
      "Question 13:\n",
      "  Type: unknown\n",
      "  Question: Write code to verify the write budget calculation for the sarcasm circuit. Given the circuit composi...\n",
      "\n",
      "Question 14:\n",
      "  Type: unknown\n",
      "  Question: Write code to analyze the distribution of the 43 attention heads in the sarcasm circuit across the 1...\n",
      "\n",
      "Question 15:\n",
      "  Type: unknown\n",
      "  Question: Write code to analyze the relative contribution of MLPs versus attention heads to the sarcasm circui...\n"
     ]
    }
   ],
   "source": [
    "# Read the exam questions\n",
    "question_path = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/question/question_sarcasm_circuit_student_version.json'\n",
    "with open(question_path, 'r') as f:\n",
    "    exam_questions = json.load(f)\n",
    "\n",
    "print(\"Exam questions loaded successfully\")\n",
    "print(f\"Number of questions: {len(exam_questions)}\")\n",
    "print(\"\\n=== Questions overview ===\")\n",
    "for i, q in enumerate(exam_questions, 1):\n",
    "    print(f\"\\nQuestion {i}:\")\n",
    "    print(f\"  Type: {q.get('type', 'unknown')}\")\n",
    "    print(f\"  Question: {q.get('question', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080fe75",
   "metadata": {},
   "source": [
    "# Sarcasm Circuit Analysis Exam - Student Answers\n",
    "\n",
    "This notebook contains answers to exam questions based solely on the documentation provided at:\n",
    "`/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/logs/documentation.md`\n",
    "\n",
    "All answers are derived strictly from the documentation without using external knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06468fef",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DOCUMENTATION ===\n",
      "\n",
      "# Sarcasm Circuit Documentation - GPT2-Small\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "Identify a precise circuit in GPT2-small that enables sarcasm recognition by detecting contradictions between literal sentiment and contextual tone. The circuit must:\n",
      "- Reproduce sarcasm detection behavior with high fidelity\n",
      "- Remain within 11,200 dimension write budget\n",
      "- Contain interpretable, minimal components\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset Description\n",
      "- **Source**: Synthetic sarcasm dataset (real mib-bench/sarcasm unavailable)\n",
      "- **Size**: 40 total examples (20 sarcastic, 20 literal)\n",
      "- **Structure**: Paired examples with similar surface structure but opposite intent\n",
      "\n",
      "### Example Sarcastic Sentences\n",
      "1. \"Oh great, another meeting at 7 AM.\"\n",
      "2. \"Wow, I just love getting stuck in traffic.\"\n",
      "3. \"Fantastic, my laptop crashed right before the deadline.\"\n",
      "4. \"Perfect, exactly what I needed today.\"\n",
      "5. \"Oh wonderful, it's raining on my day off.\"\n",
      "\n",
      "### Example Literal Sentences\n",
      "1. \"I'm excited about the meeting at 7 AM tomorrow.\"\n",
      "2. \"I really enjoy my peaceful morning commute.\"\n",
      "3. \"I successfully submitted my project before the deadline.\"\n",
      "4. \"This is exactly what I needed today.\"\n",
      "5. \"I'm happy to have a relaxing day off.\"\n",
      "\n",
      "### Key Linguistic Features of Sarcasm\n",
      "- **Discourse markers**: \"Oh\", \"Wow\", \"Just\" (emphasis particles)\n",
      "- **Positive sentiment words**: \"great\", \"love\", \"fantastic\", \"wonderful\", \"perfect\"\n",
      "- **Negative situational context**: \"another meeting\", \"stuck in traffic\", \"crashed\"\n",
      "- **Contradiction**: Positive words describe objectively negative situations\n",
      "\n",
      "## 3. Method\n",
      "\n",
      "### Experimental Approach\n",
      "We used **differential activation analysis** to identify components causally important for sarcasm detection.\n",
      "\n",
      "#### Step 1: Activation Collection\n",
      "- Ran GPT2-small on paired sarcastic/literal examples\n",
      "- Collected full activation cache for all layers and components\n",
      "- Used HookedTransformer for easy access to intermediate activations\n",
      "\n",
      "#### Step 2: Differential Analysis\n",
      "For each component (attention head or MLP):\n",
      "- Computed average activation on sarcastic examples\n",
      "- Computed average activation on literal examples  \n",
      "- Measured L2 norm of difference: `||mean_sarc - mean_lit||_2`\n",
      "- Higher difference indicates stronger sarcasm-specific processing\n",
      "\n",
      "#### Step 3: Component Selection\n",
      "- Ranked components by average differential activation\n",
      "- Selected top components within 11,200 dimension budget\n",
      "- Prioritized MLPs (768 dims each) over attention heads (64 dims each)\n",
      "\n",
      "### Technical Details\n",
      "\n",
      "**Model**: GPT2-small via HookedTransformer\n",
      "- 12 layers\n",
      "- 12 attention heads per layer\n",
      "- d_model = 768\n",
      "- d_head = 64\n",
      "\n",
      "**Write Budget Calculation**:\n",
      "- Input embedding: 768 dimensions\n",
      "- Each MLP layer: 768 dimensions\n",
      "- Each attention head: 64 dimensions  \n",
      "- Maximum budget: 11,200 dimensions\n",
      "\n",
      "**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\n",
      "\n",
      "## 4. Results\n",
      "\n",
      "### Circuit Composition\n",
      "\n",
      "**Total Components**: 54 (maximizing budget utilization)\n",
      "- Input: 1 (768 dims)\n",
      "- MLPs: 10 (7,680 dims)\n",
      "- Attention heads: 43 (2,752 dims)\n",
      "- **Total write cost**: 11,200 / 11,200 (100%)\n",
      "\n",
      "### MLP Components (Ranked by Importance)\n",
      "\n",
      "| Component | Avg Diff | Layer | Interpretation |\n",
      "|-----------|----------|-------|----------------|\n",
      "| m2 | 32.47 | 2 | **Primary sarcasm detector** |\n",
      "| m11 | 22.30 | 11 | Final pre-output processing |\n",
      "| m10 | 17.36 | 10 | Late-stage integration |\n",
      "| m9 | 13.41 | 9 | Late-stage integration |\n",
      "| m8 | 11.69 | 8 | Signal refinement |\n",
      "| m7 | 9.69 | 7 | Signal propagation |\n",
      "| m6 | 8.59 | 6 | Signal propagation |\n",
      "| m1 | 7.87 | 1 | Early context encoding |\n",
      "| m5 | 7.79 | 5 | Signal propagation |\n",
      "| m0 | 7.33 | 0 | Initial embedding processing |\n",
      "\n",
      "**Key Finding**: m2 shows **dramatically dominant** differential activation (32.47), ~45% stronger than the next strongest MLP. This suggests Layer 2 is the primary site of sarcasm/incongruity detection.\n",
      "\n",
      "### Attention Head Components\n",
      "\n",
      "**Top 10 Most Important Heads**:\n",
      "\n",
      "| Component | Avg Diff | Interpretation |\n",
      "|-----------|----------|----------------|\n",
      "| a11.h8 | 3.33 | Output integration head |\n",
      "| a11.h0 | 2.74 | Output integration head |\n",
      "| a4.h11 | 1.40 | Mid-layer information routing |\n",
      "| a9.h3 | 1.32 | Late propagation |\n",
      "| a6.h11 | 1.32 | Mid-layer integration |\n",
      "| a8.h5 | 1.31 | Late-stage processing |\n",
      "| a9.h10 | 1.29 | Late propagation |\n",
      "| a5.h3 | 1.28 | Mid-layer routing |\n",
      "| a10.h5 | 1.25 | Pre-output routing |\n",
      "| a11.h3 | 1.23 | Output integration |\n",
      "\n",
      "**Distribution by Layer**:\n",
      "- Layers 0-3: 9 heads (early processing)\n",
      "- Layers 4-7: 19 heads (dense middle routing)\n",
      "- Layers 8-11: 15 heads (late integration)\n",
      "\n",
      "### Excluded Components\n",
      "\n",
      "**MLPs excluded**: m3, m4\n",
      "- Showed minimal differential activation (<6.5)\n",
      "- Suggests these layers less involved in sarcasm processing\n",
      "\n",
      "**Attention heads excluded**: 101 heads\n",
      "- Lower differential activation (<0.83)\n",
      "- Likely performing general language modeling tasks\n",
      "\n",
      "## 5. Analysis\n",
      "\n",
      "### Hypothesis Evolution\n",
      "\n",
      "#### Phase 1: Initial Hypothesis\n",
      "We hypothesized a three-stage process:\n",
      "1. Early layers encode sentiment\n",
      "2. Middle layers detect incongruity\n",
      "3. Late layers reverse meaning\n",
      "\n",
      "#### Phase 2: Revised Understanding\n",
      "Empirical evidence revealed:\n",
      "1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\n",
      "2. Middle layers **propagate** rather than detect sarcasm signal\n",
      "3. Late layers **integrate** rather than reverse sentiment\n",
      "\n",
      "### Mechanistic Interpretation\n",
      "\n",
      "**Stage 1: Early Detection (L0-L2)**\n",
      "- m2 detects incongruity between sentiment words and context\n",
      "- Processes patterns like: positive adjective + negative situation\n",
      "- Output: sarcasm signal that propagates to later layers\n",
      "\n",
      "**Stage 2: Distributed Propagation (L3-L7)**  \n",
      "- Mid-layer MLPs refine the sarcasm signal\n",
      "- 19 attention heads route information across sequence positions\n",
      "- Enables context-aware processing throughout the sentence\n",
      "\n",
      "**Stage 3: Final Integration (L8-L11)**\n",
      "- Late MLPs (especially m11) perform final processing\n",
      "- Layer 11 attention heads (a11.h8, a11.h0) integrate into output\n",
      "- Determines how sarcasm affects final token predictions\n",
      "\n",
      "### Comparison to IOI Circuit\n",
      "\n",
      "The sarcasm circuit differs from the Indirect Object Identification (IOI) circuit:\n",
      "\n",
      "| Aspect | IOI Circuit | Sarcasm Circuit |\n",
      "|--------|-------------|-----------------|\n",
      "| **Primary mechanism** | Name copying via attention | Incongruity detection via MLP |\n",
      "| **Key layer** | Later layers (9-11) | Early layer (2) |\n",
      "| **Circuit size** | Sparse (~10 components) | Dense (54 components) |\n",
      "| **Attention importance** | Dominant | Supporting |\n",
      "| **MLP importance** | Supporting | Dominant |\n",
      "\n",
      "This suggests **different linguistic tasks use different computational strategies** in transformers.\n",
      "\n",
      "## 6. Next Steps\n",
      "\n",
      "### Validation Experiments\n",
      "1. **Ablation testing**: Systematically remove components, measure impact\n",
      "2. **Intervention experiments**: Patch activations to test causality\n",
      "3. **Attention analysis**: Visualize patterns for key heads\n",
      "4. **Probing**: Train linear classifiers to detect sarcasm at each layer\n",
      "\n",
      "### Circuit Refinement\n",
      "1. Analyze all 40 examples (currently only 5 analyzed in detail)\n",
      "2. Test on real-world sarcasm dataset\n",
      "3. Identify minimal sufficient circuit via ablation\n",
      "4. Compare to human sarcasm judgments\n",
      "\n",
      "### Mechanistic Deep Dive\n",
      "1. **m2 analysis**: What features does it compute? \n",
      "2. **Attention patterns**: How does information flow through 43 heads?\n",
      "3. **Interaction effects**: Do components work synergistically?\n",
      "4. **Generalization**: Does circuit transfer to other incongruity tasks?\n",
      "\n",
      "### Open Questions\n",
      "1. Why is m2 so dominant? What about Layer 2 enables incongruity detection?\n",
      "2. Are m3 and m4 intentionally bypassed, or do they serve other functions?\n",
      "3. How do the 43 attention heads divide labor?\n",
      "4. Does the circuit generalize to irony, understatement, and other figurative language?\n",
      "\n",
      "## 7. Main Takeaways\n",
      "\n",
      "### Scientific Insights\n",
      "\n",
      "1. **Sarcasm detection is early**: The network decides at Layer 2, not gradually\n",
      "2. **MLPs dominate**: 10 MLPs contribute 7,680 dims vs. 43 heads contributing 2,752 dims\n",
      "3. **Distributed but hierarchical**: 54 components work in coordinated stages\n",
      "4. **Task-specific architecture**: Different from other documented circuits like IOI\n",
      "\n",
      "### Implications for Interpretability\n",
      "\n",
      "1. **Component specialization**: Different layers specialize in different aspects\n",
      "2. **Non-obvious mechanisms**: Detection happens earlier than linguistically expected\n",
      "3. **Redundancy**: Circuit uses most available budget, suggesting distributed computation\n",
      "4. **Hierarchy matters**: Three-stage processing suggests compositional computation\n",
      "\n",
      "### Practical Applications\n",
      "\n",
      "1. **Sarcasm detection systems**: Focus on early-layer representations\n",
      "2. **Model editing**: m2 could be target for intervention\n",
      "3. **Probing methods**: Layer 2 most informative for sarcasm classification\n",
      "4. **Model design**: Early layers need capacity for complex semantic tasks\n",
      "\n",
      "## 8. Limitations\n",
      "\n",
      "1. **Small dataset**: Only 5 pairs analyzed in detail (40 examples total available)\n",
      "2. **Synthetic data**: Real-world sarcasm may have different patterns\n",
      "3. **No causal validation**: Differential activation ≠ causal importance\n",
      "4. **Single model**: Results specific to GPT2-small\n",
      "5. **Budget maximization**: Used full 11,200 dims; minimal circuit likely smaller\n",
      "6. **No behavioral testing**: Haven't verified circuit reproduces sarcasm detection\n",
      "\n",
      "## 9. Conclusion\n",
      "\n",
      "We identified a 54-component circuit in GPT2-small for sarcasm detection, utilizing the full 11,200 dimension write budget. The circuit exhibits a three-stage hierarchical structure with early detection (Layer 2 MLP), distributed propagation (mid-layer attention and MLPs), and final integration (late-layer components, especially Layer 11 attention heads).\n",
      "\n",
      "The dominant role of m2 (32.47 differential activation) reveals that sarcasm detection occurs remarkably early in the network, earlier than initial linguistic hypotheses suggested. This finding has implications for interpretability research, model editing, and understanding how transformers process complex pragmatic meaning beyond literal semantics.\n",
      "\n",
      "The circuit demonstrates that different linguistic tasks may require fundamentally different computational strategies within the same architecture, with sarcasm relying heavily on MLP-based pattern detection rather than attention-based information routing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let me read the full documentation to answer the questions\n",
    "print(\"=== FULL DOCUMENTATION ===\\n\")\n",
    "print(documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba300cc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXAM QUESTIONS ===\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"What is the dimension of a single attention head (d_head) in GPT2-small as used in this sarcasm circuit analysis?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. 32 dimensions\",\n",
      "    \"B. 128 dimensions\",\n",
      "    \"C. 768 dimensions\",\n",
      "    \"D. 64 dimensions\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 2\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"How many total components are included in the identified sarcasm detection circuit?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. 44 components (1 input + 12 MLPs + 31 attention heads)\",\n",
      "    \"B. 54 components (1 input + 10 MLPs + 43 attention heads)\",\n",
      "    \"C. 64 components (1 input + 12 MLPs + 51 attention heads)\",\n",
      "    \"D. 24 components (1 input + 10 MLPs + 13 attention heads)\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 3\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"Which MLP layer is identified as the primary sarcasm detector with the highest differential activation?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. m0 (Layer 0 MLP) with 7.33 average differential activation\",\n",
      "    \"B. m2 (Layer 2 MLP) with 32.47 average differential activation\",\n",
      "    \"C. m11 (Layer 11 MLP) with 22.30 average differential activation\",\n",
      "    \"D. m5 (Layer 5 MLP) with 7.79 average differential activation\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 4\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Which two MLP layers were excluded from the sarcasm circuit? Explain why they were excluded based on the documentation.\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 5\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"The initial hypothesis suggested that sarcasm detection follows a three-stage process: sentiment encoding \\u2192 incongruity detection \\u2192 meaning reversal. How did the empirical findings revise this understanding? Explain the key differences between the original hypothesis and the observed mechanism.\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 6\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"How does the sarcasm circuit differ from the Indirect Object Identification (IOI) circuit in terms of the dominant component type?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. Both circuits are equally balanced between MLPs and attention\",\n",
      "    \"B. IOI circuit is MLP-dominant while sarcasm circuit is attention-dominant\",\n",
      "    \"C. Both circuits are primarily attention-dominant\",\n",
      "    \"D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 7\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Identify the two most important attention heads in the sarcasm circuit based on differential activation. What is their interpreted function according to the documentation?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 8\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Based on the documentation, explain the key linguistic features that characterize sarcastic sentences in the dataset. How does the combination of these features create the contradiction that the circuit must detect?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 9\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"If you wanted to include all 12 MLPs and all 144 attention heads (12 layers \\u00d7 12 heads) in a circuit for GPT2-small, calculate the total write cost. Would this exceed the 11,200 dimension budget? Show your calculation.\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 10\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"According to the revised mechanistic model, what is the primary function of the middle layers (L3-L7) in the sarcasm circuit?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. Primary incongruity detection - identifying contradictions between sentiment and context\",\n",
      "    \"B. Sentiment encoding - detecting and encoding literal sentiment words\",\n",
      "    \"C. Distributed propagation - refining and routing the sarcasm signal across sequence positions\",\n",
      "    \"D. Meaning reversal - flipping sentiment polarity when sarcasm is detected\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 11\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"question\": \"What normalization technique was used to handle variable-length inputs when computing differential activations?\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null,\n",
      "  \"choice\": [\n",
      "    \"A. Max pooling over sequence positions\",\n",
      "    \"B. Averaged activations over sequence positions (mean over sequence dimension)\",\n",
      "    \"C. Used only the last token's activation\",\n",
      "    \"D. Padded all sequences to the same length\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 12\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"The documentation lists several limitations of the study. Why is the distinction between 'differential activation' and 'causal importance' considered a significant limitation? Explain what additional experiments would be needed to establish causal importance.\",\n",
      "  \"requires_code\": false,\n",
      "  \"code_id\": null\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 13\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Write code to verify the write budget calculation for the sarcasm circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads), compute the total write cost and verify it matches the documented 11,200 dimension budget.\\n\\nYour code should:\\n1. Define the dimension sizes (d_model=768 for input/MLPs, d_head=64 for attention heads)\\n2. Calculate the individual costs for each component type\\n3. Calculate the total write cost\\n4. Print the breakdown and verify it equals 11,200\\n\\nExpected output should show the individual costs and confirm the total equals 11,200.\",\n",
      "  \"requires_code\": true,\n",
      "  \"code_id\": \"CQ1\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 14\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Write code to analyze the distribution of the 43 attention heads in the sarcasm circuit across the 12 layers (0-11). \\n\\nGiven the list of attention heads in the circuit (from the JSON file or as provided below), your code should:\\n1. Parse the attention head names to extract layer numbers\\n2. Count the number of heads per layer\\n3. Group layers into three stages:\\n   - Early (L0-L3)\\n   - Middle (L4-L7)  \\n   - Late (L8-L11)\\n4. Print the count per layer and the total per stage\\n5. Verify the stage totals match the documentation: Early=9, Middle=19, Late=15\\n\\nAttention heads in circuit: ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\",\n",
      "  \"requires_code\": true,\n",
      "  \"code_id\": \"CQ2\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Question 15\n",
      "================================================================================\n",
      "{\n",
      "  \"question_type\": \"free_generation\",\n",
      "  \"question\": \"Write code to analyze the relative contribution of MLPs versus attention heads to the sarcasm circuit in terms of dimensions.\\n\\nYour code should:\\n1. Calculate the total dimensions from MLPs (10 MLPs \\u00d7 768 dims)\\n2. Calculate the total dimensions from attention heads (43 heads \\u00d7 64 dims)\\n3. Calculate the percentage of total circuit dimensions (excluding input) contributed by:\\n   - MLPs\\n   - Attention heads\\n4. Print the results and explain what this ratio tells us about whether the circuit is \\\"MLP-dominant\\\" or \\\"attention-dominant\\\"\\n\\nNote: Exclude the input embedding (768 dims) from the percentage calculation since it's always required.\\n\\nExpected conclusion: The circuit should be MLP-dominant with MLPs contributing ~74% of non-input dimensions.\",\n",
      "  \"requires_code\": true,\n",
      "  \"code_id\": \"CQ3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let me also view the full questions\n",
    "print(\"=== EXAM QUESTIONS ===\\n\")\n",
    "for i, q in enumerate(exam_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(json.dumps(q, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ad2e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 1\n",
    "\n",
    "**Question:** What is the dimension of a single attention head (d_head) in GPT2-small as used in this sarcasm circuit analysis?\n",
    "\n",
    "**Choices:**\n",
    "- A. 32 dimensions\n",
    "- B. 128 dimensions\n",
    "- C. 768 dimensions\n",
    "- D. 64 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5906a01b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: d_head dimension\n",
      "\n",
      "From documentation section 'Technical Details':\n",
      "- d_model = 768\n",
      "- d_head = 64\n",
      "\n",
      "Answer: D. 64 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Find d_head from documentation\n",
    "# From documentation: \"d_head = 64\"\n",
    "\n",
    "print(\"Question 1: d_head dimension\")\n",
    "print(\"\\nFrom documentation section 'Technical Details':\")\n",
    "print(\"- d_model = 768\")\n",
    "print(\"- d_head = 64\")\n",
    "print(\"\\nAnswer: D. 64 dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d00fc7",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation explicitly states in the \"Technical Details\" section that GPT2-small has d_head = 64 dimensions. It also mentions in the \"Write Budget Calculation\" that \"Each attention head: 64 dimensions\".\n",
    "\n",
    "**Answer:** D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a154782",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 2\n",
    "\n",
    "**Question:** How many total components are included in the identified sarcasm detection circuit?\n",
    "\n",
    "**Choices:**\n",
    "- A. 44 components (1 input + 12 MLPs + 31 attention heads)\n",
    "- B. 54 components (1 input + 10 MLPs + 43 attention heads)\n",
    "- C. 64 components (1 input + 12 MLPs + 51 attention heads)\n",
    "- D. 24 components (1 input + 10 MLPs + 13 attention heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a60b77",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2: Total components in circuit\n",
      "\n",
      "From documentation 'Circuit Composition' section:\n",
      "Total Components: 54 (maximizing budget utilization)\n",
      "- Input: 1 (768 dims)\n",
      "- MLPs: 10 (7,680 dims)\n",
      "- Attention heads: 43 (2,752 dims)\n",
      "\n",
      "Answer: B. 54 components (1 input + 10 MLPs + 43 attention heads)\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Total components\n",
    "print(\"Question 2: Total components in circuit\")\n",
    "print(\"\\nFrom documentation 'Circuit Composition' section:\")\n",
    "print(\"Total Components: 54 (maximizing budget utilization)\")\n",
    "print(\"- Input: 1 (768 dims)\")\n",
    "print(\"- MLPs: 10 (7,680 dims)\")\n",
    "print(\"- Attention heads: 43 (2,752 dims)\")\n",
    "print(\"\\nAnswer: B. 54 components (1 input + 10 MLPs + 43 attention heads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92bc62c",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation clearly states in the \"Circuit Composition\" section that the total components are 54, broken down as: Input (1) + MLPs (10) + Attention heads (43). This equals 1 + 10 + 43 = 54 components.\n",
    "\n",
    "**Answer:** B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f4847",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 3\n",
    "\n",
    "**Question:** Which MLP layer is identified as the primary sarcasm detector with the highest differential activation?\n",
    "\n",
    "**Choices:**\n",
    "- A. m0 (Layer 0 MLP) with 7.33 average differential activation\n",
    "- B. m2 (Layer 2 MLP) with 32.47 average differential activation\n",
    "- C. m11 (Layer 11 MLP) with 22.30 average differential activation\n",
    "- D. m5 (Layer 5 MLP) with 7.79 average differential activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf578ada",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3: Primary sarcasm detector\n",
      "\n",
      "From documentation 'MLP Components (Ranked by Importance)':\n",
      "m2 | 32.47 | 2 | **Primary sarcasm detector**\n",
      "\n",
      "Key Finding: m2 shows **dramatically dominant** differential activation (32.47),\n",
      "~45% stronger than the next strongest MLP.\n",
      "\n",
      "Answer: B. m2 (Layer 2 MLP) with 32.47 average differential activation\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Primary sarcasm detector\n",
    "print(\"Question 3: Primary sarcasm detector\")\n",
    "print(\"\\nFrom documentation 'MLP Components (Ranked by Importance)':\")\n",
    "print(\"m2 | 32.47 | 2 | **Primary sarcasm detector**\")\n",
    "print(\"\\nKey Finding: m2 shows **dramatically dominant** differential activation (32.47),\")\n",
    "print(\"~45% stronger than the next strongest MLP.\")\n",
    "print(\"\\nAnswer: B. m2 (Layer 2 MLP) with 32.47 average differential activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308db3e",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation explicitly identifies m2 as the \"Primary sarcasm detector\" in the MLP Components table, with the highest differential activation of 32.47. The documentation notes that this is \"dramatically dominant\" and \"~45% stronger than the next strongest MLP\" (m11 with 22.30).\n",
    "\n",
    "**Answer:** B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e148fa",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 4\n",
    "\n",
    "**Question:** Which two MLP layers were excluded from the sarcasm circuit? Explain why they were excluded based on the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019941a2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4: Excluded MLP layers\n",
      "\n",
      "From documentation 'Excluded Components' section:\n",
      "\n",
      "**MLPs excluded**: m3, m4\n",
      "- Showed minimal differential activation (<6.5)\n",
      "- Suggests these layers less involved in sarcasm processing\n",
      "\n",
      "Total MLPs in GPT2-small: 12 (m0 through m11)\n",
      "MLPs included: 10\n",
      "MLPs excluded: 2 (m3 and m4)\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Excluded MLPs\n",
    "print(\"Question 4: Excluded MLP layers\")\n",
    "print(\"\\nFrom documentation 'Excluded Components' section:\")\n",
    "print(\"\\n**MLPs excluded**: m3, m4\")\n",
    "print(\"- Showed minimal differential activation (<6.5)\")\n",
    "print(\"- Suggests these layers less involved in sarcasm processing\")\n",
    "print(\"\\nTotal MLPs in GPT2-small: 12 (m0 through m11)\")\n",
    "print(\"MLPs included: 10\")\n",
    "print(\"MLPs excluded: 2 (m3 and m4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c967bf",
   "metadata": {},
   "source": [
    "**Reasoning:** According to the documentation's \"Excluded Components\" section, m3 and m4 are the two MLP layers that were excluded from the sarcasm circuit. The reason given is that they \"showed minimal differential activation (<6.5)\", which suggests these layers are less involved in sarcasm processing compared to the other MLPs that were included in the circuit.\n",
    "\n",
    "**Answer:** The two excluded MLP layers are m3 and m4. They were excluded because they showed minimal differential activation (less than 6.5), suggesting these layers are less involved in sarcasm processing compared to the 10 MLPs that were included in the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299a637",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 5\n",
    "\n",
    "**Question:** The initial hypothesis suggested that sarcasm detection follows a three-stage process: sentiment encoding → incongruity detection → meaning reversal. How did the empirical findings revise this understanding? Explain the key differences between the original hypothesis and the observed mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b21751",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5: Revision of initial hypothesis\n",
      "\n",
      "=== Phase 1: Initial Hypothesis ===\n",
      "1. Early layers encode sentiment\n",
      "2. Middle layers detect incongruity\n",
      "3. Late layers reverse meaning\n",
      "\n",
      "=== Phase 2: Revised Understanding (Empirical Evidence) ===\n",
      "1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\n",
      "2. Middle layers **propagate** rather than detect sarcasm signal\n",
      "3. Late layers **integrate** rather than reverse sentiment\n",
      "\n",
      "=== Key Differences ===\n",
      "- Timing: Detection happens at Layer 2 (early), not middle layers\n",
      "- Middle layer function: Propagation, not detection\n",
      "- Late layer function: Integration, not reversal\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Hypothesis evolution\n",
    "print(\"Question 5: Revision of initial hypothesis\")\n",
    "print(\"\\n=== Phase 1: Initial Hypothesis ===\")\n",
    "print(\"1. Early layers encode sentiment\")\n",
    "print(\"2. Middle layers detect incongruity\")\n",
    "print(\"3. Late layers reverse meaning\")\n",
    "print(\"\\n=== Phase 2: Revised Understanding (Empirical Evidence) ===\")\n",
    "print(\"1. **Layer 2 MLP (m2) is primary detector** - earlier than expected\")\n",
    "print(\"2. Middle layers **propagate** rather than detect sarcasm signal\")\n",
    "print(\"3. Late layers **integrate** rather than reverse sentiment\")\n",
    "print(\"\\n=== Key Differences ===\")\n",
    "print(\"- Timing: Detection happens at Layer 2 (early), not middle layers\")\n",
    "print(\"- Middle layer function: Propagation, not detection\")\n",
    "print(\"- Late layer function: Integration, not reversal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a6f6",
   "metadata": {},
   "source": [
    "**Reasoning:** According to the documentation's \"Hypothesis Evolution\" section, the initial hypothesis proposed a three-stage process where early layers encode sentiment, middle layers detect incongruity, and late layers reverse meaning. However, empirical evidence revealed three key revisions:\n",
    "\n",
    "1. **Timing of detection**: The primary incongruity detection happens at Layer 2 MLP (m2), which is much earlier than the hypothesized middle layers\n",
    "2. **Middle layer function**: Instead of detecting incongruity, middle layers (L3-L7) primarily propagate and refine the sarcasm signal\n",
    "3. **Late layer function**: Rather than reversing sentiment, late layers (L8-L11) integrate the sarcasm information into the final output\n",
    "\n",
    "**Answer:** The empirical findings revised the initial hypothesis in three major ways: (1) Primary sarcasm/incongruity detection occurs at Layer 2 (earlier than expected), not in the middle layers; (2) Middle layers propagate rather than detect the sarcasm signal; (3) Late layers integrate rather than reverse sentiment. This means the network decides sarcasm at Layer 2, then propagates and refines that signal through the remaining layers, rather than gradually building up to a detection and reversal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0392c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 6\n",
    "\n",
    "**Question:** How does the sarcasm circuit differ from the Indirect Object Identification (IOI) circuit in terms of the dominant component type?\n",
    "\n",
    "**Choices:**\n",
    "- A. Both circuits are equally balanced between MLPs and attention\n",
    "- B. IOI circuit is MLP-dominant while sarcasm circuit is attention-dominant\n",
    "- C. Both circuits are primarily attention-dominant\n",
    "- D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ba2709",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6: Sarcasm vs IOI circuit comparison\n",
      "\n",
      "From documentation 'Comparison to IOI Circuit' table:\n",
      "\n",
      "| Aspect | IOI Circuit | Sarcasm Circuit |\n",
      "|--------|-------------|-----------------|\n",
      "| Attention importance | Dominant | Supporting |\n",
      "| MLP importance | Supporting | Dominant |\n",
      "\n",
      "Conclusion: Sarcasm circuit is MLP-dominant, IOI circuit is attention-dominant\n",
      "\n",
      "Answer: D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Comparison to IOI circuit\n",
    "print(\"Question 6: Sarcasm vs IOI circuit comparison\")\n",
    "print(\"\\nFrom documentation 'Comparison to IOI Circuit' table:\")\n",
    "print(\"\\n| Aspect | IOI Circuit | Sarcasm Circuit |\")\n",
    "print(\"|--------|-------------|-----------------|\")\n",
    "print(\"| Attention importance | Dominant | Supporting |\")\n",
    "print(\"| MLP importance | Supporting | Dominant |\")\n",
    "print(\"\\nConclusion: Sarcasm circuit is MLP-dominant, IOI circuit is attention-dominant\")\n",
    "print(\"\\nAnswer: D. Sarcasm circuit is MLP-dominant while IOI circuit is attention-dominant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c0013",
   "metadata": {},
   "source": [
    "**Reasoning:** According to the documentation's \"Comparison to IOI Circuit\" table, the sarcasm circuit has MLP importance as \"Dominant\" and attention importance as \"Supporting\", while the IOI circuit has the opposite pattern with attention importance as \"Dominant\" and MLP importance as \"Supporting\". This indicates that the sarcasm circuit is MLP-dominant while the IOI circuit is attention-dominant.\n",
    "\n",
    "**Answer:** D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce1016",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 7\n",
    "\n",
    "**Question:** Identify the two most important attention heads in the sarcasm circuit based on differential activation. What is their interpreted function according to the documentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dbc6da",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7: Two most important attention heads\n",
      "\n",
      "From documentation 'Top 10 Most Important Heads' table:\n",
      "\n",
      "1. a11.h8 - Avg Diff: 3.33 - Interpretation: Output integration head\n",
      "2. a11.h0 - Avg Diff: 2.74 - Interpretation: Output integration head\n",
      "\n",
      "Both are from Layer 11 and perform output integration functions.\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Top attention heads\n",
    "print(\"Question 7: Two most important attention heads\")\n",
    "print(\"\\nFrom documentation 'Top 10 Most Important Heads' table:\")\n",
    "print(\"\\n1. a11.h8 - Avg Diff: 3.33 - Interpretation: Output integration head\")\n",
    "print(\"2. a11.h0 - Avg Diff: 2.74 - Interpretation: Output integration head\")\n",
    "print(\"\\nBoth are from Layer 11 and perform output integration functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f007b",
   "metadata": {},
   "source": [
    "**Reasoning:** According to the documentation's \"Top 10 Most Important Heads\" table, the two attention heads with the highest differential activation are a11.h8 (3.33 average differential activation) and a11.h0 (2.74 average differential activation). Both heads are from Layer 11, and the documentation interprets their function as \"Output integration head\".\n",
    "\n",
    "**Answer:** The two most important attention heads are a11.h8 (differential activation: 3.33) and a11.h0 (differential activation: 2.74). Their interpreted function is \"output integration\" - they integrate the sarcasm information into the final output at Layer 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a5a9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 8\n",
    "\n",
    "**Question:** Based on the documentation, explain the key linguistic features that characterize sarcastic sentences in the dataset. How does the combination of these features create the contradiction that the circuit must detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bb595e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8: Key linguistic features of sarcasm\n",
      "\n",
      "From documentation 'Key Linguistic Features of Sarcasm' section:\n",
      "\n",
      "1. **Discourse markers**: 'Oh', 'Wow', 'Just' (emphasis particles)\n",
      "2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\n",
      "3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\n",
      "4. **Contradiction**: Positive words describe objectively negative situations\n",
      "\n",
      "Example: 'Oh great, another meeting at 7 AM.'\n",
      "- Discourse marker: 'Oh'\n",
      "- Positive word: 'great'\n",
      "- Negative context: 'another meeting at 7 AM' (undesirable)\n",
      "- Contradiction: Positive 'great' describes negative situation\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Linguistic features of sarcasm\n",
    "print(\"Question 8: Key linguistic features of sarcasm\")\n",
    "print(\"\\nFrom documentation 'Key Linguistic Features of Sarcasm' section:\")\n",
    "print(\"\\n1. **Discourse markers**: 'Oh', 'Wow', 'Just' (emphasis particles)\")\n",
    "print(\"2. **Positive sentiment words**: 'great', 'love', 'fantastic', 'wonderful', 'perfect'\")\n",
    "print(\"3. **Negative situational context**: 'another meeting', 'stuck in traffic', 'crashed'\")\n",
    "print(\"4. **Contradiction**: Positive words describe objectively negative situations\")\n",
    "print(\"\\nExample: 'Oh great, another meeting at 7 AM.'\")\n",
    "print(\"- Discourse marker: 'Oh'\")\n",
    "print(\"- Positive word: 'great'\")\n",
    "print(\"- Negative context: 'another meeting at 7 AM' (undesirable)\")\n",
    "print(\"- Contradiction: Positive 'great' describes negative situation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6982ca7",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation lists four key linguistic features that characterize sarcastic sentences in the \"Key Linguistic Features of Sarcasm\" section:\n",
    "\n",
    "1. Discourse markers (emphasis particles like \"Oh\", \"Wow\", \"Just\")\n",
    "2. Positive sentiment words (\"great\", \"love\", \"fantastic\", \"wonderful\", \"perfect\")\n",
    "3. Negative situational context (\"another meeting\", \"stuck in traffic\", \"crashed\")\n",
    "4. Contradiction between positive words and negative situations\n",
    "\n",
    "The contradiction is created when positive sentiment words are used to describe objectively negative situations. For example, in \"Oh great, another meeting at 7 AM,\" the positive word \"great\" is applied to an undesirable situation (early morning meeting), creating an incongruity that signals sarcasm.\n",
    "\n",
    "**Answer:** The key linguistic features are: (1) discourse markers like \"Oh\", \"Wow\", \"Just\"; (2) positive sentiment words like \"great\", \"love\", \"fantastic\"; (3) negative situational context like \"stuck in traffic\", \"crashed\"; and (4) contradiction. The combination creates the contradiction by using positive sentiment words to describe objectively negative situations - for example, saying something is \"great\" when describing being stuck in traffic. This incongruity between positive words and negative context is what the circuit must detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53e572",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 9\n",
    "\n",
    "**Question:** If you wanted to include all 12 MLPs and all 144 attention heads (12 layers × 12 heads) in a circuit for GPT2-small, calculate the total write cost. Would this exceed the 11,200 dimension budget? Show your calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947c5349",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 9: Total write cost for all components\n",
      "\n",
      "Given information from documentation:\n",
      "- GPT2-small has 12 layers\n",
      "- Each layer has 12 attention heads\n",
      "- Each MLP: 768 dimensions\n",
      "- Each attention head: 64 dimensions\n",
      "- Input embedding: 768 dimensions\n",
      "- Budget: 11,200 dimensions\n",
      "\n",
      "Calculation:\n",
      "Input embedding: 768 dims\n",
      "All 12 MLPs: 12 × 768 = 9216 dims\n",
      "All 144 attention heads: 12 × 12 × 64 = 9216 dims\n",
      "\n",
      "Total write cost: 768 + 9216 + 9216 = 19200 dims\n",
      "Budget: 11,200 dims\n",
      "\n",
      "Exceeds budget? True\n",
      "Difference: 8000 dims over budget\n"
     ]
    }
   ],
   "source": [
    "# Question 9: Calculate full circuit cost\n",
    "print(\"Question 9: Total write cost for all components\")\n",
    "print(\"\\nGiven information from documentation:\")\n",
    "print(\"- GPT2-small has 12 layers\")\n",
    "print(\"- Each layer has 12 attention heads\")\n",
    "print(\"- Each MLP: 768 dimensions\")\n",
    "print(\"- Each attention head: 64 dimensions\")\n",
    "print(\"- Input embedding: 768 dimensions\")\n",
    "print(\"- Budget: 11,200 dimensions\")\n",
    "\n",
    "print(\"\\nCalculation:\")\n",
    "input_cost = 768\n",
    "mlp_cost = 12 * 768\n",
    "attn_cost = 144 * 64  # 12 layers × 12 heads × 64 dims\n",
    "total_cost = input_cost + mlp_cost + attn_cost\n",
    "\n",
    "print(f\"Input embedding: {input_cost} dims\")\n",
    "print(f\"All 12 MLPs: 12 × 768 = {mlp_cost} dims\")\n",
    "print(f\"All 144 attention heads: 12 × 12 × 64 = {attn_cost} dims\")\n",
    "print(f\"\\nTotal write cost: {input_cost} + {mlp_cost} + {attn_cost} = {total_cost} dims\")\n",
    "print(f\"Budget: 11,200 dims\")\n",
    "print(f\"\\nExceeds budget? {total_cost > 11200}\")\n",
    "print(f\"Difference: {total_cost - 11200} dims over budget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaede7",
   "metadata": {},
   "source": [
    "**Reasoning:** Based on the documentation's technical details, GPT2-small has 12 layers with 12 attention heads each (144 total heads). Each MLP costs 768 dimensions, each attention head costs 64 dimensions, and the input embedding costs 768 dimensions.\n",
    "\n",
    "Calculation:\n",
    "- Input embedding: 768 dims\n",
    "- All 12 MLPs: 12 × 768 = 9,216 dims\n",
    "- All 144 attention heads: 12 × 12 × 64 = 9,216 dims\n",
    "- Total: 768 + 9,216 + 9,216 = 19,200 dims\n",
    "\n",
    "The budget is 11,200 dimensions.\n",
    "\n",
    "**Answer:** Yes, including all 12 MLPs and all 144 attention heads would result in a total write cost of 19,200 dimensions (768 for input + 9,216 for MLPs + 9,216 for attention heads), which exceeds the 11,200 dimension budget by 8,000 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c0b1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 10\n",
    "\n",
    "**Question:** According to the revised mechanistic model, what is the primary function of the middle layers (L3-L7) in the sarcasm circuit?\n",
    "\n",
    "**Choices:**\n",
    "- A. Primary incongruity detection - identifying contradictions between sentiment and context\n",
    "- B. Sentiment encoding - detecting and encoding literal sentiment words\n",
    "- C. Distributed propagation - refining and routing the sarcasm signal across sequence positions\n",
    "- D. Meaning reversal - flipping sentiment polarity when sarcasm is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3714d1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 10: Function of middle layers (L3-L7)\n",
      "\n",
      "From documentation 'Mechanistic Interpretation' section:\n",
      "\n",
      "**Stage 2: Distributed Propagation (L3-L7)**\n",
      "- Mid-layer MLPs refine the sarcasm signal\n",
      "- 19 attention heads route information across sequence positions\n",
      "- Enables context-aware processing throughout the sentence\n",
      "\n",
      "From 'Hypothesis Evolution - Phase 2: Revised Understanding':\n",
      "2. Middle layers **propagate** rather than detect sarcasm signal\n",
      "\n",
      "Answer: C. Distributed propagation - refining and routing the sarcasm signal\n"
     ]
    }
   ],
   "source": [
    "# Question 10: Middle layer function\n",
    "print(\"Question 10: Function of middle layers (L3-L7)\")\n",
    "print(\"\\nFrom documentation 'Mechanistic Interpretation' section:\")\n",
    "print(\"\\n**Stage 2: Distributed Propagation (L3-L7)**\")\n",
    "print(\"- Mid-layer MLPs refine the sarcasm signal\")\n",
    "print(\"- 19 attention heads route information across sequence positions\")\n",
    "print(\"- Enables context-aware processing throughout the sentence\")\n",
    "print(\"\\nFrom 'Hypothesis Evolution - Phase 2: Revised Understanding':\")\n",
    "print(\"2. Middle layers **propagate** rather than detect sarcasm signal\")\n",
    "print(\"\\nAnswer: C. Distributed propagation - refining and routing the sarcasm signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6445f",
   "metadata": {},
   "source": [
    "**Reasoning:** According to the documentation's \"Mechanistic Interpretation\" section, Stage 2 (L3-L7) is described as \"Distributed Propagation\" where mid-layer MLPs refine the sarcasm signal and attention heads route information across sequence positions. The \"Hypothesis Evolution\" section explicitly states that empirical evidence showed middle layers \"propagate rather than detect sarcasm signal.\"\n",
    "\n",
    "**Answer:** C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa86987",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 11\n",
    "\n",
    "**Question:** What normalization technique was used to handle variable-length inputs when computing differential activations?\n",
    "\n",
    "**Choices:**\n",
    "- A. Max pooling over sequence positions\n",
    "- B. Averaged activations over sequence positions (mean over sequence dimension)\n",
    "- C. Used only the last token's activation\n",
    "- D. Padded all sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13b8d13",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 11: Normalization technique for variable-length inputs\n",
      "\n",
      "From documentation 'Technical Details' section:\n",
      "\n",
      "**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\n",
      "\n",
      "Answer: B. Averaged activations over sequence positions (mean over sequence dimension)\n"
     ]
    }
   ],
   "source": [
    "# Question 11: Normalization technique\n",
    "print(\"Question 11: Normalization technique for variable-length inputs\")\n",
    "print(\"\\nFrom documentation 'Technical Details' section:\")\n",
    "print(\"\\n**Normalization**: Averaged activations over sequence positions to handle variable-length inputs\")\n",
    "print(\"\\nAnswer: B. Averaged activations over sequence positions (mean over sequence dimension)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923f47d",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation's \"Technical Details\" section explicitly states: \"Normalization: Averaged activations over sequence positions to handle variable-length inputs.\" This indicates that the mean over the sequence dimension was used.\n",
    "\n",
    "**Answer:** B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb5d7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 12\n",
    "\n",
    "**Question:** The documentation lists several limitations of the study. Why is the distinction between 'differential activation' and 'causal importance' considered a significant limitation? Explain what additional experiments would be needed to establish causal importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec0a15e0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 12: Limitation of differential activation analysis\n",
      "\n",
      "From documentation 'Limitations' section:\n",
      "3. **No causal validation**: Differential activation ≠ causal importance\n",
      "\n",
      "From 'Next Steps - Validation Experiments' section:\n",
      "1. **Ablation testing**: Systematically remove components, measure impact\n",
      "2. **Intervention experiments**: Patch activations to test causality\n",
      "\n",
      "Interpretation:\n",
      "- Differential activation shows CORRELATION (components that activate differently)\n",
      "- Causal importance shows CAUSATION (components that are necessary for the behavior)\n",
      "- A component could activate differently but not be causally important\n",
      "- Need ablation/intervention to establish causality\n"
     ]
    }
   ],
   "source": [
    "# Question 12: Differential activation vs causal importance\n",
    "print(\"Question 12: Limitation of differential activation analysis\")\n",
    "print(\"\\nFrom documentation 'Limitations' section:\")\n",
    "print(\"3. **No causal validation**: Differential activation ≠ causal importance\")\n",
    "print(\"\\nFrom 'Next Steps - Validation Experiments' section:\")\n",
    "print(\"1. **Ablation testing**: Systematically remove components, measure impact\")\n",
    "print(\"2. **Intervention experiments**: Patch activations to test causality\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Differential activation shows CORRELATION (components that activate differently)\")\n",
    "print(\"- Causal importance shows CAUSATION (components that are necessary for the behavior)\")\n",
    "print(\"- A component could activate differently but not be causally important\")\n",
    "print(\"- Need ablation/intervention to establish causality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe38d1",
   "metadata": {},
   "source": [
    "**Reasoning:** The documentation lists \"No causal validation: Differential activation ≠ causal importance\" as a limitation. This is significant because differential activation only measures correlation - it shows which components activate differently on sarcastic vs literal examples, but doesn't prove these components are actually necessary for sarcasm detection. A component could have high differential activation but be irrelevant to the actual computation.\n",
    "\n",
    "The documentation suggests two types of experiments in the \"Validation Experiments\" section to establish causal importance:\n",
    "1. Ablation testing - systematically removing components and measuring the impact on sarcasm detection performance\n",
    "2. Intervention experiments - patching activations to directly test causality\n",
    "\n",
    "**Answer:** The distinction is significant because differential activation only shows correlation (which components activate differently), not causation (which components are actually necessary for sarcasm detection). A component could activate differently without being causally important for the behavior. To establish causal importance, the documentation proposes: (1) ablation testing - systematically removing components and measuring impact on sarcasm detection, and (2) intervention experiments - patching activations to directly test causality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148f84b",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 13 (Code Required)\n",
    "\n",
    "**Question:** Write code to verify the write budget calculation for the sarcasm circuit. Given the circuit composition (1 input embedding, 10 MLPs, 43 attention heads), compute the total write cost and verify it matches the documented 11,200 dimension budget.\n",
    "\n",
    "Your code should:\n",
    "1. Define the dimension sizes (d_model=768 for input/MLPs, d_head=64 for attention heads)\n",
    "2. Calculate the individual costs for each component type\n",
    "3. Calculate the total write cost\n",
    "4. Print the breakdown and verify it equals 11,200\n",
    "\n",
    "Expected output should show the individual costs and confirm the total equals 11,200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb02a021",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WRITE BUDGET VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Component Dimensions:\n",
      "  d_model (input/MLP): 768 dimensions\n",
      "  d_head (attention):  64 dimensions\n",
      "\n",
      "Circuit Composition:\n",
      "  Input embeddings:    1\n",
      "  MLP layers:          10\n",
      "  Attention heads:     43\n",
      "\n",
      "Cost Breakdown:\n",
      "  Input:      1 × 768 = 768 dims\n",
      "  MLPs:       10 × 768 = 7,680 dims\n",
      "  Attn heads: 43 × 64 = 2,752 dims\n",
      "  ---------------------------------------------\n",
      "  TOTAL:                      11,200 dims\n",
      "\n",
      "Verification:\n",
      "  Expected budget:  11,200 dims\n",
      "  Calculated total: 11,200 dims\n",
      "  Match: True ✓\n",
      "  Budget utilization: 100.0%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 13: Verify write budget calculation\n",
    "\n",
    "# Define dimension sizes from documentation\n",
    "d_model = 768  # for input embedding and MLPs\n",
    "d_head = 64    # for attention heads\n",
    "\n",
    "# Circuit composition from documentation\n",
    "num_input = 1\n",
    "num_mlps = 10\n",
    "num_attn_heads = 43\n",
    "\n",
    "# Calculate individual costs\n",
    "input_cost = num_input * d_model\n",
    "mlp_cost = num_mlps * d_model\n",
    "attn_cost = num_attn_heads * d_head\n",
    "\n",
    "# Calculate total\n",
    "total_write_cost = input_cost + mlp_cost + attn_cost\n",
    "\n",
    "# Expected budget\n",
    "expected_budget = 11200\n",
    "\n",
    "# Print breakdown\n",
    "print(\"=\" * 60)\n",
    "print(\"WRITE BUDGET VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nComponent Dimensions:\")\n",
    "print(f\"  d_model (input/MLP): {d_model} dimensions\")\n",
    "print(f\"  d_head (attention):  {d_head} dimensions\")\n",
    "\n",
    "print(\"\\nCircuit Composition:\")\n",
    "print(f\"  Input embeddings:    {num_input}\")\n",
    "print(f\"  MLP layers:          {num_mlps}\")\n",
    "print(f\"  Attention heads:     {num_attn_heads}\")\n",
    "\n",
    "print(\"\\nCost Breakdown:\")\n",
    "print(f\"  Input:      {num_input} × {d_model} = {input_cost:,} dims\")\n",
    "print(f\"  MLPs:       {num_mlps} × {d_model} = {mlp_cost:,} dims\")\n",
    "print(f\"  Attn heads: {num_attn_heads} × {d_head} = {attn_cost:,} dims\")\n",
    "print(f\"  \" + \"-\" * 45)\n",
    "print(f\"  TOTAL:                      {total_write_cost:,} dims\")\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"  Expected budget:  {expected_budget:,} dims\")\n",
    "print(f\"  Calculated total: {total_write_cost:,} dims\")\n",
    "print(f\"  Match: {total_write_cost == expected_budget} ✓\" if total_write_cost == expected_budget else f\"  Match: {total_write_cost == expected_budget} ✗\")\n",
    "print(f\"  Budget utilization: {(total_write_cost / expected_budget * 100):.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a51033",
   "metadata": {},
   "source": [
    "**Reasoning:** Based on the documentation, the circuit has 1 input embedding (768 dims), 10 MLPs (768 dims each), and 43 attention heads (64 dims each). The calculation shows: 768 + (10 × 768) + (43 × 64) = 768 + 7,680 + 2,752 = 11,200 dimensions, which exactly matches the documented budget.\n",
    "\n",
    "**Answer:** The verification confirms that the total write cost is exactly 11,200 dimensions (768 for input + 7,680 for MLPs + 2,752 for attention heads), matching the documented budget with 100% utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46235b",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 14 (Code Required)\n",
    "\n",
    "**Question:** Write code to analyze the distribution of the 43 attention heads in the sarcasm circuit across the 12 layers (0-11).\n",
    "\n",
    "Given the list of attention heads in the circuit, your code should:\n",
    "1. Parse the attention head names to extract layer numbers\n",
    "2. Count the number of heads per layer\n",
    "3. Group layers into three stages:\n",
    "   - Early (L0-L3)\n",
    "   - Middle (L4-L7)\n",
    "   - Late (L8-L11)\n",
    "4. Print the count per layer and the total per stage\n",
    "5. Verify the stage totals match the documentation: Early=9, Middle=19, Late=15\n",
    "\n",
    "Attention heads in circuit: ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e4da0d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ATTENTION HEAD DISTRIBUTION BY LAYER\n",
      "============================================================\n",
      "\n",
      "Heads per layer:\n",
      "  Layer  0:  0 heads\n",
      "  Layer  1:  1 heads\n",
      "  Layer  2:  4 heads\n",
      "  Layer  3:  4 heads\n",
      "  Layer  4:  6 heads\n",
      "  Layer  5:  4 heads\n",
      "  Layer  6:  6 heads\n",
      "  Layer  7:  3 heads\n",
      "  Layer  8:  6 heads\n",
      "  Layer  9:  2 heads\n",
      "  Layer 10:  2 heads\n",
      "  Layer 11:  5 heads\n",
      "\n",
      "------------------------------------------------------------\n",
      "Stage Distribution:\n",
      "  Early (L0-L3):   9 heads\n",
      "  Middle (L4-L7):  19 heads\n",
      "  Late (L8-L11):   15 heads\n",
      "  Total:           43 heads\n",
      "\n",
      "------------------------------------------------------------\n",
      "Verification against documentation:\n",
      "  Early stage:  Expected=9, Actual=9, Match=True ✓\n",
      "  Middle stage: Expected=19, Actual=19, Match=True ✓\n",
      "  Late stage:   Expected=15, Actual=15, Match=True ✓\n",
      "  Total heads:  Expected=43, Actual=43 ✓\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 14: Analyze attention head distribution across layers\n",
    "\n",
    "attention_heads = ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', 'a9.h10', \n",
    "                   'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', 'a4.h9', 'a4.h7', \n",
    "                   'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', 'a4.h0', 'a2.h8', 'a5.h4', \n",
    "                   'a8.h10', 'a5.h7', 'a4.h1', 'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', \n",
    "                   'a8.h4', 'a3.h2', 'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', \n",
    "                   'a4.h3', 'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n",
    "\n",
    "# Step 1: Parse attention head names and extract layer numbers\n",
    "layer_counts = {i: 0 for i in range(12)}  # Initialize counts for layers 0-11\n",
    "\n",
    "for head in attention_heads:\n",
    "    # Format: a{layer}.h{head_num}\n",
    "    layer_num = int(head.split('.')[0][1:])  # Extract layer number after 'a'\n",
    "    layer_counts[layer_num] += 1\n",
    "\n",
    "# Step 2: Print count per layer\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION HEAD DISTRIBUTION BY LAYER\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nHeads per layer:\")\n",
    "for layer in range(12):\n",
    "    count = layer_counts[layer]\n",
    "    print(f\"  Layer {layer:2d}: {count:2d} heads\")\n",
    "\n",
    "# Step 3: Group into stages\n",
    "early_layers = range(0, 4)    # L0-L3\n",
    "middle_layers = range(4, 8)   # L4-L7\n",
    "late_layers = range(8, 12)    # L8-L11\n",
    "\n",
    "early_count = sum(layer_counts[l] for l in early_layers)\n",
    "middle_count = sum(layer_counts[l] for l in middle_layers)\n",
    "late_count = sum(layer_counts[l] for l in late_layers)\n",
    "\n",
    "# Step 4: Print stage totals\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Stage Distribution:\")\n",
    "print(f\"  Early (L0-L3):   {early_count} heads\")\n",
    "print(f\"  Middle (L4-L7):  {middle_count} heads\")\n",
    "print(f\"  Late (L8-L11):   {late_count} heads\")\n",
    "print(f\"  Total:           {early_count + middle_count + late_count} heads\")\n",
    "\n",
    "# Step 5: Verify against documentation\n",
    "expected_early = 9\n",
    "expected_middle = 19\n",
    "expected_late = 15\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Verification against documentation:\")\n",
    "print(f\"  Early stage:  Expected={expected_early}, Actual={early_count}, Match={early_count == expected_early} {'✓' if early_count == expected_early else '✗'}\")\n",
    "print(f\"  Middle stage: Expected={expected_middle}, Actual={middle_count}, Match={middle_count == expected_middle} {'✓' if middle_count == expected_middle else '✗'}\")\n",
    "print(f\"  Late stage:   Expected={expected_late}, Actual={late_count}, Match={late_count == expected_late} {'✓' if late_count == expected_late else '✗'}\")\n",
    "print(f\"  Total heads:  Expected=43, Actual={len(attention_heads)} {'✓' if len(attention_heads) == 43 else '✗'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99df78",
   "metadata": {},
   "source": [
    "**Reasoning:** The code parses each attention head name (format: \"a{layer}.h{head_num}\") to extract the layer number, counts heads per layer, and groups them into three stages as defined in the documentation. The distribution shows that the middle layers (L4-L7) have the highest concentration with 19 heads, which aligns with the documentation's description of \"dense middle routing.\"\n",
    "\n",
    "**Answer:** The analysis confirms the documentation's distribution:\n",
    "- Early stage (L0-L3): 9 heads\n",
    "- Middle stage (L4-L7): 19 heads  \n",
    "- Late stage (L8-L11): 15 heads\n",
    "- Total: 43 heads\n",
    "\n",
    "All stage totals match the documentation exactly. The middle layers have the highest concentration of attention heads (19 heads, 44% of total), supporting the documentation's claim about \"dense middle routing\" for signal propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01939bce",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 15 (Code Required)\n",
    "\n",
    "**Question:** Write code to analyze the relative contribution of MLPs versus attention heads to the sarcasm circuit in terms of dimensions.\n",
    "\n",
    "Your code should:\n",
    "1. Calculate the total dimensions from MLPs (10 MLPs × 768 dims)\n",
    "2. Calculate the total dimensions from attention heads (43 heads × 64 dims)\n",
    "3. Calculate the percentage of total circuit dimensions (excluding input) contributed by:\n",
    "   - MLPs\n",
    "   - Attention heads\n",
    "4. Print the results and explain what this ratio tells us about whether the circuit is \"MLP-dominant\" or \"attention-dominant\"\n",
    "\n",
    "Note: Exclude the input embedding (768 dims) from the percentage calculation since it's always required.\n",
    "\n",
    "Expected conclusion: The circuit should be MLP-dominant with MLPs contributing ~74% of non-input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b41670",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MLP vs ATTENTION HEAD CONTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Dimensional Breakdown:\n",
      "  MLPs:           10 × 768 = 7,680 dims\n",
      "  Attention heads: 43 × 64 = 2,752 dims\n",
      "  ---------------------------------------------\n",
      "  Total (non-input):        10,432 dims\n",
      "\n",
      "Percentage Contribution (excluding input):\n",
      "  MLPs:            73.6%\n",
      "  Attention heads: 26.4%\n",
      "\n",
      "Ratio Analysis:\n",
      "  MLP:Attention ratio = 2.79:1\n",
      "  MLPs contribute 2.79x more dimensions than attention\n",
      "\n",
      "Conclusion:\n",
      "  The circuit is **MLP-DOMINANT**\n",
      "  - MLPs contribute 73.6% of non-input dimensions\n",
      "  - This indicates sarcasm detection relies primarily on MLP-based\n",
      "    pattern detection rather than attention-based information routing\n",
      "\n",
      "Alignment with Documentation:\n",
      "  Expected: MLPs contribute ~74% (from documentation)\n",
      "  Actual:   MLPs contribute 73.6%\n",
      "  Match: ✓\n",
      "\n",
      "Interpretation:\n",
      "  From the documentation's 'Comparison to IOI Circuit' table:\n",
      "  - Sarcasm circuit has 'Dominant' MLP importance\n",
      "  - This ~3:1 ratio of MLP to attention dimensions confirms\n",
      "    the MLP-dominant nature of the sarcasm circuit\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 15: Analyze MLP vs Attention contribution\n",
    "\n",
    "# Circuit parameters from documentation\n",
    "num_mlps = 10\n",
    "num_attn_heads = 43\n",
    "d_model = 768  # MLP dimension\n",
    "d_head = 64    # Attention head dimension\n",
    "\n",
    "# Step 1: Calculate total dimensions from MLPs\n",
    "mlp_dims = num_mlps * d_model\n",
    "\n",
    "# Step 2: Calculate total dimensions from attention heads\n",
    "attn_dims = num_attn_heads * d_head\n",
    "\n",
    "# Total non-input dimensions\n",
    "total_non_input_dims = mlp_dims + attn_dims\n",
    "\n",
    "# Step 3: Calculate percentages\n",
    "mlp_percentage = (mlp_dims / total_non_input_dims) * 100\n",
    "attn_percentage = (attn_dims / total_non_input_dims) * 100\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"MLP vs ATTENTION HEAD CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDimensional Breakdown:\")\n",
    "print(f\"  MLPs:           {num_mlps} × {d_model} = {mlp_dims:,} dims\")\n",
    "print(f\"  Attention heads: {num_attn_heads} × {d_head} = {attn_dims:,} dims\")\n",
    "print(f\"  \" + \"-\" * 45)\n",
    "print(f\"  Total (non-input):        {total_non_input_dims:,} dims\")\n",
    "\n",
    "print(f\"\\nPercentage Contribution (excluding input):\")\n",
    "print(f\"  MLPs:            {mlp_percentage:.1f}%\")\n",
    "print(f\"  Attention heads: {attn_percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\nRatio Analysis:\")\n",
    "mlp_to_attn_ratio = mlp_dims / attn_dims\n",
    "print(f\"  MLP:Attention ratio = {mlp_to_attn_ratio:.2f}:1\")\n",
    "print(f\"  MLPs contribute {mlp_to_attn_ratio:.2f}x more dimensions than attention\")\n",
    "\n",
    "print(f\"\\nConclusion:\")\n",
    "if mlp_percentage > 60:\n",
    "    print(f\"  The circuit is **MLP-DOMINANT**\")\n",
    "    print(f\"  - MLPs contribute {mlp_percentage:.1f}% of non-input dimensions\")\n",
    "    print(f\"  - This indicates sarcasm detection relies primarily on MLP-based\")\n",
    "    print(f\"    pattern detection rather than attention-based information routing\")\n",
    "else:\n",
    "    print(f\"  The circuit is **ATTENTION-DOMINANT**\")\n",
    "    \n",
    "print(\"\\nAlignment with Documentation:\")\n",
    "print(f\"  Expected: MLPs contribute ~74% (from documentation)\")\n",
    "print(f\"  Actual:   MLPs contribute {mlp_percentage:.1f}%\")\n",
    "print(f\"  Match: {'✓' if abs(mlp_percentage - 73.6) < 1 else '✗'}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  From the documentation's 'Comparison to IOI Circuit' table:\")\n",
    "print(\"  - Sarcasm circuit has 'Dominant' MLP importance\")\n",
    "print(\"  - This ~3:1 ratio of MLP to attention dimensions confirms\")\n",
    "print(\"    the MLP-dominant nature of the sarcasm circuit\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6deab",
   "metadata": {},
   "source": [
    "**Reasoning:** The code calculates that MLPs contribute 7,680 dimensions (10 × 768) and attention heads contribute 2,752 dimensions (43 × 64). Excluding the input embedding, MLPs represent 73.6% of the circuit dimensions while attention heads represent 26.4%, giving a ratio of approximately 2.79:1 in favor of MLPs.\n",
    "\n",
    "**Answer:** The analysis shows:\n",
    "- MLPs: 7,680 dims (73.6% of non-input dimensions)\n",
    "- Attention heads: 2,752 dims (26.4% of non-input dimensions)  \n",
    "- MLP:Attention ratio: 2.79:1\n",
    "\n",
    "This confirms the circuit is **MLP-dominant**, matching the expected ~74% contribution from the documentation. The roughly 3:1 ratio indicates that sarcasm detection relies primarily on MLP-based pattern detection rather than attention-based information routing, which aligns with the documentation's characterization of the sarcasm circuit as having \"Dominant\" MLP importance versus \"Supporting\" attention importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fdd0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exam Complete\n",
    "\n",
    "All 15 questions have been answered based solely on the provided documentation. The answers include:\n",
    "\n",
    "- **Questions 1-3**: Multiple choice questions about technical specifications\n",
    "- **Questions 4-5**: Free-generation questions about excluded components and hypothesis evolution\n",
    "- **Questions 6, 10-11**: Multiple choice questions about circuit mechanisms\n",
    "- **Questions 7-9, 12**: Free-generation questions about circuit analysis and limitations\n",
    "- **Questions 13-15**: Code-required questions verifying budget calculations and analyzing circuit composition\n",
    "\n",
    "All answers were derived strictly from the documentation at:\n",
    "`/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00/logs/documentation.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-20-09-07_CircuitAnalysisExam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
