{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5d685e",
   "metadata": {},
   "source": [
    "# Sarcasm Circuit Analysis - Code Question Solutions\n",
    "\n",
    "This notebook contains the solutions for the code-based questions.\n",
    "Each question has:\n",
    "1. Student-facing stub\n",
    "2. Solution cell\n",
    "3. Auto-check cell for validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f2e3c",
   "metadata": {},
   "source": [
    "## CQ1: Verify Write Budget Calculation\n",
    "\n",
    "Write code to verify the write budget calculation for the sarcasm circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT STUB - CQ1\n",
    "# TODO: Verify the write budget calculation\n",
    "# Define dimension sizes\n",
    "d_model = None  # TODO: Set to 768\n",
    "d_head = None   # TODO: Set to 64\n",
    "\n",
    "# Circuit composition\n",
    "num_input = None        # TODO: Set to 1\n",
    "num_mlps = None         # TODO: Set to 10\n",
    "num_attention_heads = None  # TODO: Set to 43\n",
    "\n",
    "# Calculate individual costs\n",
    "input_cost = None       # TODO: Calculate\n",
    "mlp_cost = None         # TODO: Calculate\n",
    "attn_cost = None        # TODO: Calculate\n",
    "\n",
    "# Calculate total\n",
    "total_cost = None       # TODO: Calculate\n",
    "\n",
    "# Print results\n",
    "print(f\"Input embedding cost: {input_cost} dimensions\")\n",
    "print(f\"MLP cost ({num_mlps} MLPs): {mlp_cost} dimensions\")\n",
    "print(f\"Attention head cost ({num_attention_heads} heads): {attn_cost} dimensions\")\n",
    "print(f\"Total write cost: {total_cost} dimensions\")\n",
    "print(f\"Budget verification: {total_cost == 11200}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - CQ1\n",
    "# Define dimension sizes\n",
    "d_model = 768\n",
    "d_head = 64\n",
    "\n",
    "# Circuit composition\n",
    "num_input = 1\n",
    "num_mlps = 10\n",
    "num_attention_heads = 43\n",
    "\n",
    "# Calculate individual costs\n",
    "input_cost = num_input * d_model\n",
    "mlp_cost = num_mlps * d_model\n",
    "attn_cost = num_attention_heads * d_head\n",
    "\n",
    "# Calculate total\n",
    "total_cost = input_cost + mlp_cost + attn_cost\n",
    "\n",
    "# Print results\n",
    "print(f\"Input embedding cost: {input_cost} dimensions\")\n",
    "print(f\"MLP cost ({num_mlps} MLPs): {mlp_cost} dimensions\")\n",
    "print(f\"Attention head cost ({num_attention_heads} heads): {attn_cost} dimensions\")\n",
    "print(f\"Total write cost: {total_cost} dimensions\")\n",
    "print(f\"Budget verification: {total_cost == 11200}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-CHECK - CQ1\n",
    "# Verify the computed values\n",
    "expected_input = 768\n",
    "expected_mlp = 7680\n",
    "expected_attn = 2752\n",
    "expected_total = 11200\n",
    "\n",
    "assert input_cost == expected_input, f\"Input cost mismatch: {input_cost} != {expected_input}\"\n",
    "assert mlp_cost == expected_mlp, f\"MLP cost mismatch: {mlp_cost} != {expected_mlp}\"\n",
    "assert attn_cost == expected_attn, f\"Attention cost mismatch: {attn_cost} != {expected_attn}\"\n",
    "assert total_cost == expected_total, f\"Total cost mismatch: {total_cost} != {expected_total}\"\n",
    "print(\"✓ All CQ1 checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3f396",
   "metadata": {},
   "source": [
    "## CQ2: Analyze Attention Head Distribution by Layer\n",
    "\n",
    "Write code to analyze the distribution of the 43 attention heads across the 12 layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT STUB - CQ2\n",
    "attention_heads = ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', \n",
    "                   'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', \n",
    "                   'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', \n",
    "                   'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', \n",
    "                   'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', \n",
    "                   'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', \n",
    "                   'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n",
    "\n",
    "# TODO: Initialize a counter for each layer (0-11)\n",
    "layer_counts = None  # Use a dictionary or list\n",
    "\n",
    "# TODO: Parse each attention head name to extract layer number\n",
    "for head in attention_heads:\n",
    "    layer = None  # Parse the layer number\n",
    "    pass\n",
    "\n",
    "# TODO: Print count per layer\n",
    "print(\"Heads per layer:\")\n",
    "for layer in range(12):\n",
    "    pass\n",
    "\n",
    "# TODO: Calculate stage totals\n",
    "early_total = None    # Layers 0-3\n",
    "middle_total = None   # Layers 4-7\n",
    "late_total = None     # Layers 8-11\n",
    "\n",
    "print(f\"\\nStage totals:\")\n",
    "print(f\"Early (L0-L3): {early_total} heads\")\n",
    "print(f\"Middle (L4-L7): {middle_total} heads\")\n",
    "print(f\"Late (L8-L11): {late_total} heads\")\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Early matches doc (9): {early_total == 9}\")\n",
    "print(f\"Middle matches doc (19): {middle_total == 19}\")\n",
    "print(f\"Late matches doc (15): {late_total == 15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - CQ2\n",
    "attention_heads = ['a11.h8', 'a11.h0', 'a4.h11', 'a9.h3', 'a6.h11', 'a8.h5', \n",
    "                   'a9.h10', 'a5.h3', 'a10.h5', 'a11.h3', 'a3.h9', 'a10.h9', \n",
    "                   'a4.h9', 'a4.h7', 'a3.h11', 'a8.h7', 'a7.h8', 'a6.h0', \n",
    "                   'a4.h0', 'a2.h8', 'a5.h4', 'a8.h10', 'a5.h7', 'a4.h1', \n",
    "                   'a6.h8', 'a5.h2', 'a11.h11', 'a6.h7', 'a8.h4', 'a3.h2', \n",
    "                   'a8.h8', 'a2.h5', 'a6.h4', 'a7.h9', 'a7.h3', 'a4.h3', \n",
    "                   'a2.h2', 'a3.h6', 'a6.h5', 'a11.h4', 'a2.h3', 'a8.h2', 'a1.h0']\n",
    "\n",
    "# Initialize counter for each layer\n",
    "layer_counts = {i: 0 for i in range(12)}\n",
    "\n",
    "# Parse each attention head name\n",
    "for head in attention_heads:\n",
    "    # Format is 'aX.hY' where X is layer\n",
    "    layer = int(head.split('.')[0][1:])\n",
    "    layer_counts[layer] += 1\n",
    "\n",
    "# Print count per layer\n",
    "print(\"Heads per layer:\")\n",
    "for layer in range(12):\n",
    "    print(f\"  Layer {layer}: {layer_counts[layer]} heads\")\n",
    "\n",
    "# Calculate stage totals\n",
    "early_total = sum(layer_counts[i] for i in range(0, 4))\n",
    "middle_total = sum(layer_counts[i] for i in range(4, 8))\n",
    "late_total = sum(layer_counts[i] for i in range(8, 12))\n",
    "\n",
    "print(f\"\\nStage totals:\")\n",
    "print(f\"Early (L0-L3): {early_total} heads\")\n",
    "print(f\"Middle (L4-L7): {middle_total} heads\")\n",
    "print(f\"Late (L8-L11): {late_total} heads\")\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Early matches doc (9): {early_total == 9}\")\n",
    "print(f\"Middle matches doc (19): {middle_total == 19}\")\n",
    "print(f\"Late matches doc (15): {late_total == 15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-CHECK - CQ2\n",
    "# Verify stage totals\n",
    "assert early_total == 9, f\"Early stage mismatch: {early_total} != 9\"\n",
    "assert middle_total == 19, f\"Middle stage mismatch: {middle_total} != 19\"\n",
    "assert late_total == 15, f\"Late stage mismatch: {late_total} != 15\"\n",
    "assert early_total + middle_total + late_total == 43, \"Total heads should be 43\"\n",
    "print(\"✓ All CQ2 checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ef69b",
   "metadata": {},
   "source": [
    "## CQ3: Analyze MLP vs Attention Head Contribution\n",
    "\n",
    "Write code to analyze the relative contribution of MLPs versus attention heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT STUB - CQ3\n",
    "d_model = 768\n",
    "d_head = 64\n",
    "num_mlps = 10\n",
    "num_attention_heads = 43\n",
    "\n",
    "# TODO: Calculate total dimensions\n",
    "mlp_dims = None\n",
    "attn_dims = None\n",
    "\n",
    "# TODO: Calculate total non-input dimensions\n",
    "total_non_input = None\n",
    "\n",
    "# TODO: Calculate percentages\n",
    "mlp_percentage = None\n",
    "attn_percentage = None\n",
    "\n",
    "# Print results\n",
    "print(f\"MLP dimensions: {mlp_dims}\")\n",
    "print(f\"Attention head dimensions: {attn_dims}\")\n",
    "print(f\"Total non-input dimensions: {total_non_input}\")\n",
    "print(f\"\\nPercentage contribution:\")\n",
    "print(f\"MLPs: {mlp_percentage:.1f}%\")\n",
    "print(f\"Attention heads: {attn_percentage:.1f}%\")\n",
    "\n",
    "if mlp_percentage > attn_percentage:\n",
    "    print(\"\\nConclusion: Circuit is MLP-dominant\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Circuit is attention-dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4871c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - CQ3\n",
    "d_model = 768\n",
    "d_head = 64\n",
    "num_mlps = 10\n",
    "num_attention_heads = 43\n",
    "\n",
    "# Calculate total dimensions\n",
    "mlp_dims = num_mlps * d_model\n",
    "attn_dims = num_attention_heads * d_head\n",
    "\n",
    "# Calculate total non-input dimensions\n",
    "total_non_input = mlp_dims + attn_dims\n",
    "\n",
    "# Calculate percentages\n",
    "mlp_percentage = (mlp_dims / total_non_input) * 100\n",
    "attn_percentage = (attn_dims / total_non_input) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"MLP dimensions: {mlp_dims}\")\n",
    "print(f\"Attention head dimensions: {attn_dims}\")\n",
    "print(f\"Total non-input dimensions: {total_non_input}\")\n",
    "print(f\"\\nPercentage contribution:\")\n",
    "print(f\"MLPs: {mlp_percentage:.1f}%\")\n",
    "print(f\"Attention heads: {attn_percentage:.1f}%\")\n",
    "\n",
    "if mlp_percentage > attn_percentage:\n",
    "    print(\"\\nConclusion: Circuit is MLP-dominant\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Circuit is attention-dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-CHECK - CQ3\n",
    "# Verify calculations with tolerance\n",
    "expected_mlp_dims = 7680\n",
    "expected_attn_dims = 2752\n",
    "expected_total = 10432\n",
    "expected_mlp_pct = 73.6\n",
    "expected_attn_pct = 26.4\n",
    "\n",
    "assert mlp_dims == expected_mlp_dims, f\"MLP dims mismatch: {mlp_dims} != {expected_mlp_dims}\"\n",
    "assert attn_dims == expected_attn_dims, f\"Attention dims mismatch: {attn_dims} != {expected_attn_dims}\"\n",
    "assert total_non_input == expected_total, f\"Total mismatch: {total_non_input} != {expected_total}\"\n",
    "assert abs(mlp_percentage - expected_mlp_pct) < 0.1, f\"MLP percentage mismatch: {mlp_percentage} != {expected_mlp_pct}\"\n",
    "assert abs(attn_percentage - expected_attn_pct) < 0.1, f\"Attention percentage mismatch: {attn_percentage} != {expected_attn_pct}\"\n",
    "assert mlp_percentage > attn_percentage, \"Circuit should be MLP-dominant\"\n",
    "print(\"✓ All CQ3 checks passed!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
