{
  "nodes": [
    "input",
    "m2",
    "m11",
    "m10",
    "m9",
    "m8",
    "m7",
    "m6",
    "m0",
    "m1",
    "m5",
    "m4",
    "a11.h8",
    "a11.h0",
    "a8.h5",
    "a9.h3",
    "a6.h11",
    "a5.h3",
    "a10.h5",
    "a4.h11",
    "a9.h10",
    "a11.h3",
    "a8.h8",
    "a3.h11",
    "a4.h9",
    "a8.h7",
    "a3.h9",
    "a7.h8",
    "a4.h7",
    "a4.h0",
    "a6.h7",
    "a11.h11",
    "a5.h7",
    "a4.h1",
    "a10.h9",
    "a2.h8",
    "a8.h10",
    "a7.h9",
    "a6.h0",
    "a7.h3",
    "a6.h4",
    "a2.h2",
    "a5.h4"
  ],
  "metadata": {
    "total_components": 43,
    "write_budget_used": 11200,
    "write_budget_max": 11200,
    "budget_utilization": "100.0%",
    "num_mlps": 11,
    "num_attention_heads": 31,
    "model": "gpt2-small",
    "task": "sarcasm_detection",
    "version": "replication_v1",
    "method": "differential_activation_analysis",
    "mlp_threshold": 7.0,
    "replication_date": "2025-11-10"
  },
  "component_rankings": {
    "mlps": {
      "m2": 30.80913887023926,
      "m11": 22.853029632568358,
      "m10": 17.7833740234375,
      "m9": 14.043318748474121,
      "m8": 11.798775100708008,
      "m7": 9.844790840148926,
      "m6": 8.947447395324707,
      "m0": 8.108276081085204,
      "m1": 7.8784795761108395,
      "m5": 7.848785877227783,
      "m4": 7.3352508544921875,
      "m3": 6.176705837249756
    },
    "top_attention_heads": {
      "a11.h8": 3.322826290130615,
      "a11.h0": 2.808633899688721,
      "a8.h5": 1.5003644943237304,
      "a9.h3": 1.4817564010620117,
      "a6.h11": 1.4510411024093628,
      "a5.h3": 1.3469312906265258,
      "a10.h5": 1.3150179624557494,
      "a4.h11": 1.3132207632064818,
      "a9.h10": 1.3067935943603515,
      "a11.h3": 1.2624706625938416
    }
  }
}