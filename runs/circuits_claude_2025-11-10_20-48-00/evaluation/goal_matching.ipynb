{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cf74a9",
   "metadata": {},
   "source": [
    "# Goal Matching Evaluation\n",
    "\n",
    "## Critic Model Assessment of Student Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "repo_path = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00'\n",
    "\n",
    "print(\"Evaluation initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51b0c6",
   "metadata": {},
   "source": [
    "## 1. Read Instructor's Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read instructor's goal and hypothesis\n",
    "with open('/home/smallyan/critic_model_mechinterp/prompts/l3/circuit_prompt_sarcasm.txt', 'r') as f:\n",
    "    instructor_prompt = f.read()\n",
    "\n",
    "# Extract key sections\n",
    "print(\"=\" * 80)\n",
    "print(\"INSTRUCTOR'S GOAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print('''\n",
    "KEY OBJECTIVES:\n",
    "1. Identify a precise circuit (attention heads and MLPs) for sarcasm recognition\n",
    "2. Stay within write-budget constraint (≤ 11,200 dimensions)\n",
    "3. Follow iterative hypothesis-testing-refinement workflow\n",
    "\n",
    "EXPECTED MECHANISM (Hypothesis):\n",
    "- Early layers: Encode sentiment direction (positive/negative polarity)\n",
    "- Mid layers: Encode tone or incongruity\n",
    "- Late layers: Perform meaning reversal or contextual correction\n",
    "\n",
    "REQUIRED OUTPUTS:\n",
    "- logs/plan_v1.md, plan_v2.md, etc.\n",
    "- logs/documentation.md\n",
    "- results/real_circuits_1.json\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c68be3",
   "metadata": {},
   "source": [
    "## 2. Read Student's Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read student's plan files\n",
    "with open(f'{repo_path}/logs/plan_v1.md', 'r') as f:\n",
    "    plan_v1 = f.read()\n",
    "\n",
    "with open(f'{repo_path}/logs/plan_v2.md', 'r') as f:\n",
    "    plan_v2 = f.read()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STUDENT'S STATED GOAL (from plan_v1.md)\")\n",
    "print(\"=\" * 80)\n",
    "print('''\n",
    "STUDENT'S GOAL:\n",
    "\"Identify the precise circuit in GPT2-small that enables sarcasm recognition \n",
    "by detecting contradictions between literal sentiment and contextual tone.\"\n",
    "\n",
    "INITIAL HYPOTHESIS:\n",
    "- Stage 1 (L0-L3): Early layers encode literal sentiment\n",
    "- Stage 2 (L4-L7): Middle layers detect incongruity\n",
    "- Stage 3 (L8-L11): Late layers perform meaning reversal\n",
    "\n",
    "CONSTRAINTS:\n",
    "- Total budget: ≤ 11,200 dimensions\n",
    "- Minimal, interpretable components\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a821c1",
   "metadata": {},
   "source": [
    "## 3. Goal Alignment Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GOAL ALIGNMENT EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation = {\n",
    "    \"goal_alignment\": {\n",
    "        \"score\": 95,\n",
    "        \"max_score\": 100,\n",
    "        \"aligned_aspects\": [\n",
    "            \"Both target sarcasm detection circuit in GPT2-small\",\n",
    "            \"Both use same write budget constraint (11,200 dimensions)\",\n",
    "            \"Both follow three-stage hypothesis (early-mid-late)\",\n",
    "            \"Both expect sentiment encoding → incongruity detection → reversal\",\n",
    "            \"Both require iterative hypothesis refinement\"\n",
    "        ],\n",
    "        \"minor_differences\": [\n",
    "            \"Student uses synthetic dataset instead of mib-bench/sarcasm\",\n",
    "            \"Student's plan predicts specific heads vs instructor's generic examples\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nGOAL ALIGNMENT SCORE: {evaluation['goal_alignment']['score']}/100\")\n",
    "print(\"\\n✅ ALIGNED ASPECTS:\")\n",
    "for aspect in evaluation['goal_alignment']['aligned_aspects']:\n",
    "    print(f\"   • {aspect}\")\n",
    "\n",
    "print(\"\\n⚠️ MINOR DIFFERENCES:\")\n",
    "for diff in evaluation['goal_alignment']['minor_differences']:\n",
    "    print(f\"   • {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054664a",
   "metadata": {},
   "source": [
    "## 4. Plan Adherence Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PLAN ADHERENCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for plan files\n",
    "import os\n",
    "\n",
    "plan_files_found = []\n",
    "for f in ['plan_v1.md', 'plan_v2.md', 'plan_v3.md']:\n",
    "    path = f'{repo_path}/logs/{f}'\n",
    "    if os.path.exists(path):\n",
    "        plan_files_found.append(f)\n",
    "        \n",
    "print(f\"\\nPlan files found: {plan_files_found}\")\n",
    "\n",
    "evaluation[\"plan_adherence\"] = {\n",
    "    \"score\": 85,\n",
    "    \"max_score\": 100,\n",
    "    \"followed\": [\n",
    "        \"Created plan_v1.md with detailed initial hypothesis\",\n",
    "        \"Tested hypothesis using differential activation analysis\",\n",
    "        \"Created plan_v2.md with refined understanding\",\n",
    "        \"Documented evolution of hypothesis\",\n",
    "        \"Included evidence criteria for each sub-hypothesis\"\n",
    "    ],\n",
    "    \"missing\": [\n",
    "        \"No plan_v3.md - stopped at 2 iterations instead of continuing\",\n",
    "        \"No behavioral testing to verify circuit reproduces sarcasm detection\",\n",
    "        \"Ablation experiments mentioned but not fully executed\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nPLAN ADHERENCE SCORE: {evaluation['plan_adherence']['score']}/100\")\n",
    "print(\"\\n✅ FOLLOWED:\")\n",
    "for item in evaluation['plan_adherence']['followed']:\n",
    "    print(f\"   • {item}\")\n",
    "\n",
    "print(\"\\n❌ MISSING/INCOMPLETE:\")\n",
    "for item in evaluation['plan_adherence']['missing']:\n",
    "    print(f\"   • {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666a01c",
   "metadata": {},
   "source": [
    "## 5. Hypothesis Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037add83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"HYPOTHESIS TESTING EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation[\"hypothesis_testing\"] = {\n",
    "    \"score\": 90,\n",
    "    \"max_score\": 100,\n",
    "    \"hypotheses_tested\": [\n",
    "        {\n",
    "            \"hypothesis\": \"Early layers encode literal sentiment\",\n",
    "            \"tested\": True,\n",
    "            \"method\": \"Differential activation analysis on m0, m1, m2\",\n",
    "            \"result\": \"m2 emerged as primary detector (32.47 differential)\",\n",
    "            \"revised\": True\n",
    "        },\n",
    "        {\n",
    "            \"hypothesis\": \"Middle layers detect incongruity\",\n",
    "            \"tested\": True,\n",
    "            \"method\": \"Analyzed m5, m6 and attention heads in L4-L7\",\n",
    "            \"result\": \"Found moderate differential (7-10 range)\",\n",
    "            \"revised\": True,\n",
    "            \"revision\": \"Incongruity detection actually happens at L2\"\n",
    "        },\n",
    "        {\n",
    "            \"hypothesis\": \"Late MLPs perform meaning reversal\",\n",
    "            \"tested\": True,\n",
    "            \"method\": \"Analyzed m7-m11 activation patterns\",\n",
    "            \"result\": \"Strong late-layer activity (m11=22.30, m10=17.36)\",\n",
    "            \"revised\": True,\n",
    "            \"revision\": \"Not 'reversal' but 'signal integration'\"\n",
    "        },\n",
    "        {\n",
    "            \"hypothesis\": \"Circuit is sparse (<50 components)\",\n",
    "            \"tested\": True,\n",
    "            \"method\": \"Budget optimization\",\n",
    "            \"result\": \"Final circuit has 54 components (slightly over target)\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nHYPOTHESIS TESTING SCORE: {evaluation['hypothesis_testing']['score']}/100\")\n",
    "print(\"\\nHypotheses Tested:\")\n",
    "for i, h in enumerate(evaluation['hypothesis_testing']['hypotheses_tested'], 1):\n",
    "    status = \"✅\" if h['tested'] else \"❌\"\n",
    "    print(f\"\\n{i}. {status} {h['hypothesis']}\")\n",
    "    print(f\"   Method: {h['method']}\")\n",
    "    print(f\"   Result: {h['result']}\")\n",
    "    if h.get('revision'):\n",
    "        print(f\"   ⚠️ Revision: {h['revision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bb86d",
   "metadata": {},
   "source": [
    "## 6. Hypothesis Convergence Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e19a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"HYPOTHESIS CONVERGENCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation[\"convergence\"] = {\n",
    "    \"score\": 75,\n",
    "    \"max_score\": 100,\n",
    "    \"instructor_expected\": {\n",
    "        \"early\": \"Sentiment direction encoding\",\n",
    "        \"mid\": \"Tone/incongruity detection\",\n",
    "        \"late\": \"Meaning reversal\"\n",
    "    },\n",
    "    \"student_discovered\": {\n",
    "        \"early\": \"Primary sarcasm detection (m2)\",\n",
    "        \"mid\": \"Signal propagation\",\n",
    "        \"late\": \"Signal integration\"\n",
    "    },\n",
    "    \"similarities\": [\n",
    "        \"Three-stage hierarchical processing confirmed\",\n",
    "        \"MLPs more important than attention heads\",\n",
    "        \"Late layers critical for final output\"\n",
    "    ],\n",
    "    \"differences\": [\n",
    "        \"Detection timing: L2 (student) vs L4-L7 (instructor expected)\",\n",
    "        \"Mechanism: Integration (student) vs Reversal (instructor expected)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nCONVERGENCE SCORE: {evaluation['convergence']['score']}/100\")\n",
    "\n",
    "print(\"\\nINSTRUCTOR'S EXPECTED MECHANISM:\")\n",
    "for stage, func in evaluation['convergence']['instructor_expected'].items():\n",
    "    print(f\"   {stage.upper()}: {func}\")\n",
    "\n",
    "print(\"\\nSTUDENT'S DISCOVERED MECHANISM:\")\n",
    "for stage, func in evaluation['convergence']['student_discovered'].items():\n",
    "    print(f\"   {stage.upper()}: {func}\")\n",
    "\n",
    "print(\"\\n✅ SIMILARITIES:\")\n",
    "for s in evaluation['convergence']['similarities']:\n",
    "    print(f\"   • {s}\")\n",
    "\n",
    "print(\"\\n⚠️ KEY DIFFERENCES:\")\n",
    "for d in evaluation['convergence']['differences']:\n",
    "    print(f\"   • {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63535efa",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe44c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall score\n",
    "scores = [\n",
    "    evaluation['goal_alignment']['score'],\n",
    "    evaluation['plan_adherence']['score'],\n",
    "    evaluation['hypothesis_testing']['score'],\n",
    "    evaluation['convergence']['score']\n",
    "]\n",
    "overall_score = sum(scores) / len(scores)\n",
    "\n",
    "evaluation[\"overall\"] = {\n",
    "    \"score\": overall_score,\n",
    "    \"verdict\": \"PASS\" if overall_score >= 70 else \"FAIL\",\n",
    "    \"summary\": '''\n",
    "The student successfully executed the research workflow as specified by the instructor.\n",
    "They formulated an initial hypothesis, tested it empirically, and refined their\n",
    "understanding based on results. While the final mechanism differs from the instructor's\n",
    "expected hypothesis (early detection vs mid-layer detection, integration vs reversal),\n",
    "the differences are well-documented and empirically justified.\n",
    "\n",
    "The student demonstrated good scientific practice by:\n",
    "1. Documenting clear hypotheses before testing\n",
    "2. Using appropriate experimental methods\n",
    "3. Updating their model when evidence contradicted expectations\n",
    "4. Acknowledging limitations of their approach\n",
    "\n",
    "Areas for improvement:\n",
    "1. Continue iterations beyond plan_v2\n",
    "2. Perform behavioral validation of the circuit\n",
    "3. Complete planned ablation experiments\n",
    "'''\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Category':<25} {'Score':<10} {'Status'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Goal Alignment':<25} {evaluation['goal_alignment']['score']}/100    {'✅ Excellent'}\")\n",
    "print(f\"{'Plan Adherence':<25} {evaluation['plan_adherence']['score']}/100    {'✅ Good'}\")\n",
    "print(f\"{'Hypothesis Testing':<25} {evaluation['hypothesis_testing']['score']}/100    {'✅ Excellent'}\")\n",
    "print(f\"{'Hypothesis Convergence':<25} {evaluation['convergence']['score']}/100    {'⚠️ Acceptable'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'OVERALL':<25} {overall_score:.2f}/100 {'✅ PASS' if overall_score >= 70 else '❌ FAIL'}\")\n",
    "\n",
    "print(evaluation['overall']['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "import json\n",
    "\n",
    "output_path = f'{repo_path}/evaluation/goal_matching_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(evaluation, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
