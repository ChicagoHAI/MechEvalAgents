{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff38c96",
   "metadata": {},
   "source": [
    "# Hidden Neuron Testing\n",
    "\n",
    "This notebook tests whether each neuron (MLP and attention head) in the student's circuit matches its hypothesized function.\n",
    "\n",
    "## Testing Strategy\n",
    "\n",
    "For each component, we will:\n",
    "1. Load the model and dataset\n",
    "2. Extract activations for that component\n",
    "3. Design test cases based on the student's hypothesized function\n",
    "4. Evaluate whether the component behaves as expected\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065cc458",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/critic_model_mechinterp\n",
      "Device: cuda\n",
      "\n",
      "Circuit loaded: 54 components\n",
      "MLPs: 10\n",
      "Attention heads: 43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load the circuit\n",
    "circuit_path = Path('runs/circuits_claude_2025-11-10_20-48-00/results/real_circuits_1.json')\n",
    "with open(circuit_path, 'r') as f:\n",
    "    circuit = json.load(f)\n",
    "\n",
    "print(f\"\\nCircuit loaded: {len(circuit['nodes'])} components\")\n",
    "print(f\"MLPs: {circuit['metadata']['num_mlps']}\")\n",
    "print(f\"Attention heads: {circuit['metadata']['num_attention_heads']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c912d024",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2-small model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Model loaded successfully!\n",
      "\n",
      "Model configuration:\n",
      "  Layers: 12\n",
      "  Heads per layer: 12\n",
      "  d_model: 768\n",
      "  d_head: 64\n"
     ]
    }
   ],
   "source": [
    "# Load GPT2-small model\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "print(\"Loading GPT2-small model...\")\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Model configuration\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"  d_model: {model.cfg.d_model}\")\n",
    "print(f\"  d_head: {model.cfg.d_head}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c9bc1",
   "metadata": {},
   "source": [
    "## Test Dataset Creation\n",
    "\n",
    "We'll create test cases to validate each component's hypothesized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546d99d4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset created:\n",
      "  Sarcastic examples: 10\n",
      "  Literal examples: 10\n",
      "  Neutral examples: 10\n",
      "  Total: 30\n",
      "\n",
      "Example sarcastic:  Oh great, another meeting at 7 AM.\n",
      "Example literal:  I'm excited about the meeting at 7 AM tomorrow.\n",
      "Example neutral:  The meeting is scheduled for 7 AM tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive test dataset\n",
    "test_data = {\n",
    "    \"sarcastic\": [\n",
    "        \"Oh great, another meeting at 7 AM.\",\n",
    "        \"Wow, I just love getting stuck in traffic.\",\n",
    "        \"Fantastic, my laptop crashed right before the deadline.\",\n",
    "        \"Perfect, exactly what I needed today.\",\n",
    "        \"Oh wonderful, it's raining on my day off.\",\n",
    "        \"Just what I wanted, more paperwork.\",\n",
    "        \"Brilliant idea to schedule this on a Friday evening.\",\n",
    "        \"Marvelous, the coffee machine is broken again.\",\n",
    "        \"Absolutely thrilled to work overtime this weekend.\",\n",
    "        \"How delightful, another software update.\"\n",
    "    ],\n",
    "    \"literal\": [\n",
    "        \"I'm excited about the meeting at 7 AM tomorrow.\",\n",
    "        \"I really enjoy my peaceful morning commute.\",\n",
    "        \"I successfully submitted my project before the deadline.\",\n",
    "        \"This is exactly what I needed today.\",\n",
    "        \"I'm happy to have a relaxing day off.\",\n",
    "        \"I appreciate the new tasks assigned to me.\",\n",
    "        \"Great idea to finish early and enjoy the weekend.\",\n",
    "        \"The coffee tastes wonderful this morning.\",\n",
    "        \"I'm happy to contribute extra hours when needed.\",\n",
    "        \"I love how the new update improves functionality.\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"The meeting is scheduled for 7 AM tomorrow.\",\n",
    "        \"I take the same route to work every day.\",\n",
    "        \"The project deadline is approaching.\",\n",
    "        \"Today is a regular day.\",\n",
    "        \"It might rain on my day off.\",\n",
    "        \"There is paperwork on my desk.\",\n",
    "        \"The schedule includes Friday evening.\",\n",
    "        \"The coffee machine needs maintenance.\",\n",
    "        \"Some people work overtime on weekends.\",\n",
    "        \"A software update is available.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Test dataset created:\")\n",
    "print(f\"  Sarcastic examples: {len(test_data['sarcastic'])}\")\n",
    "print(f\"  Literal examples: {len(test_data['literal'])}\")\n",
    "print(f\"  Neutral examples: {len(test_data['neutral'])}\")\n",
    "print(f\"  Total: {sum(len(v) for v in test_data.values())}\")\n",
    "\n",
    "# Example sentences\n",
    "print(\"\\nExample sarcastic: \", test_data['sarcastic'][0])\n",
    "print(\"Example literal: \", test_data['literal'][0])\n",
    "print(\"Example neutral: \", test_data['neutral'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1734fe",
   "metadata": {},
   "source": [
    "## Component Function Testing\n",
    "\n",
    "We'll test the key components identified by the student against their hypothesized functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918a9179",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction function defined.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to collect activations\n",
    "def get_activations(model, texts, component_name):\n",
    "    \"\"\"\n",
    "    Get activations for a specific component across all texts.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        texts: List of text strings\n",
    "        component_name: e.g., 'm2', 'a11.h8', 'input'\n",
    "    \n",
    "    Returns:\n",
    "        List of activation tensors (one per text)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = model.to_tokens(text)\n",
    "        \n",
    "        if component_name == 'input':\n",
    "            hook_name = 'hook_embed'\n",
    "        elif component_name.startswith('m'):\n",
    "            layer = int(component_name[1:])\n",
    "            hook_name = f'blocks.{layer}.hook_mlp_out'\n",
    "        elif component_name.startswith('a'):\n",
    "            parts = component_name.split('.')\n",
    "            layer = int(parts[0][1:])\n",
    "            head = int(parts[1][1:])\n",
    "            # Use the correct hook name for attention output\n",
    "            hook_name = f'blocks.{layer}.attn.hook_z'\n",
    "        \n",
    "        # Run model and cache activations\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Extract the specific activation\n",
    "        if component_name.startswith('a'):\n",
    "            # For attention heads, extract specific head\n",
    "            act = cache[hook_name][0, :, head, :]  # [seq_len, d_head]\n",
    "        else:\n",
    "            # For MLPs and input\n",
    "            act = cache[hook_name][0, :, :]  # [seq_len, d_model]\n",
    "        \n",
    "        activations.append(act.cpu())\n",
    "    \n",
    "    return activations\n",
    "\n",
    "print(\"Activation extraction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5670248d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component hypotheses loaded:\n",
      "\n",
      "m2: Primary sarcasm/incongruity detector\n",
      "\n",
      "m11: Final pre-output processing and integration\n",
      "\n",
      "m0: Initial embedding processing and early context encoding\n",
      "\n",
      "m1: Early context encoding, feeds into m2\n",
      "\n",
      "a11.h8: Output integration head (strongest attention head)\n",
      "\n",
      "a11.h0: Output integration head (second strongest)\n"
     ]
    }
   ],
   "source": [
    "# Define component hypotheses from student's work\n",
    "component_hypotheses = {\n",
    "    \"m2\": {\n",
    "        \"hypothesis\": \"Primary sarcasm/incongruity detector\",\n",
    "        \"expected_behavior\": \"High differential activation between sarcastic and literal/neutral\",\n",
    "        \"test\": \"Mean activation on sarcastic >> mean activation on literal/neutral\"\n",
    "    },\n",
    "    \"m11\": {\n",
    "        \"hypothesis\": \"Final pre-output processing and integration\",\n",
    "        \"expected_behavior\": \"Strong differential, integrates sarcasm signal into final representation\",\n",
    "        \"test\": \"High differential activation, second highest among MLPs\"\n",
    "    },\n",
    "    \"m0\": {\n",
    "        \"hypothesis\": \"Initial embedding processing and early context encoding\",\n",
    "        \"expected_behavior\": \"Moderate differential, provides foundation for m2\",\n",
    "        \"test\": \"Moderate differential activation\"\n",
    "    },\n",
    "    \"m1\": {\n",
    "        \"hypothesis\": \"Early context encoding, feeds into m2\",\n",
    "        \"expected_behavior\": \"Moderate differential, supports m2's incongruity detection\",\n",
    "        \"test\": \"Moderate differential activation\"\n",
    "    },\n",
    "    \"a11.h8\": {\n",
    "        \"hypothesis\": \"Output integration head (strongest attention head)\",\n",
    "        \"expected_behavior\": \"Highest differential among attention heads\",\n",
    "        \"test\": \"Strongest differential activation among all attention heads\"\n",
    "    },\n",
    "    \"a11.h0\": {\n",
    "        \"hypothesis\": \"Output integration head (second strongest)\",\n",
    "        \"expected_behavior\": \"Second highest differential among attention heads\",\n",
    "        \"test\": \"High differential activation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Component hypotheses loaded:\")\n",
    "for comp, details in component_hypotheses.items():\n",
    "    print(f\"\\n{comp}: {details['hypothesis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95219a1e",
   "metadata": {},
   "source": [
    "## Testing Key Components\n",
    "\n",
    "We'll test each key component by comparing differential activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf21868",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component hypotheses...\n",
      "================================================================================\n",
      "\n",
      "Testing m2: Primary sarcasm/incongruity detector\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 238.331\n",
      "  Literal mean: 243.978\n",
      "  Neutral mean: 304.707\n",
      "  Sarc-Lit differential: -5.647\n",
      "  Sarc-Neut differential: -66.376\n",
      "\n",
      "Testing m11: Final pre-output processing and integration\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 98.719\n",
      "  Literal mean: 102.286\n",
      "  Neutral mean: 98.828\n",
      "  Sarc-Lit differential: -3.567\n",
      "  Sarc-Neut differential: -0.109\n",
      "\n",
      "Testing m0: Initial embedding processing and early context encoding\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 48.130\n",
      "  Literal mean: 48.220\n",
      "  Neutral mean: 51.648\n",
      "  Sarc-Lit differential: -0.090\n",
      "  Sarc-Neut differential: -3.518\n",
      "\n",
      "Testing m1: Early context encoding, feeds into m2\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 56.637\n",
      "  Literal mean: 57.731\n",
      "  Neutral mean: 70.048\n",
      "  Sarc-Lit differential: -1.094\n",
      "  Sarc-Neut differential: -13.411\n",
      "\n",
      "Testing a11.h8: Output integration head (strongest attention head)\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'blocks.11.attn.hook_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get activations for all three categories\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Extracting activations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m sarc_acts \u001b[38;5;241m=\u001b[39m \u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msarcastic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m lit_acts \u001b[38;5;241m=\u001b[39m get_activations(model, test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliteral\u001b[39m\u001b[38;5;124m'\u001b[39m], comp_name)\n\u001b[1;32m     16\u001b[0m neut_acts \u001b[38;5;241m=\u001b[39m get_activations(model, test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m], comp_name)\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mget_activations\u001b[0;34m(model, texts, component_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Extract the specific activation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# For attention heads, extract specific head\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m, :, head, :]  \u001b[38;5;66;03m# [seq_len, d_head]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# For MLPs and input\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     act \u001b[38;5;241m=\u001b[39m cache[hook_name][\u001b[38;5;241m0\u001b[39m, :, :]  \u001b[38;5;66;03m# [seq_len, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformer_lens/ActivationCache.py:168\u001b[0m, in \u001b[0;36mActivationCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dict[key]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_act_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blocks.11.attn.hook_result'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test each component\n",
    "print(\"Testing component hypotheses...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for comp_name, hypothesis_info in component_hypotheses.items():\n",
    "    print(f\"\\nTesting {comp_name}: {hypothesis_info['hypothesis']}\")\n",
    "    \n",
    "    # Get activations for all three categories\n",
    "    print(f\"  Extracting activations...\")\n",
    "    sarc_acts = get_activations(model, test_data['sarcastic'], comp_name)\n",
    "    lit_acts = get_activations(model, test_data['literal'], comp_name)\n",
    "    neut_acts = get_activations(model, test_data['neutral'], comp_name)\n",
    "    \n",
    "    # Compute mean activation magnitude for each category\n",
    "    # Average over sequence positions\n",
    "    sarc_mean = np.mean([act.norm(dim=-1).mean().item() for act in sarc_acts])\n",
    "    lit_mean = np.mean([act.norm(dim=-1).mean().item() for act in lit_acts])\n",
    "    neut_mean = np.mean([act.norm(dim=-1).mean().item() for act in neut_acts])\n",
    "    \n",
    "    # Compute differential (sarcastic vs literal)\n",
    "    sarc_lit_diff = sarc_mean - lit_mean\n",
    "    sarc_neut_diff = sarc_mean - neut_mean\n",
    "    \n",
    "    print(f\"  Sarcastic mean: {sarc_mean:.3f}\")\n",
    "    print(f\"  Literal mean: {lit_mean:.3f}\")\n",
    "    print(f\"  Neutral mean: {neut_mean:.3f}\")\n",
    "    print(f\"  Sarc-Lit differential: {sarc_lit_diff:.3f}\")\n",
    "    print(f\"  Sarc-Neut differential: {sarc_neut_diff:.3f}\")\n",
    "    \n",
    "    test_results.append({\n",
    "        'component': comp_name,\n",
    "        'hypothesis': hypothesis_info['hypothesis'],\n",
    "        'sarc_mean': sarc_mean,\n",
    "        'lit_mean': lit_mean,\n",
    "        'neut_mean': neut_mean,\n",
    "        'sarc_lit_diff': sarc_lit_diff,\n",
    "        'sarc_neut_diff': sarc_neut_diff\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Component testing complete!\")\n",
    "\n",
    "# Create dataframe for easy analysis\n",
    "results_df = pd.DataFrame(test_results)\n",
    "print(\"\\nResults summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da481f79",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component hypotheses...\n",
      "================================================================================\n",
      "\n",
      "Testing m2: Primary sarcasm/incongruity detector\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 238.331\n",
      "  Literal mean: 243.978\n",
      "  Neutral mean: 304.707\n",
      "  Sarc-Lit differential: -5.647\n",
      "  Sarc-Neut differential: -66.376\n",
      "\n",
      "Testing m11: Final pre-output processing and integration\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 98.719\n",
      "  Literal mean: 102.286\n",
      "  Neutral mean: 98.828\n",
      "  Sarc-Lit differential: -3.567\n",
      "  Sarc-Neut differential: -0.109\n",
      "\n",
      "Testing m0: Initial embedding processing and early context encoding\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 48.130\n",
      "  Literal mean: 48.220\n",
      "  Neutral mean: 51.648\n",
      "  Sarc-Lit differential: -0.090\n",
      "  Sarc-Neut differential: -3.518\n",
      "\n",
      "Testing m1: Early context encoding, feeds into m2\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 56.637\n",
      "  Literal mean: 57.731\n",
      "  Neutral mean: 70.048\n",
      "  Sarc-Lit differential: -1.094\n",
      "  Sarc-Neut differential: -13.411\n",
      "\n",
      "Testing a11.h8: Output integration head (strongest attention head)\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 13.115\n",
      "  Literal mean: 13.076\n",
      "  Neutral mean: 13.787\n",
      "  Sarc-Lit differential: 0.039\n",
      "  Sarc-Neut differential: -0.672\n",
      "\n",
      "Testing a11.h0: Output integration head (second strongest)\n",
      "  Extracting activations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcastic mean: 6.558\n",
      "  Literal mean: 6.734\n",
      "  Neutral mean: 6.292\n",
      "  Sarc-Lit differential: -0.176\n",
      "  Sarc-Neut differential: 0.267\n",
      "\n",
      "================================================================================\n",
      "Component testing complete!\n",
      "\n",
      "Results summary:\n",
      "component                                              hypothesis  sarc_mean   lit_mean  neut_mean  sarc_lit_diff  sarc_neut_diff\n",
      "       m2                    Primary sarcasm/incongruity detector 238.331343 243.978174 304.707077      -5.646831      -66.375734\n",
      "      m11             Final pre-output processing and integration  98.718972 102.285580  98.827623      -3.566608       -0.108651\n",
      "       m0 Initial embedding processing and early context encoding  48.130013  48.219968  51.647693      -0.089955       -3.517680\n",
      "       m1                   Early context encoding, feeds into m2  56.636879  57.730761  70.048146      -1.093882      -13.411267\n",
      "   a11.h8      Output integration head (strongest attention head)  13.115464  13.076326  13.787024       0.039138       -0.671560\n",
      "   a11.h0              Output integration head (second strongest)   6.558395   6.734366   6.291549      -0.175972        0.266845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test each component\n",
    "print(\"Testing component hypotheses...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for comp_name, hypothesis_info in component_hypotheses.items():\n",
    "    print(f\"\\nTesting {comp_name}: {hypothesis_info['hypothesis']}\")\n",
    "    \n",
    "    # Get activations for all three categories\n",
    "    print(f\"  Extracting activations...\")\n",
    "    sarc_acts = get_activations(model, test_data['sarcastic'], comp_name)\n",
    "    lit_acts = get_activations(model, test_data['literal'], comp_name)\n",
    "    neut_acts = get_activations(model, test_data['neutral'], comp_name)\n",
    "    \n",
    "    # Compute mean activation magnitude for each category\n",
    "    # Average over sequence positions\n",
    "    sarc_mean = np.mean([act.norm(dim=-1).mean().item() for act in sarc_acts])\n",
    "    lit_mean = np.mean([act.norm(dim=-1).mean().item() for act in lit_acts])\n",
    "    neut_mean = np.mean([act.norm(dim=-1).mean().item() for act in neut_acts])\n",
    "    \n",
    "    # Compute differential (sarcastic vs literal)\n",
    "    sarc_lit_diff = sarc_mean - lit_mean\n",
    "    sarc_neut_diff = sarc_mean - neut_mean\n",
    "    \n",
    "    print(f\"  Sarcastic mean: {sarc_mean:.3f}\")\n",
    "    print(f\"  Literal mean: {lit_mean:.3f}\")\n",
    "    print(f\"  Neutral mean: {neut_mean:.3f}\")\n",
    "    print(f\"  Sarc-Lit differential: {sarc_lit_diff:.3f}\")\n",
    "    print(f\"  Sarc-Neut differential: {sarc_neut_diff:.3f}\")\n",
    "    \n",
    "    test_results.append({\n",
    "        'component': comp_name,\n",
    "        'hypothesis': hypothesis_info['hypothesis'],\n",
    "        'sarc_mean': sarc_mean,\n",
    "        'lit_mean': lit_mean,\n",
    "        'neut_mean': neut_mean,\n",
    "        'sarc_lit_diff': sarc_lit_diff,\n",
    "        'sarc_neut_diff': sarc_neut_diff\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Component testing complete!\")\n",
    "\n",
    "# Create dataframe for easy analysis\n",
    "results_df = pd.DataFrame(test_results)\n",
    "print(\"\\nResults summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b693416",
   "metadata": {},
   "source": [
    "## Analysis: Component Hypothesis Validation\n",
    "\n",
    "The results show **CRITICAL ISSUES** with the student's hypothesized component functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb091718",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYPOTHESIS VALIDATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "m2: Primary sarcasm/incongruity detector\n",
      "  Expected: sarc_mean >> lit_mean and neut_mean\n",
      "  Observed: sarc_mean < lit_mean AND sarc_mean << neut_mean\n",
      "  Verdict: FAILED: m2 shows OPPOSITE pattern - sarcastic has LOWER activation\n",
      "\n",
      "m11: Final pre-output processing and integration\n",
      "  Expected: Strong positive differential (second highest MLP)\n",
      "  Observed: Negative differential for both sarc-lit and sarc-neut\n",
      "  Verdict: FAILED: Negative differential, inconsistent with hypothesis\n",
      "\n",
      "m0: Initial embedding processing and early context encoding\n",
      "  Expected: Moderate positive differential\n",
      "  Observed: Negative differential (sarc < lit and sarc < neut)\n",
      "  Verdict: FAILED: Opposite direction differential\n",
      "\n",
      "m1: Early context encoding, feeds into m2\n",
      "  Expected: Moderate positive differential\n",
      "  Observed: Negative differential (sarc < lit and sarc < neut)\n",
      "  Verdict: FAILED: Opposite direction differential\n",
      "\n",
      "a11.h8: Output integration head (strongest attention head)\n",
      "  Expected: Highest positive differential among attention heads\n",
      "  Observed: Very small positive sarc-lit diff (+0.039), negative sarc-neut (-0.672)\n",
      "  Verdict: WEAK PASS: Tiny positive differential, not strong evidence\n",
      "\n",
      "a11.h0: Output integration head (second strongest)\n",
      "  Expected: High positive differential\n",
      "  Observed: Negative sarc-lit (-0.176), positive sarc-neut (+0.267)\n",
      "  Verdict: FAILED: Inconsistent differential pattern\n",
      "\n",
      "================================================================================\n",
      "VALIDATION SUMMARY: 1/6 hypotheses validated (16.7%)\n",
      "================================================================================\n",
      "\n",
      "⚠️  CRITICAL FAILURE: Most component hypotheses are NOT supported by empirical testing!\n",
      "The student's circuit interpretations are likely INCORRECT.\n"
     ]
    }
   ],
   "source": [
    "# Analyze the results\n",
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    comp = row['component']\n",
    "    hypothesis = row['hypothesis']\n",
    "    sarc_lit_diff = row['sarc_lit_diff']\n",
    "    sarc_neut_diff = row['sarc_neut_diff']\n",
    "    \n",
    "    # Determine if hypothesis is supported\n",
    "    if comp == 'm2':\n",
    "        # Should have HIGH positive differential (sarc > lit/neut)\n",
    "        expected = \"sarc_mean >> lit_mean and neut_mean\"\n",
    "        observed = \"sarc_mean < lit_mean AND sarc_mean << neut_mean\"\n",
    "        passed = False\n",
    "        verdict = \"FAILED: m2 shows OPPOSITE pattern - sarcastic has LOWER activation\"\n",
    "    elif comp == 'm11':\n",
    "        # Should have strong positive differential\n",
    "        expected = \"Strong positive differential (second highest MLP)\"\n",
    "        observed = \"Negative differential for both sarc-lit and sarc-neut\"\n",
    "        passed = False\n",
    "        verdict = \"FAILED: Negative differential, inconsistent with hypothesis\"\n",
    "    elif comp in ['m0', 'm1']:\n",
    "        # Should have moderate positive differential\n",
    "        expected = \"Moderate positive differential\"\n",
    "        observed = \"Negative differential (sarc < lit and sarc < neut)\"\n",
    "        passed = False\n",
    "        verdict = \"FAILED: Opposite direction differential\"\n",
    "    elif comp == 'a11.h8':\n",
    "        # Should be strongest attention head with positive differential\n",
    "        expected = \"Highest positive differential among attention heads\"\n",
    "        observed = f\"Very small positive sarc-lit diff (+0.039), negative sarc-neut (-0.672)\"\n",
    "        passed = sarc_lit_diff > 0  # At least positive for sarc vs lit\n",
    "        verdict = \"WEAK PASS: Tiny positive differential, not strong evidence\"\n",
    "    elif comp == 'a11.h0':\n",
    "        # Should have second highest differential\n",
    "        expected = \"High positive differential\"\n",
    "        observed = \"Negative sarc-lit (-0.176), positive sarc-neut (+0.267)\"\n",
    "        passed = False\n",
    "        verdict = \"FAILED: Inconsistent differential pattern\"\n",
    "    \n",
    "    validation_results.append({\n",
    "        'component': comp,\n",
    "        'expected': expected,\n",
    "        'observed': observed,\n",
    "        'passed': passed,\n",
    "        'verdict': verdict\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{comp}: {hypothesis}\")\n",
    "    print(f\"  Expected: {expected}\")\n",
    "    print(f\"  Observed: {observed}\")\n",
    "    print(f\"  Verdict: {verdict}\")\n",
    "\n",
    "# Calculate pass rate\n",
    "pass_count = sum(1 for r in validation_results if r['passed'])\n",
    "total_count = len(validation_results)\n",
    "pass_rate = pass_count / total_count * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"VALIDATION SUMMARY: {pass_count}/{total_count} hypotheses validated ({pass_rate:.1f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if pass_rate < 50:\n",
    "    print(\"\\n⚠️  CRITICAL FAILURE: Most component hypotheses are NOT supported by empirical testing!\")\n",
    "    print(\"The student's circuit interpretations are likely INCORRECT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d245",
   "metadata": {},
   "source": [
    "## Deeper Investigation: Why the Discrepancy?\n",
    "\n",
    "The student used differential activation based on L2 norm differences, but we're seeing opposite patterns. Let me investigate the student's methodology more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c55973",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INVESTIGATING STUDENT'S DIFFERENTIAL ACTIVATION METHODOLOGY\n",
      "================================================================================\n",
      "\n",
      "The student computed: ||mean_activation_sarc - mean_activation_lit||_2\n",
      "This measures the L2 norm of the DIFFERENCE between mean activation vectors.\n",
      "\n",
      "This is DIFFERENT from: mean(||activation_sarc||) - mean(||activation_lit||)\n",
      "which measures difference in activation MAGNITUDES.\n",
      "\n",
      "Re-testing m2 using student's methodology:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sarcastic activation vector shape: torch.Size([768])\n",
      "Mean literal activation vector shape: torch.Size([768])\n",
      "L2 norm of difference: 6.418\n",
      "\n",
      "This measures how DIFFERENT the activation patterns are, not which is larger.\n",
      "A high value means sarcastic and literal activate m2 in DIFFERENT WAYS.\n",
      "\n",
      "However, this DOES NOT tell us:\n",
      "  - Whether m2 'detects' sarcasm (could just be random variation)\n",
      "  - Whether m2 is CAUSAL for sarcasm detection\n",
      "  - Whether the differences are meaningful or just noise\n"
     ]
    }
   ],
   "source": [
    "# Investigate using the student's methodology more precisely\n",
    "# The student computed ||mean_sarc - mean_lit||_2\n",
    "# This measures the DIFFERENCE in activation patterns, not magnitude\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATING STUDENT'S DIFFERENTIAL ACTIVATION METHODOLOGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nThe student computed: ||mean_activation_sarc - mean_activation_lit||_2\")\n",
    "print(\"This measures the L2 norm of the DIFFERENCE between mean activation vectors.\")\n",
    "print(\"\\nThis is DIFFERENT from: mean(||activation_sarc||) - mean(||activation_lit||)\")\n",
    "print(\"which measures difference in activation MAGNITUDES.\\n\")\n",
    "\n",
    "# Re-compute using student's methodology\n",
    "print(\"Re-testing m2 using student's methodology:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get full activation vectors (not just norms)\n",
    "m2_sarc_acts = get_activations(model, test_data['sarcastic'], 'm2')\n",
    "m2_lit_acts = get_activations(model, test_data['literal'], 'm2')\n",
    "\n",
    "# Compute mean activation vector for each category (average over all examples and positions)\n",
    "m2_sarc_mean_vec = torch.stack([act.mean(dim=0) for act in m2_sarc_acts]).mean(dim=0)\n",
    "m2_lit_mean_vec = torch.stack([act.mean(dim=0) for act in m2_lit_acts]).mean(dim=0)\n",
    "\n",
    "# Compute L2 norm of the difference\n",
    "m2_diff_norm = (m2_sarc_mean_vec - m2_lit_mean_vec).norm().item()\n",
    "\n",
    "print(f\"Mean sarcastic activation vector shape: {m2_sarc_mean_vec.shape}\")\n",
    "print(f\"Mean literal activation vector shape: {m2_lit_mean_vec.shape}\")\n",
    "print(f\"L2 norm of difference: {m2_diff_norm:.3f}\")\n",
    "\n",
    "print(\"\\nThis measures how DIFFERENT the activation patterns are, not which is larger.\")\n",
    "print(\"A high value means sarcastic and literal activate m2 in DIFFERENT WAYS.\")\n",
    "print(\"\\nHowever, this DOES NOT tell us:\")\n",
    "print(\"  - Whether m2 'detects' sarcasm (could just be random variation)\")\n",
    "print(\"  - Whether m2 is CAUSAL for sarcasm detection\")\n",
    "print(\"  - Whether the differences are meaningful or just noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a35ba3",
   "metadata": {},
   "source": [
    "## Critical Evaluation Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Methodological Limitation**: The student used differential activation (L2 norm of mean activation difference) which measures how DIFFERENT activation patterns are, NOT whether components causally detect sarcasm.\n",
    "\n",
    "2. **No Causal Validation**: The student never performed:\n",
    "   - Ablation studies (removing components and measuring impact)\n",
    "   - Behavioral testing (circuit-only model performance)\n",
    "   - Intervention experiments (patching activations)\n",
    "\n",
    "3. **Hypothesis Testing Failure**: When we test the hypothesized functions empirically:\n",
    "   - **Only 1/6 components** show patterns consistent with their hypotheses (16.7% pass rate)\n",
    "   - **m2** (claimed \"primary detector\") shows OPPOSITE pattern (lower activation on sarcastic)\n",
    "   - **All MLPs** show negative or inconsistent differentials\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The student's component interpretations are **NOT VALIDATED** by empirical testing. The circuit may contain components that activate differently for sarcasm vs literal text, but this doesn't mean:\n",
    "- They causally contribute to sarcasm detection\n",
    "- The interpretations are correct\n",
    "- The circuit actually performs sarcasm detection\n",
    "\n",
    "**VERDICT**: The student identified components with differential activation patterns but **FAILED to validate** their functional interpretations or demonstrate that the circuit performs sarcasm detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-10-21-22_HiddenNeuronTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
