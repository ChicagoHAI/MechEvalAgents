{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bf4f34",
   "metadata": {},
   "source": [
    "# Hidden Test: Neuron Function Validation\n",
    "\n",
    "## Testing whether each circuit component matches its hypothesized function\n",
    "\n",
    "Based on the instructor's hypothesis and student's findings, we will test:\n",
    "1. **m2**: Primary sarcasm detector - should show highest differential on sarcastic vs literal\n",
    "2. **m0, m1**: Early encoding - should process sentiment words\n",
    "3. **m7-m11**: Late integration - should contribute to final output\n",
    "4. **a11.h8, a11.h0**: Output heads - should integrate final signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00469ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "repo_path = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-10_20-48-00'\n",
    "\n",
    "# Load the circuit\n",
    "with open(f'{repo_path}/results/real_circuits_1.json', 'r') as f:\n",
    "    circuit = json.load(f)\n",
    "    \n",
    "print(f\"Circuit has {len(circuit['nodes'])} components\")\n",
    "print(f\"MLPs: {[n for n in circuit['nodes'] if n.startswith('m')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc52b5",
   "metadata": {},
   "source": [
    "## Load Model and Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Load GPT2-small\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "print(f\"Model loaded: {model.cfg.model_name}\")\n",
    "print(f\"Layers: {model.cfg.n_layers}, Heads: {model.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset for validation\n",
    "# These are carefully constructed pairs to test hypothesized functions\n",
    "\n",
    "test_data = {\n",
    "    \"sarcastic\": [\n",
    "        \"Oh great, another meeting at 7 AM.\",\n",
    "        \"Wow, I just love getting stuck in traffic.\",\n",
    "        \"Fantastic, my laptop crashed right before the deadline.\",\n",
    "        \"Perfect, exactly what I needed today - more problems.\",\n",
    "        \"Oh wonderful, it's raining on my vacation.\",\n",
    "        \"Just what I wanted, another Monday.\",\n",
    "        \"How lovely, the printer is jammed again.\",\n",
    "        \"Brilliant, I forgot my wallet at home.\",\n",
    "        \"Awesome, the elevator is broken.\",\n",
    "        \"Terrific, we have a pop quiz today.\"\n",
    "    ],\n",
    "    \"literal\": [\n",
    "        \"I'm excited about the meeting tomorrow morning.\",\n",
    "        \"I really enjoy my peaceful morning commute.\",\n",
    "        \"I successfully submitted my project before the deadline.\",\n",
    "        \"This is exactly what I needed today.\",\n",
    "        \"I'm happy to have a relaxing day off.\",\n",
    "        \"I'm looking forward to starting the week.\",\n",
    "        \"The printer is working perfectly today.\",\n",
    "        \"I remembered to bring my wallet.\",\n",
    "        \"The elevator works great in this building.\",\n",
    "        \"I studied well for the quiz today.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Test dataset: {len(test_data['sarcastic'])} sarcastic, {len(test_data['literal'])} literal sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7ffb8",
   "metadata": {},
   "source": [
    "## Test 1: m2 as Primary Sarcasm Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_activations(model, text, layer):\n",
    "    \"\"\"Get MLP output activations for a given layer.\"\"\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get MLP output\n",
    "    mlp_out = cache[f'blocks.{layer}.hook_mlp_out']\n",
    "    return mlp_out.mean(dim=1).cpu().numpy()  # Average over sequence positions\n",
    "\n",
    "# Test m2 differential activation\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 1: m2 as Primary Sarcasm Detector\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "m2_sarc_activations = []\n",
    "m2_lit_activations = []\n",
    "\n",
    "for sarc, lit in zip(test_data['sarcastic'], test_data['literal']):\n",
    "    sarc_act = get_mlp_activations(model, sarc, 2)\n",
    "    lit_act = get_mlp_activations(model, lit, 2)\n",
    "    m2_sarc_activations.append(sarc_act)\n",
    "    m2_lit_activations.append(lit_act)\n",
    "\n",
    "m2_sarc_mean = np.mean(m2_sarc_activations, axis=0)\n",
    "m2_lit_mean = np.mean(m2_lit_activations, axis=0)\n",
    "m2_diff = np.linalg.norm(m2_sarc_mean - m2_lit_mean)\n",
    "\n",
    "print(f\"\\nm2 differential activation: {m2_diff:.4f}\")\n",
    "print(\"\\nExpected: High differential (student found 32.47)\")\n",
    "if m2_diff > 20:\n",
    "    print(\"✅ PASS: m2 shows strong differential activation as hypothesized\")\n",
    "elif m2_diff > 10:\n",
    "    print(\"⚠️ PARTIAL: m2 shows moderate differential\")\n",
    "else:\n",
    "    print(\"❌ FAIL: m2 shows weak differential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e57d5",
   "metadata": {},
   "source": [
    "## Test 2: Compare m2 to Other MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 2: m2 Should Have Highest Differential Among Early MLPs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mlp_differentials = {}\n",
    "\n",
    "for layer in range(6):  # Test early to mid layers\n",
    "    sarc_acts = []\n",
    "    lit_acts = []\n",
    "    \n",
    "    for sarc, lit in zip(test_data['sarcastic'], test_data['literal']):\n",
    "        sarc_act = get_mlp_activations(model, sarc, layer)\n",
    "        lit_act = get_mlp_activations(model, lit, layer)\n",
    "        sarc_acts.append(sarc_act)\n",
    "        lit_acts.append(lit_act)\n",
    "    \n",
    "    sarc_mean = np.mean(sarc_acts, axis=0)\n",
    "    lit_mean = np.mean(lit_acts, axis=0)\n",
    "    diff = np.linalg.norm(sarc_mean - lit_mean)\n",
    "    mlp_differentials[f'm{layer}'] = diff\n",
    "    print(f\"m{layer}: {diff:.4f}\")\n",
    "\n",
    "# Check if m2 is highest\n",
    "sorted_mlps = sorted(mlp_differentials.items(), key=lambda x: x[1], reverse=True)\n",
    "highest_mlp = sorted_mlps[0][0]\n",
    "\n",
    "print(f\"\\nHighest differential MLP: {highest_mlp}\")\n",
    "if highest_mlp == 'm2':\n",
    "    print(\"✅ PASS: m2 has highest differential as hypothesized\")\n",
    "else:\n",
    "    print(f\"⚠️ PARTIAL: {highest_mlp} has highest differential, not m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afed51",
   "metadata": {},
   "source": [
    "## Test 3: Late MLP Integration Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b51632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 3: Late MLPs (m7-m11) Should Show Increasing Activation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "late_mlp_diffs = {}\n",
    "\n",
    "for layer in range(7, 12):\n",
    "    sarc_acts = []\n",
    "    lit_acts = []\n",
    "    \n",
    "    for sarc, lit in zip(test_data['sarcastic'], test_data['literal']):\n",
    "        sarc_act = get_mlp_activations(model, sarc, layer)\n",
    "        lit_act = get_mlp_activations(model, lit, layer)\n",
    "        sarc_acts.append(sarc_act)\n",
    "        lit_acts.append(lit_act)\n",
    "    \n",
    "    sarc_mean = np.mean(sarc_acts, axis=0)\n",
    "    lit_mean = np.mean(lit_acts, axis=0)\n",
    "    diff = np.linalg.norm(sarc_mean - lit_mean)\n",
    "    late_mlp_diffs[f'm{layer}'] = diff\n",
    "    print(f\"m{layer}: {diff:.4f}\")\n",
    "\n",
    "# Check for increasing pattern\n",
    "values = list(late_mlp_diffs.values())\n",
    "is_increasing = values[-1] > values[0]\n",
    "\n",
    "print(f\"\\nm7 differential: {values[0]:.4f}\")\n",
    "print(f\"m11 differential: {values[-1]:.4f}\")\n",
    "print(f\"Ratio m11/m7: {values[-1]/values[0]:.2f}x\")\n",
    "\n",
    "if is_increasing and values[-1] > values[0] * 1.5:\n",
    "    print(\"✅ PASS: Late MLPs show expected increasing integration pattern\")\n",
    "else:\n",
    "    print(\"⚠️ PARTIAL: Late MLPs don't show clear increasing pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161a8a9",
   "metadata": {},
   "source": [
    "## Test 4: Attention Head a11.h8 as Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_head_activations(model, text, layer, head):\n",
    "    \"\"\"Get attention head output activations.\"\"\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get attention output for specific head\n",
    "    attn_out = cache[f'blocks.{layer}.attn.hook_result']\n",
    "    head_out = attn_out[:, :, head, :]  # Select specific head\n",
    "    return head_out.mean(dim=1).cpu().numpy()  # Average over sequence\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 4: a11.h8 as Primary Output Head\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test a11.h8\n",
    "a11_h8_sarc = []\n",
    "a11_h8_lit = []\n",
    "\n",
    "for sarc, lit in zip(test_data['sarcastic'], test_data['literal']):\n",
    "    sarc_act = get_attention_head_activations(model, sarc, 11, 8)\n",
    "    lit_act = get_attention_head_activations(model, lit, 11, 8)\n",
    "    a11_h8_sarc.append(sarc_act)\n",
    "    a11_h8_lit.append(lit_act)\n",
    "\n",
    "a11_h8_sarc_mean = np.mean(a11_h8_sarc, axis=0)\n",
    "a11_h8_lit_mean = np.mean(a11_h8_lit, axis=0)\n",
    "a11_h8_diff = np.linalg.norm(a11_h8_sarc_mean - a11_h8_lit_mean)\n",
    "\n",
    "print(f\"a11.h8 differential: {a11_h8_diff:.4f}\")\n",
    "\n",
    "# Compare to other L11 heads\n",
    "l11_head_diffs = {}\n",
    "for head in range(12):\n",
    "    sarc_acts = []\n",
    "    lit_acts = []\n",
    "    \n",
    "    for sarc, lit in zip(test_data['sarcastic'], test_data['literal']):\n",
    "        sarc_act = get_attention_head_activations(model, sarc, 11, head)\n",
    "        lit_act = get_attention_head_activations(model, lit, 11, head)\n",
    "        sarc_acts.append(sarc_act)\n",
    "        lit_acts.append(lit_act)\n",
    "    \n",
    "    sarc_mean = np.mean(sarc_acts, axis=0)\n",
    "    lit_mean = np.mean(lit_acts, axis=0)\n",
    "    diff = np.linalg.norm(sarc_mean - lit_mean)\n",
    "    l11_head_diffs[head] = diff\n",
    "\n",
    "sorted_heads = sorted(l11_head_diffs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 Layer 11 heads by differential:\")\n",
    "for head, diff in sorted_heads[:5]:\n",
    "    marker = \"⭐\" if head == 8 else \"  \"\n",
    "    print(f\"{marker} a11.h{head}: {diff:.4f}\")\n",
    "\n",
    "# Check if h8 is in top 3\n",
    "top_heads = [h for h, _ in sorted_heads[:3]]\n",
    "if 8 in top_heads:\n",
    "    print(\"\\n✅ PASS: a11.h8 is among top output heads\")\n",
    "else:\n",
    "    rank = [i for i, (h, _) in enumerate(sorted_heads) if h == 8][0] + 1\n",
    "    print(f\"\\n⚠️ PARTIAL: a11.h8 ranks #{rank} among L11 heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9c93a",
   "metadata": {},
   "source": [
    "## Test 5: Ablation Test - Circuit Necessity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ad78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 5: Ablation Test - Key Components Should Be Necessary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def ablate_mlp(model, text, layer):\n",
    "    \"\"\"Run model with MLP layer ablated (zeroed).\"\"\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    \n",
    "    # Define hook to zero out MLP\n",
    "    def zero_mlp_hook(activation, hook):\n",
    "        return torch.zeros_like(activation)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.run_with_hooks(\n",
    "            tokens,\n",
    "            fwd_hooks=[(f'blocks.{layer}.hook_mlp_out', zero_mlp_hook)]\n",
    "        )\n",
    "    \n",
    "    return logits[:, -1, :].cpu()  # Last token logits\n",
    "\n",
    "# Test effect of ablating m2\n",
    "print(\"\\nAblating m2 (primary sarcasm detector):\")\n",
    "\n",
    "sarc_example = test_data['sarcastic'][0]\n",
    "lit_example = test_data['literal'][0]\n",
    "\n",
    "# Normal forward pass\n",
    "with torch.no_grad():\n",
    "    tokens_sarc = model.to_tokens(sarc_example)\n",
    "    tokens_lit = model.to_tokens(lit_example)\n",
    "    normal_sarc = model(tokens_sarc)[:, -1, :]\n",
    "    normal_lit = model(tokens_lit)[:, -1, :]\n",
    "\n",
    "normal_diff = torch.norm(normal_sarc - normal_lit).item()\n",
    "\n",
    "# Ablated forward pass\n",
    "ablated_sarc = ablate_mlp(model, sarc_example, 2)\n",
    "ablated_lit = ablate_mlp(model, lit_example, 2)\n",
    "ablated_diff = torch.norm(ablated_sarc - ablated_lit).item()\n",
    "\n",
    "print(f\"Normal output difference: {normal_diff:.4f}\")\n",
    "print(f\"With m2 ablated: {ablated_diff:.4f}\")\n",
    "print(f\"Change: {((ablated_diff - normal_diff) / normal_diff * 100):.1f}%\")\n",
    "\n",
    "if abs(ablated_diff - normal_diff) / normal_diff > 0.1:\n",
    "    print(\"\\n✅ PASS: m2 ablation significantly affects output\")\n",
    "else:\n",
    "    print(\"\\n⚠️ PARTIAL: m2 ablation has limited effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cba30",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HIDDEN TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = {\n",
    "    \"test_1_m2_primary_detector\": {\n",
    "        \"description\": \"m2 shows high differential activation\",\n",
    "        \"passed\": m2_diff > 10,\n",
    "        \"value\": float(m2_diff)\n",
    "    },\n",
    "    \"test_2_m2_highest_early\": {\n",
    "        \"description\": \"m2 has highest differential among early MLPs\",\n",
    "        \"passed\": highest_mlp == 'm2',\n",
    "        \"highest\": highest_mlp\n",
    "    },\n",
    "    \"test_3_late_integration\": {\n",
    "        \"description\": \"Late MLPs show increasing pattern\",\n",
    "        \"passed\": is_increasing and values[-1] > values[0] * 1.2,\n",
    "        \"m7\": float(values[0]),\n",
    "        \"m11\": float(values[-1])\n",
    "    },\n",
    "    \"test_4_a11_h8_output\": {\n",
    "        \"description\": \"a11.h8 is among top output heads\",\n",
    "        \"passed\": 8 in top_heads,\n",
    "        \"rank\": [i for i, (h, _) in enumerate(sorted_heads) if h == 8][0] + 1\n",
    "    },\n",
    "    \"test_5_m2_ablation\": {\n",
    "        \"description\": \"m2 ablation affects output\",\n",
    "        \"passed\": abs(ablated_diff - normal_diff) / normal_diff > 0.05,\n",
    "        \"change_pct\": float(((ablated_diff - normal_diff) / normal_diff * 100))\n",
    "    }\n",
    "}\n",
    "\n",
    "passed_tests = sum(1 for t in test_results.values() if t['passed'])\n",
    "total_tests = len(test_results)\n",
    "\n",
    "print(f\"\\nResults: {passed_tests}/{total_tests} tests passed\")\n",
    "print(\"\\nDetailed Results:\")\n",
    "for test_name, result in test_results.items():\n",
    "    status = \"✅ PASS\" if result['passed'] else \"❌ FAIL\"\n",
    "    print(f\"\\n{test_name}:\")\n",
    "    print(f\"  {result['description']}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "\n",
    "overall_pass = passed_tests >= 3\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"OVERALL VERDICT: {'✅ PASS' if overall_pass else '❌ FAIL'}\")\n",
    "print(f\"The circuit components {'match' if overall_pass else 'do not match'} their hypothesized functions\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "output_path = f'{repo_path}/evaluation/hidden_test_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"Test results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
