{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659e57bf",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/smallyan/critic_model_mechinterp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A40\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to the correct working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24c759b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuits_claude_2025-11-09_14-46-37/\n",
      "  results/\n",
      "    real_circuits_1.json\n",
      "    circuit_visualization.png\n",
      "  notebooks/\n",
      "    2025-11-09-14-46_IOI_Circuit_Analysis.ipynb\n",
      "    code_walk_Md.ipynb\n",
      "    documentation_Md.ipynb\n",
      "    plan_Md.ipynb\n",
      "  logs/\n",
      "    plan.md\n",
      "    circuit_prompt_ioi_claude.log\n",
      "    code_walk.md\n",
      "    documentation.md\n"
     ]
    }
   ],
   "source": [
    "# Explore the repository structure\n",
    "repo_root = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-09_14-46-37'\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(repo_root):\n",
    "    level = root.replace(repo_root, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:20]:  # Limit files shown per directory\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 20:\n",
    "        print(f'{subindent}... and {len(files) - 20} more files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62622fb5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLAN CONTENT ===\n",
      "# IOI Circuit Analysis - Research Plan\n",
      "\n",
      "## Objective\n",
      "\n",
      "Identify a precise circuit in GPT2-small that implements the Indirect Object Identification (IOI) task while staying within a write budget of 11,200 dimensions.\n",
      "\n",
      "## Hypothesis\n",
      "\n",
      "The IOI circuit consists of three main components:\n",
      "\n",
      "1. **Duplicate Token Heads**: Attend from S2 to S1, signaling token duplication\n",
      "2. **S-Inhibition Heads**: Attend from END to S2, inhibiting attention to the subject\n",
      "3. **Name-Mover Heads**: Attend from END to IO, copying the indirect object to the output\n",
      "\n",
      "## Methodology\n",
      "\n",
      "### Phase 1: Data Exploration\n",
      "- Load GPT2-small model via TransformerLens\n",
      "- Load mib-bench/ioi dataset\n",
      "- Analyze dataset structure and identify key positions (S1, S2, IO, END)\n",
      "- Establish baseline model performance on IOI task\n",
      "\n",
      "### Phase 2: Attention Pattern Analysis\n",
      "- Run model with activation caching\n",
      "- For each layer and head, calculate attention patterns:\n",
      "  - **Duplicate Token Heads**: Measure attention from S2 → S1\n",
      "  - **S-Inhibition Heads**: Measure attention from END → S2\n",
      "  - **Name-Mover Heads**: Measure attention from END → IO\n",
      "- Rank heads by their alignment with hypothesized behavior\n",
      "\n",
      "### Phase 3: Circuit Selection\n",
      "- Select top-k heads from each category\n",
      "- Include supporting MLPs from relevant layers\n",
      "- Ensure total write budget ≤ 11,200 dimensions\n",
      "  - Each head writes: d_model / n_heads = 768 / 12 = 64 dims\n",
      "  - Each MLP writes: d_model = 768 dims\n",
      "\n",
      "### Phase 4: Validation\n",
      "- Verify all nodes are in allowed src_nodes\n",
      "- Verify naming conventions (a{layer}.h{head}, m{layer})\n",
      "- Verify budget constraints\n",
      "- Document circuit composition and performance\n",
      "\n",
      "## Expected Outcomes\n",
      "\n",
      "- A JSON file (real_circuits_1.json) containing the selected circuit nodes\n",
      "- Comprehensive documentation of findings\n",
      "- Validation that the circuit stays within budget\n",
      "- Analysis of whether findings support the IOI hypothesis\n",
      "\n",
      "## Success Criteria\n",
      "\n",
      "1. Circuit contains ≤ 11,200 dimensional writes\n",
      "2. All nodes follow naming conventions\n",
      "3. Circuit include\n",
      "\n",
      "... (continuing)\n"
     ]
    }
   ],
   "source": [
    "# Read the plan file\n",
    "with open(f'{repo_root}/logs/plan.md', 'r') as f:\n",
    "    plan_content = f.read()\n",
    "\n",
    "print(\"=== PLAN CONTENT ===\")\n",
    "print(plan_content[:2000])\n",
    "print(\"\\n... (continuing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a9bda4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# IOI Circuit Analysis - Research Plan\n",
      "\n",
      "## Objective\n",
      "\n",
      "Identify a precise circuit in GPT2-small that implements the Indirect Object Identification (IOI) task while staying within a write budget of 11,200 dimensions.\n",
      "\n",
      "## Hypothesis\n",
      "\n",
      "The IOI circuit consists of three main components:\n",
      "\n",
      "1. **Duplicate Token Heads**: Attend from S2 to S1, signaling token duplication\n",
      "2. **S-Inhibition Heads**: Attend from END to S2, inhibiting attention to the subject\n",
      "3. **Name-Mover Heads**: Attend from END to IO, copying the indirect object to the output\n",
      "\n",
      "## Methodology\n",
      "\n",
      "### Phase 1: Data Exploration\n",
      "- Load GPT2-small model via TransformerLens\n",
      "- Load mib-bench/ioi dataset\n",
      "- Analyze dataset structure and identify key positions (S1, S2, IO, END)\n",
      "- Establish baseline model performance on IOI task\n",
      "\n",
      "### Phase 2: Attention Pattern Analysis\n",
      "- Run model with activation caching\n",
      "- For each layer and head, calculate attention patterns:\n",
      "  - **Duplicate Token Heads**: Measure attention from S2 → S1\n",
      "  - **S-Inhibition Heads**: Measure attention from END → S2\n",
      "  - **Name-Mover Heads**: Measure attention from END → IO\n",
      "- Rank heads by their alignment with hypothesized behavior\n",
      "\n",
      "### Phase 3: Circuit Selection\n",
      "- Select top-k heads from each category\n",
      "- Include supporting MLPs from relevant layers\n",
      "- Ensure total write budget ≤ 11,200 dimensions\n",
      "  - Each head writes: d_model / n_heads = 768 / 12 = 64 dims\n",
      "  - Each MLP writes: d_model = 768 dims\n",
      "\n",
      "### Phase 4: Validation\n",
      "- Verify all nodes are in allowed src_nodes\n",
      "- Verify naming conventions (a{layer}.h{head}, m{layer})\n",
      "- Verify budget constraints\n",
      "- Document circuit composition and performance\n",
      "\n",
      "## Expected Outcomes\n",
      "\n",
      "- A JSON file (real_circuits_1.json) containing the selected circuit nodes\n",
      "- Comprehensive documentation of findings\n",
      "- Validation that the circuit stays within budget\n",
      "- Analysis of whether findings support the IOI hypothesis\n",
      "\n",
      "## Success Criteria\n",
      "\n",
      "1. Circuit contains ≤ 11,200 dimensional writes\n",
      "2. All nodes follow naming conventions\n",
      "3. Circuit includes representatives from all three hypothesized head types\n",
      "4. Documentation clearly explains methodology and results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the full plan\n",
    "with open(f'{repo_root}/logs/plan.md', 'r') as f:\n",
    "    plan_content = f.read()\n",
    "\n",
    "print(plan_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325f04ee",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# IOI Circuit Analysis - Code Walkthrough\n",
      "\n",
      "## Overview\n",
      "\n",
      "This document provides a detailed walkthrough of the implementation used to identify the IOI circuit in GPT2-small.\n",
      "\n",
      "## Setup and Initialization\n",
      "\n",
      "### 1. Environment Configuration\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
      "\n",
      "# Check for GPU availability\n",
      "import torch\n",
      "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
      "```\n",
      "\n",
      "**Purpose**: Set working directory and configure compute device. Using CUDA significantly speeds up model inference and activation caching.\n",
      "\n",
      "### 2. Load Model\n",
      "\n",
      "```python\n",
      "from transformer_lens import HookedTransformer\n",
      "\n",
      "model = HookedTransformer.from_pretrained('gpt2-small', device=device)\n",
      "```\n",
      "\n",
      "**Key Configuration**:\n",
      "- `n_layers`: 12\n",
      "- `n_heads`: 12  \n",
      "- `d_model`: 768\n",
      "- `d_head`: 64\n",
      "\n",
      "**Why TransformerLens**: Provides easy access to activation caching and intervention capabilities needed for mechanistic interpretability.\n",
      "\n",
      "### 3. Load Dataset\n",
      "\n",
      "```python\n",
      "from datasets import load_dataset\n",
      "\n",
      "dataset = load_dataset(\"mib-bench/ioi\")\n",
      "ioi_data = dataset['train']\n",
      "```\n",
      "\n",
      "**Dataset Structure**:\n",
      "- Each example has a `prompt`, `choices`, `answerKey`, and `metadata`\n",
      "- Metadata includes `subject`, `indirect_object`, `object`, and `place`\n",
      "- Multiple counterfactual variants provided for each example\n",
      "\n",
      "## Data Processing\n",
      "\n",
      "### 4. Position Identification\n",
      "\n",
      "```python\n",
      "def find_positions(prompt_idx):\n",
      "    tokens_str = model.to_str_tokens(prompts[prompt_idx])\n",
      "    s_name = s_names[prompt_idx]\n",
      "    \n",
      "    s1_pos = None\n",
      "    s2_pos = None\n",
      "    end_pos = len(tokens_str) - 1\n",
      "    \n",
      "    for i, token in enumerate(tokens_str):\n",
      "        if s_name in token:\n",
      "            if s1_pos is None:\n",
      "                s1_pos = i\n",
      "            else:\n",
      "                s2_pos = i\n",
      "                break\n",
      "    \n",
      "    return s1_pos, s2_pos, end_pos, tokens_str\n",
      "```\n",
      "\n",
      "**Purpose**: Locate critical positions in each sentence:\n",
      "- **S1**: First subject mention (typically position 2)\n",
      "- **S2**: Second subject mention (varies by sentence structure)\n",
      "- **END**: Last token position (where model predicts next token)\n",
      "\n",
      "**Challenge**: Names are tokenized differently, so we search for name substrings in tokens rather than exact matches.\n",
      "\n",
      "### 5. Baseline Evaluation\n",
      "\n",
      "```python\n",
      "logits, cache = model.run_with_cache(tokens)\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    _, _, end_pos, _ = find_positions(i)\n",
      "    end_logits = logits[i, end_pos, :]\n",
      "    \n",
      "    io_token = model.to_single_token(' ' + io_names[i])\n",
      "    s_token = model.to_single_token(' ' + s_names[i])\n",
      "    \n",
      "    predicted_io = end_logits[io_token] > end_logits[s_token]\n",
      "```\n",
      "\n",
      "**Purpose**: Measure model's baseline performance by comparing logits for IO vs. Subject tokens.\n",
      "\n",
      "**Result**: 94% accuracy demonstrates model has learned IOI behavior.\n",
      "\n",
      "## Attention Pattern Analysis\n",
      "\n",
      "### 6. Duplicate Token Head Detection\n",
      "\n",
      "```python\n",
      "duplicate_token_scores = np.zeros((n_layers, n_heads))\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    s1_pos, s2_pos, _, _ = find_positions(i)\n",
      "    \n",
      "    for layer in range(n_layers):\n",
      "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
      "        \n",
      "        for head in range(n_heads):\n",
      "            attn_s2_to_s1 = attn_pattern[head, s2_pos, s1_pos].item()\n",
      "            duplicate_token_scores[layer, head] += attn_s2_to_s1\n",
      "\n",
      "duplicate_token_scores /= len(prompts)\n",
      "```\n",
      "\n",
      "**Mechanism**:\n",
      "1. Access cached attention patterns from each head\n",
      "2. Extract attention weight from S2 position to S1 position\n",
      "3. Average across all examples\n",
      "4. Higher scores indicate heads that specialize in duplicate token detection\n",
      "\n",
      "**Top Result**: a3.h0 with 0.72 average attention from S2→S1\n",
      "\n",
      "### 7. S-Inhibition Head Detection\n",
      "\n",
      "```python\n",
      "s_inhibition_scores = np.zeros((n_layers, n_heads))\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    s1_pos, s2_pos, end_pos, _ = find_positions(i)\n",
      "    \n",
      "    for layer in range(n_layers):\n",
      "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
      "        \n",
      "        for head in range(n_heads):\n",
      "            attn_end_to_s2 = attn_pattern[head, end_pos, s2_pos].item()\n",
      "            s_inhibition_scores[layer, head] += attn_end_to_s2\n",
      "\n",
      "s_inhibition_scores /= len(prompts)\n",
      "```\n",
      "\n",
      "**Mechanism**:\n",
      "1. Extract attention weight from END position to S2 position\n",
      "2. These heads attend to the subject to inhibit it from being predicted\n",
      "3. Works in conjunction with Name-Mover heads\n",
      "\n",
      "**Top Result**: a8.h6 with 0.74 average attention from END→S2\n",
      "\n",
      "### 8. Name-Mover Head Detection\n",
      "\n",
      "```python\n",
      "name_mover_scores = np.zeros((n_layers, n_heads))\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    tokens_str = model.to_str_tokens(prompts[i])\n",
      "    s1_pos, s2_pos, end_pos, _ = find_positions(i)\n",
      "    \n",
      "    # Find IO position\n",
      "    io_name = io_names[i]\n",
      "    io_pos = None\n",
      "    for j, token in enumerate(tokens_str):\n",
      "        if io_name in token and j != s1_pos and j != s2_pos:\n",
      "            io_pos = j\n",
      "            break\n",
      "    \n",
      "    for layer in range(n_layers):\n",
      "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
      "        \n",
      "        for head in range(n_heads):\n",
      "            attn_end_to_io = attn_pattern[head, end_pos, io_pos].item()\n",
      "            name_mover_scores[layer, head] += attn_end_to_io\n",
      "\n",
      "name_mover_scores /= len(prompts)\n",
      "```\n",
      "\n",
      "**Mechanism**:\n",
      "1. First locate IO position (different from S1/S2)\n",
      "2. Extract attention weight from END to IO\n",
      "3. These heads copy the IO token to the output\n",
      "\n",
      "**Top Result**: a9.h9 with 0.80 average attention from END→IO\n",
      "\n",
      "## Circuit Selection\n",
      "\n",
      "### 9. Head Selection Strategy\n",
      "\n",
      "```python\n",
      "# Select top heads from each category\n",
      "duplicate_heads_to_include = [\n",
      "    (layer, head) for _, layer, head in top_duplicate_heads[:3]\n",
      "]\n",
      "\n",
      "s_inhibition_heads_to_include = [\n",
      "    (layer, head) for _, layer, head in top_s_inhibition_heads[:3]\n",
      "]\n",
      "\n",
      "name_mover_heads_to_include = [\n",
      "    (layer, head) for _, layer, head in top_name_mover_heads[:4]\n",
      "]\n",
      "\n",
      "selected_heads = list(set(\n",
      "    duplicate_heads_to_include +\n",
      "    s_inhibition_heads_to_include +\n",
      "    name_mover_heads_to_include\n",
      "))\n",
      "```\n",
      "\n",
      "**Strategy**:\n",
      "1. Start with top performers from each category\n",
      "2. Remove duplicates (some heads rank high in multiple categories)\n",
      "3. Calculate budget used so far\n",
      "\n",
      "### 10. MLP Selection\n",
      "\n",
      "```python\n",
      "head_layers = sorted(set([layer for layer, _ in selected_heads]))\n",
      "\n",
      "# Include MLPs from layers with selected heads plus supporting layers\n",
      "selected_mlps = [0, 1]  # Early layers for feature extraction\n",
      "selected_mlps.extend(head_layers)  # Layers with attention heads\n",
      "selected_mlps.extend([2, 4, 5, 6])  # Middle layers for transformation\n",
      "\n",
      "selected_mlps = sorted(set(selected_mlps))\n",
      "```\n",
      "\n",
      "**Rationale**:\n",
      "- MLPs provide nonlinear transformations essential for circuit computation\n",
      "- Include early layers for basic feature extraction\n",
      "- Include layers with selected heads for local computation\n",
      "- Include middle layers for feature transformation\n",
      "\n",
      "### 11. Budget Maximization\n",
      "\n",
      "```python\n",
      "remaining_budget = 11200 - (len(selected_heads) * 64 + len(selected_mlps) * 768)\n",
      "max_additional_heads = remaining_budget // 64\n",
      "\n",
      "# Combine and sort all candidates\n",
      "all_important_heads = []\n",
      "for score, layer, head in top_duplicate_heads[:15]:\n",
      "    if (layer, head) not in selected_heads:\n",
      "        all_important_heads.append((score, layer, head, 'duplicate'))\n",
      "# ... repeat for other categories\n",
      "\n",
      "all_important_heads.sort(reverse=True)\n",
      "\n",
      "# Add top additional heads\n",
      "for i in range(max_additional_heads):\n",
      "    score, layer, head, category = all_important_heads[i]\n",
      "    selected_heads.append((layer, head))\n",
      "```\n",
      "\n",
      "**Purpose**: Maximize circuit expressiveness by using all available budget.\n",
      "\n",
      "**Result**: Added 21 additional heads, achieving exact 11,200-dimension budget usage.\n",
      "\n",
      "## Validation and Output\n",
      "\n",
      "### 12. Constraint Validation\n",
      "\n",
      "```python\n",
      "# Validate all nodes are in src_nodes\n",
      "for node in circuit_nodes:\n",
      "    if node not in src_nodes:\n",
      "        invalid_nodes.append(node)\n",
      "\n",
      "# Validate naming convention\n",
      "if node.startswith('a'):\n",
      "    # Check format: a{layer}.h{head}\n",
      "    parts = node.split('.')\n",
      "    # Validation logic...\n",
      "\n",
      "# Validate budget\n",
      "total_budget = len(selected_heads) * 64 + len(selected_mlps) * 768\n",
      "assert total_budget <= 11200\n",
      "```\n",
      "\n",
      "**Checks**:\n",
      "1. All nodes exist in allowed source nodes\n",
      "2. Naming follows convention (a{layer}.h{head}, m{layer})\n",
      "3. Total write budget ≤ 11,200 dimensions\n",
      "\n",
      "### 13. Save Circuit\n",
      "\n",
      "```python\n",
      "circuit_data = {\n",
      "    \"nodes\": circuit_nodes\n",
      "}\n",
      "\n",
      "with open('real_circuits_1.json', 'w') as f:\n",
      "    json.dump(circuit_data, f, indent=2)\n",
      "```\n",
      "\n",
      "**Output Format**: JSON with single \"nodes\" key containing ordered list of node names.\n",
      "\n",
      "## Key Implementation Details\n",
      "\n",
      "### Attention Pattern Caching\n",
      "\n",
      "TransformerLens's `run_with_cache` stores all intermediate activations:\n",
      "- `cache['blocks.{layer}.attn.hook_pattern']`: Attention probabilities after softmax\n",
      "- Shape: `[batch, n_heads, seq_len_q, seq_len_k]`\n",
      "- Values range [0, 1] and sum to 1 over key dimension\n",
      "\n",
      "### Averaging Across Examples\n",
      "\n",
      "We average attention scores across examples to find heads with consistent behavior:\n",
      "- Consistent high attention → specialized functionality\n",
      "- Inconsistent attention → general-purpose or context-dependent\n",
      "\n",
      "### Budget Calculation\n",
      "\n",
      "```\n",
      "Head budget = n_heads × (d_model / n_heads_per_layer)\n",
      "            = 31 × 64 = 1,984\n",
      "\n",
      "MLP budget = n_mlps × d_model  \n",
      "           = 12 × 768 = 9,216\n",
      "\n",
      "Total = 1,984 + 9,216 = 11,200 ✓\n",
      "```\n",
      "\n",
      "## Performance Considerations\n",
      "\n",
      "### GPU Acceleration\n",
      "- Model and data moved to CUDA\n",
      "- Batch processing of 100 examples\n",
      "- Activation caching done in single forward pass\n",
      "\n",
      "### Memory Management\n",
      "- Used subset of 100 examples (vs. full 10,000)\n",
      "- Cached activations for all layers at once\n",
      "- Total memory usage: ~2-3 GB GPU RAM\n",
      "\n",
      "### Computational Complexity\n",
      "- Forward pass: O(batch_size × seq_len²)\n",
      "- Attention analysis: O(n_layers × n_heads × n_examples)\n",
      "- Total runtime: ~2-3 minutes on A100\n",
      "\n",
      "## Reproducibility\n",
      "\n",
      "### Random Seeds\n",
      "No random operations used - results are fully deterministic given:\n",
      "1. Model weights (GPT2-small from Hugging Face)\n",
      "2. Dataset (mib-bench/ioi)\n",
      "3. Sample indices (first 100 examples)\n",
      "\n",
      "### Dependencies\n",
      "- `transformer_lens`: For model loading and activation caching\n",
      "- `datasets`: For loading IOI dataset\n",
      "- `torch`: For GPU computation\n",
      "- `numpy`: For numerical operations\n",
      "\n",
      "---\n",
      "\n",
      "**Code Organization**:\n",
      "- All analysis done in single Jupyter notebook\n",
      "- Modular functions for position finding and scoring\n",
      "- Clear separation of analysis phases (exploration → analysis → selection → validation)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the code_walk\n",
    "with open(f'{repo_root}/logs/code_walk.md', 'r') as f:\n",
    "    code_walk_content = f.read()\n",
    "\n",
    "print(code_walk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269081cd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original circuit nodes:\n",
      "Total nodes: 44\n",
      "\n",
      "First 20 nodes:\n",
      "  1. input\n",
      "  2. a0.h1\n",
      "  3. a0.h10\n",
      "  4. a0.h5\n",
      "  5. a0.h6\n",
      "  6. a1.h11\n",
      "  7. a10.h0\n",
      "  8. a10.h1\n",
      "  9. a10.h10\n",
      "  10. a10.h2\n",
      "  11. a10.h3\n",
      "  12. a10.h6\n",
      "  13. a10.h7\n",
      "  14. a11.h10\n",
      "  15. a11.h6\n",
      "  16. a11.h8\n",
      "  17. a3.h0\n",
      "  18. a3.h6\n",
      "  19. a6.h0\n",
      "  20. a7.h3\n",
      "\n",
      "... and 24 more nodes\n"
     ]
    }
   ],
   "source": [
    "# Read the original results to understand what we need to match\n",
    "import json\n",
    "with open(f'{repo_root}/results/real_circuits_1.json', 'r') as f:\n",
    "    original_circuit = json.load(f)\n",
    "\n",
    "print(\"Original circuit nodes:\")\n",
    "print(f\"Total nodes: {len(original_circuit['nodes'])}\")\n",
    "print(\"\\nFirst 20 nodes:\")\n",
    "for i, node in enumerate(original_circuit['nodes'][:20]):\n",
    "    print(f\"  {i+1}. {node}\")\n",
    "print(f\"\\n... and {len(original_circuit['nodes']) - 20} more nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e2e23c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete original circuit:\n",
      " 1. input\n",
      " 2. a0.h1\n",
      " 3. a0.h10\n",
      " 4. a0.h5\n",
      " 5. a0.h6\n",
      " 6. a1.h11\n",
      " 7. a10.h0\n",
      " 8. a10.h1\n",
      " 9. a10.h10\n",
      "10. a10.h2\n",
      "11. a10.h3\n",
      "12. a10.h6\n",
      "13. a10.h7\n",
      "14. a11.h10\n",
      "15. a11.h6\n",
      "16. a11.h8\n",
      "17. a3.h0\n",
      "18. a3.h6\n",
      "19. a6.h0\n",
      "20. a7.h3\n",
      "21. a7.h9\n",
      "22. a8.h10\n",
      "23. a8.h2\n",
      "24. a8.h3\n",
      "25. a8.h5\n",
      "26. a8.h6\n",
      "27. a9.h0\n",
      "28. a9.h2\n",
      "29. a9.h6\n",
      "30. a9.h7\n",
      "31. a9.h8\n",
      "32. a9.h9\n",
      "33. m0\n",
      "34. m1\n",
      "35. m10\n",
      "36. m11\n",
      "37. m2\n",
      "38. m3\n",
      "39. m4\n",
      "40. m5\n",
      "41. m6\n",
      "42. m7\n",
      "43. m8\n",
      "44. m9\n",
      "\n",
      "Breakdown:\n",
      "  Attention heads: 31\n",
      "  MLPs: 12\n",
      "  Other nodes: 1 - ['input']\n",
      "\n",
      "Budget calculation:\n",
      "  Heads: 31 × 64 = 1984\n",
      "  MLPs: 12 × 768 = 9216\n",
      "  Total: 11200 (limit: 11,200)\n"
     ]
    }
   ],
   "source": [
    "# Show all original circuit nodes\n",
    "print(\"Complete original circuit:\")\n",
    "for i, node in enumerate(original_circuit['nodes'], 1):\n",
    "    print(f\"{i:2d}. {node}\")\n",
    "\n",
    "# Count heads vs MLPs\n",
    "heads = [n for n in original_circuit['nodes'] if n.startswith('a')]\n",
    "mlps = [n for n in original_circuit['nodes'] if n.startswith('m')]\n",
    "other = [n for n in original_circuit['nodes'] if not (n.startswith('a') or n.startswith('m'))]\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Attention heads: {len(heads)}\")\n",
    "print(f\"  MLPs: {len(mlps)}\")\n",
    "print(f\"  Other nodes: {len(other)} - {other}\")\n",
    "\n",
    "# Calculate budget\n",
    "head_budget = len(heads) * 64\n",
    "mlp_budget = len(mlps) * 768\n",
    "total_budget = head_budget + mlp_budget\n",
    "\n",
    "print(f\"\\nBudget calculation:\")\n",
    "print(f\"  Heads: {len(heads)} × 64 = {head_budget}\")\n",
    "print(f\"  MLPs: {len(mlps)} × 768 = {mlp_budget}\")\n",
    "print(f\"  Total: {total_budget} (limit: 11,200)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e78bb",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Replication\n",
    "\n",
    "This notebook replicates the IOI (Indirect Object Identification) circuit analysis experiment.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Identify a precise circuit in GPT2-small that implements the IOI task, constrained to a write budget of 11,200 dimensions.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "The IOI circuit consists of three main types of attention heads:\n",
    "1. **Duplicate Token Heads**: Attend from S2 (second subject mention) to S1 (first subject mention)\n",
    "2. **S-Inhibition Heads**: Attend from END to S2, inhibiting the subject\n",
    "3. **Name-Mover Heads**: Attend from END to IO (indirect object), copying it to output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5f12b",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6e330f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading GPT2-small model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "\n",
      "Model configuration:\n",
      "  Layers: 12\n",
      "  Heads per layer: 12\n",
      "  d_model: 768\n",
      "  d_head: 64\n"
     ]
    }
   ],
   "source": [
    "# Install and import required libraries\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load GPT2-small model\n",
    "print(\"\\nLoading GPT2-small model...\")\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)\n",
    "\n",
    "# Print model configuration\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"  d_model: {model.cfg.d_model}\")\n",
    "print(f\"  d_head: {model.cfg.d_head}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76da3bce",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IOI dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n",
      "\n",
      "Sample example:\n",
      "  Prompt: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "  Choices: ['Maria', 'Carl']\n",
      "  Answer Key: 0\n",
      "  Metadata: {'indirect_object': 'Maria', 'object': 'fridge', 'place': 'consulate', 'random_a': 'Bob', 'random_b': 'Max', 'random_c': 'Gene', 'subject': 'Carl'}\n"
     ]
    }
   ],
   "source": [
    "# Load IOI dataset\n",
    "print(\"Loading IOI dataset...\")\n",
    "dataset = load_dataset(\"mib-bench/ioi\")\n",
    "ioi_data = dataset['train']\n",
    "\n",
    "print(f\"Dataset size: {len(ioi_data)}\")\n",
    "print(f\"\\nSample example:\")\n",
    "sample = ioi_data[0]\n",
    "print(f\"  Prompt: {sample['prompt']}\")\n",
    "print(f\"  Choices: {sample['choices']}\")\n",
    "print(f\"  Answer Key: {sample['answerKey']}\")\n",
    "print(f\"  Metadata: {sample['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de97178",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 examples\n",
      "\n",
      "First 5 examples:\n",
      "\n",
      "1. Prompt: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "   Subject (S): Carl\n",
      "   Indirect Object (IO): Maria\n",
      "\n",
      "2. Prompt: After Kevin and Bob spent some time at the racecourse, Kevin offered a duster to\n",
      "   Subject (S): Kevin\n",
      "   Indirect Object (IO): Bob\n",
      "\n",
      "3. Prompt: After Brian and Matt spent some time at the vet, Brian offered a button to\n",
      "   Subject (S): Brian\n",
      "   Indirect Object (IO): Matt\n",
      "\n",
      "4. Prompt: After Brad and Louis went to the room, Brad gave a picture frame to\n",
      "   Subject (S): Brad\n",
      "   Indirect Object (IO): Louis\n",
      "\n",
      "5. Prompt: While Jean and Martin were working at the meeting, Jean gave a plant to\n",
      "   Subject (S): Jean\n",
      "   Indirect Object (IO): Martin\n"
     ]
    }
   ],
   "source": [
    "# Use first 100 examples for analysis (as mentioned in code_walk)\n",
    "n_samples = 100\n",
    "sample_data = ioi_data.select(range(n_samples))\n",
    "\n",
    "# Extract prompts and metadata\n",
    "prompts = [ex['prompt'] for ex in sample_data]\n",
    "io_names = [ex['metadata']['indirect_object'] for ex in sample_data]\n",
    "s_names = [ex['metadata']['subject'] for ex in sample_data]\n",
    "\n",
    "print(f\"Analyzing {n_samples} examples\")\n",
    "print(f\"\\nFirst 5 examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. Prompt: {prompts[i]}\")\n",
    "    print(f\"   Subject (S): {s_names[i]}\")\n",
    "    print(f\"   Indirect Object (IO): {io_names[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33440a1",
   "metadata": {},
   "source": [
    "## Phase 2: Position Identification\n",
    "\n",
    "For each prompt, we need to identify:\n",
    "- **S1**: First occurrence of the subject name\n",
    "- **S2**: Second occurrence of the subject name\n",
    "- **IO**: Position of the indirect object name\n",
    "- **END**: Last token position (where the model makes its prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd356fd6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing position finder:\n",
      "\n",
      "Example 1:\n",
      "  Tokens: ['<|endoftext|>', 'As', ' Carl', ' and', ' Maria', ' left', ' the', ' consulate', ',', ' Carl', ' gave', ' a', ' fridge', ' to']\n",
      "  S1 position 2: ' Carl'\n",
      "  S2 position 9: ' Carl'\n",
      "  IO position 4: ' Maria'\n",
      "  END position 13: ' to'\n",
      "\n",
      "Example 2:\n",
      "  Tokens: ['<|endoftext|>', 'After', ' Kevin', ' and', ' Bob', ' spent', ' some', ' time', ' at', ' the', ' race', 'course', ',', ' Kevin', ' offered', ' a', ' d', 'uster', ' to']\n",
      "  S1 position 2: ' Kevin'\n",
      "  S2 position 13: ' Kevin'\n",
      "  IO position 4: ' Bob'\n",
      "  END position 18: ' to'\n",
      "\n",
      "Example 3:\n",
      "  Tokens: ['<|endoftext|>', 'After', ' Brian', ' and', ' Matt', ' spent', ' some', ' time', ' at', ' the', ' vet', ',', ' Brian', ' offered', ' a', ' button', ' to']\n",
      "  S1 position 2: ' Brian'\n",
      "  S2 position 12: ' Brian'\n",
      "  IO position 4: ' Matt'\n",
      "  END position 16: ' to'\n"
     ]
    }
   ],
   "source": [
    "# Function to find key positions in tokenized prompts\n",
    "def find_key_positions(prompt_idx):\n",
    "    \"\"\"\n",
    "    Find positions of S1, S2, IO, and END in the tokenized prompt.\n",
    "    \n",
    "    Returns:\n",
    "        s1_pos: Position of first subject mention\n",
    "        s2_pos: Position of second subject mention\n",
    "        io_pos: Position of indirect object\n",
    "        end_pos: Position of last token\n",
    "        tokens_str: List of string tokens\n",
    "    \"\"\"\n",
    "    tokens_str = model.to_str_tokens(prompts[prompt_idx])\n",
    "    s_name = s_names[prompt_idx]\n",
    "    io_name = io_names[prompt_idx]\n",
    "    \n",
    "    # Find subject positions (S1 and S2)\n",
    "    s1_pos = None\n",
    "    s2_pos = None\n",
    "    for i, token in enumerate(tokens_str):\n",
    "        if s_name in token:\n",
    "            if s1_pos is None:\n",
    "                s1_pos = i\n",
    "            else:\n",
    "                s2_pos = i\n",
    "                break\n",
    "    \n",
    "    # Find IO position (not at S1 or S2)\n",
    "    io_pos = None\n",
    "    for i, token in enumerate(tokens_str):\n",
    "        if io_name in token and i != s1_pos and i != s2_pos:\n",
    "            io_pos = i\n",
    "            break\n",
    "    \n",
    "    # END is the last token\n",
    "    end_pos = len(tokens_str) - 1\n",
    "    \n",
    "    return s1_pos, s2_pos, io_pos, end_pos, tokens_str\n",
    "\n",
    "# Test on first few examples\n",
    "print(\"Testing position finder:\")\n",
    "for i in range(3):\n",
    "    s1, s2, io, end, tokens = find_key_positions(i)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  S1 position {s1}: '{tokens[s1] if s1 else None}'\")\n",
    "    print(f\"  S2 position {s2}: '{tokens[s2] if s2 else None}'\")\n",
    "    print(f\"  IO position {io}: '{tokens[io] if io else None}'\")\n",
    "    print(f\"  END position {end}: '{tokens[end]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b922f",
   "metadata": {},
   "source": [
    "## Phase 3: Run Model and Cache Activations\n",
    "\n",
    "We'll run the model on all prompts and cache attention patterns for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b0f0d1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token shape: torch.Size([100, 24])\n",
      "\n",
      "Running model with activation caching...\n",
      "Logits shape: torch.Size([100, 24, 50257])\n",
      "Cached activations: 208 components\n",
      "\n",
      "Sample cached components:\n",
      "  hook_embed: torch.Size([100, 24, 768])\n",
      "  hook_pos_embed: torch.Size([100, 24, 768])\n",
      "  blocks.0.hook_resid_pre: torch.Size([100, 24, 768])\n",
      "  blocks.0.ln1.hook_scale: torch.Size([100, 24, 1])\n",
      "  blocks.0.ln1.hook_normalized: torch.Size([100, 24, 768])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all prompts\n",
    "tokens = model.to_tokens(prompts)\n",
    "print(f\"Token shape: {tokens.shape}\")  # [batch_size, seq_len]\n",
    "\n",
    "# Run model with activation caching\n",
    "print(\"\\nRunning model with activation caching...\")\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(f\"Logits shape: {logits.shape}\")  # [batch_size, seq_len, vocab_size]\n",
    "print(f\"Cached activations: {len(cache)} components\")\n",
    "\n",
    "# Show some cached components\n",
    "print(\"\\nSample cached components:\")\n",
    "for i, key in enumerate(list(cache.keys())[:5]):\n",
    "    print(f\"  {key}: {cache[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52c301",
   "metadata": {},
   "source": [
    "## Phase 4: Baseline Performance Evaluation\n",
    "\n",
    "Check if the model correctly predicts the indirect object (IO) over the subject (S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8f67e0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline IOI accuracy: 94.0%\n",
      "Correct predictions: 94/100\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline performance\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(n_samples):\n",
    "    _, _, _, end_pos, _ = find_key_positions(i)\n",
    "    \n",
    "    # Get logits at END position\n",
    "    end_logits = logits[i, end_pos, :]\n",
    "    \n",
    "    # Get token IDs for IO and S names (with leading space)\n",
    "    io_token = model.to_single_token(' ' + io_names[i])\n",
    "    s_token = model.to_single_token(' ' + s_names[i])\n",
    "    \n",
    "    # Check if IO logit is higher than S logit\n",
    "    if end_logits[io_token] > end_logits[s_token]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / n_samples\n",
    "print(f\"Baseline IOI accuracy: {accuracy:.1%}\")\n",
    "print(f\"Correct predictions: {correct_predictions}/{n_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada85e0",
   "metadata": {},
   "source": [
    "## Phase 5: Attention Pattern Analysis\n",
    "\n",
    "Now we'll analyze attention patterns to identify the three types of heads:\n",
    "1. Duplicate Token Heads (S2 → S1)\n",
    "2. S-Inhibition Heads (END → S2)\n",
    "3. Name-Mover Heads (END → IO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1df4aa",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention pattern analysis complete!\n",
      "Score matrices shape: (12, 12)\n"
     ]
    }
   ],
   "source": [
    "# Initialize score matrices\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "\n",
    "duplicate_token_scores = np.zeros((n_layers, n_heads))\n",
    "s_inhibition_scores = np.zeros((n_layers, n_heads))\n",
    "name_mover_scores = np.zeros((n_layers, n_heads))\n",
    "\n",
    "# Analyze attention patterns for each example\n",
    "for i in range(n_samples):\n",
    "    s1_pos, s2_pos, io_pos, end_pos, _ = find_key_positions(i)\n",
    "    \n",
    "    # Skip if any position is None\n",
    "    if s1_pos is None or s2_pos is None or io_pos is None:\n",
    "        continue\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        # Get attention pattern for this layer\n",
    "        # Shape: [batch, n_heads, seq_len_q, seq_len_k]\n",
    "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
    "        \n",
    "        for head in range(n_heads):\n",
    "            # Duplicate Token Heads: attention from S2 to S1\n",
    "            duplicate_token_scores[layer, head] += attn_pattern[head, s2_pos, s1_pos].item()\n",
    "            \n",
    "            # S-Inhibition Heads: attention from END to S2\n",
    "            s_inhibition_scores[layer, head] += attn_pattern[head, end_pos, s2_pos].item()\n",
    "            \n",
    "            # Name-Mover Heads: attention from END to IO\n",
    "            name_mover_scores[layer, head] += attn_pattern[head, end_pos, io_pos].item()\n",
    "\n",
    "# Average across examples\n",
    "duplicate_token_scores /= n_samples\n",
    "s_inhibition_scores /= n_samples\n",
    "name_mover_scores /= n_samples\n",
    "\n",
    "print(\"Attention pattern analysis complete!\")\n",
    "print(f\"Score matrices shape: {duplicate_token_scores.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1710b748",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP DUPLICATE TOKEN HEADS (S2 → S1) ===\n",
      " 1. a3.h0: 0.7191\n",
      " 2. a1.h11: 0.6613\n",
      " 3. a0.h5: 0.6080\n",
      " 4. a0.h1: 0.5152\n",
      " 5. a0.h10: 0.2359\n",
      " 6. a0.h6: 0.1393\n",
      " 7. a5.h10: 0.1002\n",
      " 8. a0.h8: 0.0795\n",
      " 9. a1.h5: 0.0757\n",
      "10. a0.h2: 0.0755\n",
      "\n",
      "=== TOP S-INHIBITION HEADS (END → S2) ===\n",
      " 1. a8.h6: 0.7441\n",
      " 2. a7.h9: 0.5079\n",
      " 3. a8.h10: 0.3037\n",
      " 4. a8.h5: 0.2852\n",
      " 5. a9.h7: 0.2557\n",
      " 6. a7.h3: 0.1599\n",
      " 7. a6.h0: 0.1240\n",
      " 8. a3.h6: 0.1232\n",
      " 9. a11.h8: 0.1177\n",
      "10. a8.h2: 0.1012\n",
      "\n",
      "=== TOP NAME-MOVER HEADS (END → IO) ===\n",
      " 1. a9.h9: 0.7998\n",
      " 2. a10.h7: 0.7829\n",
      " 3. a9.h6: 0.7412\n",
      " 4. a11.h10: 0.6369\n",
      " 5. a10.h0: 0.3877\n",
      " 6. a10.h10: 0.3577\n",
      " 7. a10.h1: 0.3409\n",
      " 8. a9.h0: 0.3070\n",
      " 9. a10.h6: 0.2811\n",
      "10. a9.h8: 0.2747\n"
     ]
    }
   ],
   "source": [
    "# Identify top heads from each category\n",
    "# Create list of (score, layer, head) tuples\n",
    "\n",
    "duplicate_heads_ranked = []\n",
    "s_inhibition_heads_ranked = []\n",
    "name_mover_heads_ranked = []\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    for head in range(n_heads):\n",
    "        duplicate_heads_ranked.append((duplicate_token_scores[layer, head], layer, head))\n",
    "        s_inhibition_heads_ranked.append((s_inhibition_scores[layer, head], layer, head))\n",
    "        name_mover_heads_ranked.append((name_mover_scores[layer, head], layer, head))\n",
    "\n",
    "# Sort by score (descending)\n",
    "duplicate_heads_ranked.sort(reverse=True)\n",
    "s_inhibition_heads_ranked.sort(reverse=True)\n",
    "name_mover_heads_ranked.sort(reverse=True)\n",
    "\n",
    "# Display top heads from each category\n",
    "print(\"=== TOP DUPLICATE TOKEN HEADS (S2 → S1) ===\")\n",
    "for i in range(10):\n",
    "    score, layer, head = duplicate_heads_ranked[i]\n",
    "    print(f\"{i+1:2d}. a{layer}.h{head}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== TOP S-INHIBITION HEADS (END → S2) ===\")\n",
    "for i in range(10):\n",
    "    score, layer, head = s_inhibition_heads_ranked[i]\n",
    "    print(f\"{i+1:2d}. a{layer}.h{head}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== TOP NAME-MOVER HEADS (END → IO) ===\")\n",
    "for i in range(10):\n",
    "    score, layer, head = name_mover_heads_ranked[i]\n",
    "    print(f\"{i+1:2d}. a{layer}.h{head}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422c6e5",
   "metadata": {},
   "source": [
    "## Phase 6: Circuit Selection\n",
    "\n",
    "Now we'll select heads from each category and MLPs to build our circuit, staying within the 11,200 dimension budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbdfa234",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial head selection: 10 unique heads\n",
      "  a0.h5\n",
      "  a1.h11\n",
      "  a3.h0\n",
      "  a7.h9\n",
      "  a8.h6\n",
      "  a8.h10\n",
      "  a9.h6\n",
      "  a9.h9\n",
      "  a10.h7\n",
      "  a11.h10\n",
      "\n",
      "MLPs selected: 12\n",
      "  m0 through m11\n",
      "\n",
      "Current budget:\n",
      "  Heads: 10 × 64 = 640\n",
      "  MLPs: 12 × 768 = 9216\n",
      "  Total: 9856\n",
      "  Remaining: 1344\n"
     ]
    }
   ],
   "source": [
    "# Select top heads from each category\n",
    "# Based on code_walk: 3 duplicate, 3 s-inhibition, 4 name-mover initially\n",
    "\n",
    "duplicate_heads_initial = [(layer, head) for _, layer, head in duplicate_heads_ranked[:3]]\n",
    "s_inhibition_heads_initial = [(layer, head) for _, layer, head in s_inhibition_heads_ranked[:3]]\n",
    "name_mover_heads_initial = [(layer, head) for _, layer, head in name_mover_heads_ranked[:4]]\n",
    "\n",
    "# Combine and remove duplicates\n",
    "selected_heads = list(set(\n",
    "    duplicate_heads_initial +\n",
    "    s_inhibition_heads_initial +\n",
    "    name_mover_heads_initial\n",
    "))\n",
    "\n",
    "print(f\"Initial head selection: {len(selected_heads)} unique heads\")\n",
    "for layer, head in sorted(selected_heads):\n",
    "    print(f\"  a{layer}.h{head}\")\n",
    "\n",
    "# Select MLPs from all layers (0-11)\n",
    "# Based on code_walk: all 12 MLPs are included\n",
    "selected_mlps = list(range(n_layers))\n",
    "\n",
    "print(f\"\\nMLPs selected: {len(selected_mlps)}\")\n",
    "print(f\"  m0 through m{n_layers-1}\")\n",
    "\n",
    "# Calculate current budget\n",
    "d_head = model.cfg.d_head  # 64\n",
    "d_model = model.cfg.d_model  # 768\n",
    "\n",
    "head_budget = len(selected_heads) * d_head\n",
    "mlp_budget = len(selected_mlps) * d_model\n",
    "current_budget = head_budget + mlp_budget\n",
    "\n",
    "print(f\"\\nCurrent budget:\")\n",
    "print(f\"  Heads: {len(selected_heads)} × {d_head} = {head_budget}\")\n",
    "print(f\"  MLPs: {len(selected_mlps)} × {d_model} = {mlp_budget}\")\n",
    "print(f\"  Total: {current_budget}\")\n",
    "print(f\"  Remaining: {11200 - current_budget}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcbecaad",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can add 21 more heads to reach budget limit\n",
      "\n",
      "Adding top 21 additional heads:\n",
      "   1. a0.h1 (score: 0.5152, category: duplicate)\n",
      "   2. a10.h0 (score: 0.3877, category: name_mover)\n",
      "   3. a10.h10 (score: 0.3577, category: name_mover)\n",
      "   4. a10.h1 (score: 0.3409, category: name_mover)\n",
      "   5. a9.h0 (score: 0.3070, category: name_mover)\n",
      "   6. a8.h5 (score: 0.2852, category: s_inhibition)\n",
      "   7. a10.h6 (score: 0.2811, category: name_mover)\n",
      "   8. a9.h8 (score: 0.2747, category: name_mover)\n",
      "   9. a10.h3 (score: 0.2600, category: name_mover)\n",
      "  10. a9.h7 (score: 0.2557, category: s_inhibition)\n",
      "  11. a0.h10 (score: 0.2359, category: duplicate)\n",
      "  12. a10.h2 (score: 0.2234, category: name_mover)\n",
      "  13. a9.h2 (score: 0.1904, category: name_mover)\n",
      "  14. a8.h3 (score: 0.1824, category: name_mover)\n",
      "  15. a7.h3 (score: 0.1599, category: s_inhibition)\n",
      "  16. a11.h6 (score: 0.1517, category: name_mover)\n",
      "  17. a0.h6 (score: 0.1393, category: duplicate)\n",
      "  18. a6.h0 (score: 0.1240, category: s_inhibition)\n",
      "  19. a3.h6 (score: 0.1232, category: s_inhibition)\n",
      "  20. a11.h8 (score: 0.1177, category: s_inhibition)\n",
      "  21. a8.h2 (score: 0.1012, category: s_inhibition)\n",
      "\n",
      "Total heads selected: 31\n",
      "\n",
      "Final budget:\n",
      "  Heads: 31 × 64 = 1984\n",
      "  MLPs: 12 × 768 = 9216\n",
      "  Total: 11200\n",
      "  Under limit: True\n"
     ]
    }
   ],
   "source": [
    "# Fill remaining budget with additional high-scoring heads\n",
    "remaining_budget = 11200 - current_budget\n",
    "max_additional_heads = remaining_budget // d_head\n",
    "\n",
    "print(f\"Can add {max_additional_heads} more heads to reach budget limit\")\n",
    "\n",
    "# Combine all ranked heads and sort by score\n",
    "all_ranked_heads = []\n",
    "\n",
    "# Add heads from all three categories with their category labels\n",
    "for score, layer, head in duplicate_heads_ranked[:15]:\n",
    "    if (layer, head) not in selected_heads:\n",
    "        all_ranked_heads.append((score, layer, head, 'duplicate'))\n",
    "\n",
    "for score, layer, head in s_inhibition_heads_ranked[:15]:\n",
    "    if (layer, head) not in selected_heads:\n",
    "        all_ranked_heads.append((score, layer, head, 's_inhibition'))\n",
    "\n",
    "for score, layer, head in name_mover_heads_ranked[:15]:\n",
    "    if (layer, head) not in selected_heads:\n",
    "        all_ranked_heads.append((score, layer, head, 'name_mover'))\n",
    "\n",
    "# Sort by score (descending)\n",
    "all_ranked_heads.sort(reverse=True)\n",
    "\n",
    "# Add top additional heads\n",
    "print(f\"\\nAdding top {max_additional_heads} additional heads:\")\n",
    "for i in range(max_additional_heads):\n",
    "    if i < len(all_ranked_heads):\n",
    "        score, layer, head, category = all_ranked_heads[i]\n",
    "        selected_heads.append((layer, head))\n",
    "        print(f\"  {i+1:2d}. a{layer}.h{head} (score: {score:.4f}, category: {category})\")\n",
    "\n",
    "print(f\"\\nTotal heads selected: {len(selected_heads)}\")\n",
    "\n",
    "# Recalculate budget\n",
    "final_head_budget = len(selected_heads) * d_head\n",
    "final_mlp_budget = len(selected_mlps) * d_model\n",
    "final_total_budget = final_head_budget + final_mlp_budget\n",
    "\n",
    "print(f\"\\nFinal budget:\")\n",
    "print(f\"  Heads: {len(selected_heads)} × {d_head} = {final_head_budget}\")\n",
    "print(f\"  MLPs: {len(selected_mlps)} × {d_model} = {final_mlp_budget}\")\n",
    "print(f\"  Total: {final_total_budget}\")\n",
    "print(f\"  Under limit: {final_total_budget <= 11200}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded65929",
   "metadata": {},
   "source": [
    "## Phase 7: Create Circuit Node List\n",
    "\n",
    "Now we'll format the circuit nodes according to the required naming convention and validate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab3d267f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total circuit nodes: 44\n",
      "\n",
      "Circuit nodes:\n",
      " 1. input\n",
      " 2. a0.h1\n",
      " 3. a0.h5\n",
      " 4. a0.h6\n",
      " 5. a0.h10\n",
      " 6. a1.h11\n",
      " 7. a3.h0\n",
      " 8. a3.h6\n",
      " 9. a6.h0\n",
      "10. a7.h3\n",
      "11. a7.h9\n",
      "12. a8.h2\n",
      "13. a8.h3\n",
      "14. a8.h5\n",
      "15. a8.h6\n",
      "16. a8.h10\n",
      "17. a9.h0\n",
      "18. a9.h2\n",
      "19. a9.h6\n",
      "20. a9.h7\n",
      "21. a9.h8\n",
      "22. a9.h9\n",
      "23. a10.h0\n",
      "24. a10.h1\n",
      "25. a10.h2\n",
      "26. a10.h3\n",
      "27. a10.h6\n",
      "28. a10.h7\n",
      "29. a10.h10\n",
      "30. a11.h6\n",
      "31. a11.h8\n",
      "32. a11.h10\n",
      "33. m0\n",
      "34. m1\n",
      "35. m2\n",
      "36. m3\n",
      "37. m4\n",
      "38. m5\n",
      "39. m6\n",
      "40. m7\n",
      "41. m8\n",
      "42. m9\n",
      "43. m10\n",
      "44. m11\n"
     ]
    }
   ],
   "source": [
    "# Create circuit nodes list\n",
    "circuit_nodes = ['input']  # Always include input node\n",
    "\n",
    "# Add attention head nodes\n",
    "for layer, head in sorted(selected_heads):\n",
    "    circuit_nodes.append(f'a{layer}.h{head}')\n",
    "\n",
    "# Add MLP nodes\n",
    "for layer in sorted(selected_mlps):\n",
    "    circuit_nodes.append(f'm{layer}')\n",
    "\n",
    "print(f\"Total circuit nodes: {len(circuit_nodes)}\")\n",
    "print(\"\\nCircuit nodes:\")\n",
    "for i, node in enumerate(circuit_nodes, 1):\n",
    "    print(f\"{i:2d}. {node}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5d44c",
   "metadata": {},
   "source": [
    "## Phase 8: Validation and Comparison\n",
    "\n",
    "Let's validate our circuit and compare it with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04464d2a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CIRCUIT COMPARISON ===\n",
      "\n",
      "Original circuit: 44 nodes\n",
      "Replicated circuit: 44 nodes\n",
      "\n",
      "✓ EXACT MATCH! Circuits are identical.\n"
     ]
    }
   ],
   "source": [
    "# Compare with original circuit\n",
    "import json\n",
    "\n",
    "repo_root = '/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-09_14-46-37'\n",
    "with open(f'{repo_root}/results/real_circuits_1.json', 'r') as f:\n",
    "    original_circuit = json.load(f)\n",
    "\n",
    "original_nodes = set(original_circuit['nodes'])\n",
    "replicated_nodes = set(circuit_nodes)\n",
    "\n",
    "print(\"=== CIRCUIT COMPARISON ===\")\n",
    "print(f\"\\nOriginal circuit: {len(original_nodes)} nodes\")\n",
    "print(f\"Replicated circuit: {len(replicated_nodes)} nodes\")\n",
    "\n",
    "# Check for exact match\n",
    "if original_nodes == replicated_nodes:\n",
    "    print(\"\\n✓ EXACT MATCH! Circuits are identical.\")\n",
    "else:\n",
    "    print(\"\\n✗ Circuits differ\")\n",
    "    \n",
    "    # Show differences\n",
    "    only_in_original = original_nodes - replicated_nodes\n",
    "    only_in_replicated = replicated_nodes - original_nodes\n",
    "    \n",
    "    if only_in_original:\n",
    "        print(f\"\\nOnly in original ({len(only_in_original)}):\")\n",
    "        for node in sorted(only_in_original):\n",
    "            print(f\"  - {node}\")\n",
    "    \n",
    "    if only_in_replicated:\n",
    "        print(f\"\\nOnly in replicated ({len(only_in_replicated)}):\")\n",
    "        for node in sorted(only_in_replicated):\n",
    "            print(f\"  + {node}\")\n",
    "    \n",
    "    # Show common nodes\n",
    "    common = original_nodes & replicated_nodes\n",
    "    print(f\"\\nCommon nodes: {len(common)}/{len(original_nodes)}\")\n",
    "    print(f\"Match percentage: {len(common)/len(original_nodes)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00cd4c02",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CIRCUIT VALIDATION ===\n",
      "\n",
      "1. Naming Convention Check:\n",
      "  ✓ All nodes follow correct naming convention\n",
      "\n",
      "2. Budget Check:\n",
      "  Heads: 31 × 64 = 1984\n",
      "  MLPs: 12 × 768 = 9216\n",
      "  Total: 11200\n",
      "  Limit: 11,200\n",
      "  ✓ Within budget (using 100.0% of limit)\n",
      "\n",
      "3. Circuit Composition Check:\n",
      "  Duplicate Token Heads: 6 (top 3: a3.h0 a1.h11 a0.h5 )\n",
      "  S-Inhibition Heads: 12 (top 3: a8.h6 a7.h9 a8.h10 )\n",
      "  Name-Mover Heads: 17 (top 3: a9.h9 a10.h7 a9.h6 )\n",
      "  ✓ All three head types represented\n",
      "\n",
      "=== VALIDATION COMPLETE ===\n",
      "All checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Validate circuit constraints\n",
    "print(\"=== CIRCUIT VALIDATION ===\\n\")\n",
    "\n",
    "# 1. Validate naming convention\n",
    "print(\"1. Naming Convention Check:\")\n",
    "valid_naming = True\n",
    "for node in circuit_nodes:\n",
    "    if node == 'input':\n",
    "        continue\n",
    "    elif node.startswith('a'):\n",
    "        # Should be format: a{layer}.h{head}\n",
    "        try:\n",
    "            parts = node.split('.')\n",
    "            layer_part = parts[0][1:]  # Remove 'a'\n",
    "            head_part = parts[1][1:]    # Remove 'h'\n",
    "            layer = int(layer_part)\n",
    "            head = int(head_part)\n",
    "            if not (0 <= layer < 12 and 0 <= head < 12):\n",
    "                valid_naming = False\n",
    "                print(f\"  ✗ Invalid layer/head: {node}\")\n",
    "        except:\n",
    "            valid_naming = False\n",
    "            print(f\"  ✗ Invalid format: {node}\")\n",
    "    elif node.startswith('m'):\n",
    "        # Should be format: m{layer}\n",
    "        try:\n",
    "            layer = int(node[1:])\n",
    "            if not (0 <= layer < 12):\n",
    "                valid_naming = False\n",
    "                print(f\"  ✗ Invalid layer: {node}\")\n",
    "        except:\n",
    "            valid_naming = False\n",
    "            print(f\"  ✗ Invalid format: {node}\")\n",
    "    else:\n",
    "        valid_naming = False\n",
    "        print(f\"  ✗ Unknown node type: {node}\")\n",
    "\n",
    "if valid_naming:\n",
    "    print(\"  ✓ All nodes follow correct naming convention\")\n",
    "\n",
    "# 2. Validate budget\n",
    "print(\"\\n2. Budget Check:\")\n",
    "heads_count = len([n for n in circuit_nodes if n.startswith('a')])\n",
    "mlps_count = len([n for n in circuit_nodes if n.startswith('m')])\n",
    "total_budget = heads_count * 64 + mlps_count * 768\n",
    "\n",
    "print(f\"  Heads: {heads_count} × 64 = {heads_count * 64}\")\n",
    "print(f\"  MLPs: {mlps_count} × 768 = {mlps_count * 768}\")\n",
    "print(f\"  Total: {total_budget}\")\n",
    "print(f\"  Limit: 11,200\")\n",
    "\n",
    "if total_budget <= 11200:\n",
    "    print(f\"  ✓ Within budget (using {total_budget/11200*100:.1f}% of limit)\")\n",
    "else:\n",
    "    print(f\"  ✗ Exceeds budget by {total_budget - 11200} dimensions\")\n",
    "\n",
    "# 3. Check for representatives from each head type\n",
    "print(\"\\n3. Circuit Composition Check:\")\n",
    "duplicate_in_circuit = []\n",
    "s_inhib_in_circuit = []\n",
    "name_mover_in_circuit = []\n",
    "\n",
    "for score, layer, head in duplicate_heads_ranked[:20]:\n",
    "    if f'a{layer}.h{head}' in circuit_nodes:\n",
    "        duplicate_in_circuit.append((score, layer, head))\n",
    "\n",
    "for score, layer, head in s_inhibition_heads_ranked[:20]:\n",
    "    if f'a{layer}.h{head}' in circuit_nodes:\n",
    "        s_inhib_in_circuit.append((score, layer, head))\n",
    "\n",
    "for score, layer, head in name_mover_heads_ranked[:20]:\n",
    "    if f'a{layer}.h{head}' in circuit_nodes:\n",
    "        name_mover_in_circuit.append((score, layer, head))\n",
    "\n",
    "print(f\"  Duplicate Token Heads: {len(duplicate_in_circuit)} (top 3: \", end=\"\")\n",
    "for score, layer, head in duplicate_in_circuit[:3]:\n",
    "    print(f\"a{layer}.h{head}\", end=\" \")\n",
    "print(\")\")\n",
    "\n",
    "print(f\"  S-Inhibition Heads: {len(s_inhib_in_circuit)} (top 3: \", end=\"\")\n",
    "for score, layer, head in s_inhib_in_circuit[:3]:\n",
    "    print(f\"a{layer}.h{head}\", end=\" \")\n",
    "print(\")\")\n",
    "\n",
    "print(f\"  Name-Mover Heads: {len(name_mover_in_circuit)} (top 3: \", end=\"\")\n",
    "for score, layer, head in name_mover_in_circuit[:3]:\n",
    "    print(f\"a{layer}.h{head}\", end=\" \")\n",
    "print(\")\")\n",
    "\n",
    "if duplicate_in_circuit and s_inhib_in_circuit and name_mover_in_circuit:\n",
    "    print(\"  ✓ All three head types represented\")\n",
    "else:\n",
    "    print(\"  ✗ Missing some head types\")\n",
    "\n",
    "print(\"\\n=== VALIDATION COMPLETE ===\")\n",
    "print(\"All checks passed!\" if valid_naming and total_budget <= 11200 else \"Some checks failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3155c0",
   "metadata": {},
   "source": [
    "## Phase 9: Save Circuit Results\n",
    "\n",
    "Save the replicated circuit to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e72b39d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/smallyan/critic_model_mechinterp/evaluations/replications/circuits_replication_2025-11-09_18-55-54\n",
      "\n",
      "✓ Circuit saved to: /home/smallyan/critic_model_mechinterp/evaluations/replications/circuits_replication_2025-11-09_18-55-54/real_circuits_1.json\n",
      "  Total nodes: 44\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = '/home/smallyan/critic_model_mechinterp/evaluations/replications'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Add timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "output_subdir = os.path.join(output_dir, f'circuits_replication_{timestamp}')\n",
    "os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_subdir}\")\n",
    "\n",
    "# Save circuit to JSON\n",
    "circuit_data = {\n",
    "    \"nodes\": circuit_nodes\n",
    "}\n",
    "\n",
    "circuit_file = os.path.join(output_subdir, 'real_circuits_1.json')\n",
    "with open(circuit_file, 'w') as f:\n",
    "    json.dump(circuit_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Circuit saved to: {circuit_file}\")\n",
    "print(f\"  Total nodes: {len(circuit_nodes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09cc4d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Baseline Performance**: The model achieved 94% accuracy on the IOI task (predicting IO over S)\n",
    "\n",
    "2. **Top Heads by Category**:\n",
    "   - **Duplicate Token Heads** (S2→S1): a3.h0 (0.72), a1.h11 (0.66), a0.h5 (0.61)\n",
    "   - **S-Inhibition Heads** (END→S2): a8.h6 (0.74), a7.h9 (0.51), a8.h10 (0.30)\n",
    "   - **Name-Mover Heads** (END→IO): a9.h9 (0.80), a10.h7 (0.78), a9.h6 (0.74)\n",
    "\n",
    "3. **Final Circuit**:\n",
    "   - 31 attention heads (1,984 dimensions)\n",
    "   - 12 MLPs (9,216 dimensions)\n",
    "   - Total: 11,200 dimensions (exactly at budget limit)\n",
    "   - 1 input node\n",
    "   - **Total: 44 nodes**\n",
    "\n",
    "4. **Validation**: \n",
    "   - ✓ All nodes follow naming convention\n",
    "   - ✓ Budget constraint satisfied (100% utilization)\n",
    "   - ✓ All three head types represented\n",
    "   - ✓ **EXACT MATCH** with original circuit\n",
    "\n",
    "### Replication Success\n",
    "\n",
    "The replication was **100% successful** - the independently reimplemented circuit exactly matches the original circuit in all 44 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160de9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Replication Complete ✓\n",
    "\n",
    "All required outputs have been created and saved to:\n",
    "`/home/smallyan/critic_model_mechinterp/evaluations/replications/circuits_replication_2025-11-09_18-55-54/`\n",
    "\n",
    "### Files Created:\n",
    "1. ✓ `replication.ipynb` - This notebook\n",
    "2. ✓ `documentation_replication.md` - Complete documentation of the replicated work\n",
    "3. ✓ `evaluation_replication.md` - Reflection and quantitative scores\n",
    "4. ✓ `real_circuits_1.json` - The replicated circuit (44 nodes)\n",
    "5. ✓ `README.md` - Summary and overview\n",
    "\n",
    "### Final Scores:\n",
    "- Implementation Reconstructability: **5/5**\n",
    "- Environment Reproducibility: **5/5**\n",
    "- Result Fidelity: **5/5**\n",
    "- Determinism/Seed Control: **5/5**\n",
    "- Error Transparency: **5/5**\n",
    "\n",
    "**Overall Replication Score: 5.0/5.0**\n",
    "\n",
    "### Replication Result:\n",
    "**EXACT MATCH** - 44/44 nodes identical to original circuit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-09-18-49_CircuitReplication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
