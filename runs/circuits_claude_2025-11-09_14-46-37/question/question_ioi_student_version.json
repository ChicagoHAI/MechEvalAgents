[
  {
    "question_type": "multiple_choice",
    "question": "What is the primary research objective of the IOI circuit analysis?",
    "choice": [
      "A. Train GPT2-small to perform better on name identification tasks",
      "B. Identify a precise circuit in GPT2-small that implements IOI behavior within a dimension budget",
      "C. Compare IOI performance across different transformer models",
      "D. Develop a new attention mechanism for indirect object identification"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "According to the IOI hypothesis, which of the following correctly describes the three functional components and their attention patterns?",
    "choice": [
      "A. Duplicate Token Heads (END\u2192S1), S-Inhibition Heads (S2\u2192END), Name-Mover Heads (IO\u2192END)",
      "B. Duplicate Token Heads (S1\u2192S2), S-Inhibition Heads (S2\u2192END), Name-Mover Heads (END\u2192IO)",
      "C. Duplicate Token Heads (S2\u2192S1), S-Inhibition Heads (END\u2192S2), Name-Mover Heads (END\u2192IO)",
      "D. Duplicate Token Heads (S2\u2192S1), S-Inhibition Heads (IO\u2192S2), Name-Mover Heads (END\u2192S1)"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Given the example sentence 'As Carl and Maria left the consulate, Carl gave a fridge to', identify the S1, S2, IO, and END positions. Explain why Carl appears twice and what role each position plays in the IOI circuit hypothesis.",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "In GPT2-small, how many dimensions does each attention head write to the residual stream?",
    "choice": [
      "A. 768 dimensions",
      "B. 64 dimensions",
      "C. 3,072 dimensions",
      "D. 12 dimensions"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The final IOI circuit contains 31 attention heads and 12 MLPs. Show the calculation to verify that this circuit uses exactly 11,200 dimensions and achieves 100% budget utilization.",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Which attention head showed the highest average attention score for Name-Mover behavior (END\u2192IO attention)?",
    "choice": [
      "A. a3.h0 with 0.7191 average attention",
      "B. a8.h6 with 0.7441 average attention",
      "C. a10.h7 with 0.7829 average attention",
      "D. a9.h9 with 0.7998 average attention"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The documentation shows that Duplicate Token Heads are found in early layers (0-3), S-Inhibition Heads in middle layers (7-8), and Name-Mover Heads in late layers (9-11). Explain why this layered distribution makes sense from an information processing perspective.",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "What baseline accuracy did GPT2-small achieve on the IOI task before any circuit interventions?",
    "choice": [
      "A. 100%",
      "B. 84%",
      "C. 94%",
      "D. 74%"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The final circuit includes 6 Duplicate Token Heads, 12 S-Inhibition Heads, and 15 Name-Mover Heads (31 total). Why might there be more Name-Mover Heads than Duplicate Token Heads in the circuit?",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "How many examples from the IOI dataset were used for the analysis in this study?",
    "choice": [
      "A. 10,000 examples",
      "B. 100 examples",
      "C. 1,000 examples",
      "D. 50 examples"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Based on the results, evaluate whether the three-component IOI hypothesis was supported. What specific evidence from the attention pattern analysis supports or contradicts the hypothesis?",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "What is the total number of nodes in the final IOI circuit?",
    "choice": [
      "A. 31 nodes",
      "B. 12 nodes",
      "C. 43 nodes",
      "D. 44 nodes"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The documentation mentions 'Cross-Dataset Validation' as a potential extension. Describe a specific experiment that could test whether the identified IOI circuit generalizes to other name-based tasks, and what results would support or refute generalization.",
    "choice": null,
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Write code to verify the IOI circuit budget calculation. Given the circuit contains 31 attention heads (each writing 64 dimensions) and 12 MLPs (each writing 768 dimensions), calculate:\n1. Total dimensions used by attention heads\n2. Total dimensions used by MLPs  \n3. Total circuit dimensions\n4. Budget utilization percentage (budget limit is 11,200)\n\nPrint each value with appropriate labels.",
    "choice": null,
    "requires_code": true,
    "code_id": "CQ1"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to analyze the layer distribution of the IOI circuit heads. Given the following selected attention heads:\n['a0.h1', 'a0.h10', 'a0.h5', 'a0.h6', 'a1.h11', 'a3.h0', 'a3.h6', 'a6.h0', 'a7.h3', 'a7.h9', 'a8.h10', 'a8.h2', 'a8.h3', 'a8.h5', 'a8.h6', 'a9.h0', 'a9.h2', 'a9.h6', 'a9.h7', 'a9.h8', 'a9.h9', 'a10.h0', 'a10.h1', 'a10.h10', 'a10.h2', 'a10.h3', 'a10.h6', 'a10.h7', 'a11.h10', 'a11.h6', 'a11.h8']\n\nCalculate and print:\n1. Number of heads per layer (0-11)\n2. Total number of heads\n3. Which layer has the most heads",
    "choice": null,
    "requires_code": true,
    "code_id": "CQ2"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to verify the attention score rankings for the three head types. Given the following attention scores:\n\nDuplicate Token Heads (S2\u2192S1):\n- a3.h0: 0.7191, a1.h11: 0.6613, a0.h5: 0.6080, a0.h1: 0.5152, a0.h10: 0.2359\n\nS-Inhibition Heads (END\u2192S2):\n- a8.h6: 0.7441, a7.h9: 0.5079, a8.h10: 0.3037, a8.h5: 0.2852, a9.h7: 0.2557\n\nName-Mover Heads (END\u2192IO):\n- a9.h9: 0.7998, a10.h7: 0.7829, a9.h6: 0.7412, a11.h10: 0.6369, a10.h0: 0.3877\n\nFor each head type, calculate and print:\n1. The mean attention score of the top 5 heads\n2. The head with highest attention score\n3. Whether the top head has attention > 0.7 (strong selectivity)",
    "choice": null,
    "requires_code": true,
    "code_id": "CQ3"
  }
]