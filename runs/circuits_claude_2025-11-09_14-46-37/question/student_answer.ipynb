{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa3ee4b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/critic_model_mechinterp\n",
      "CUDA available: True\n",
      "Device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Setup: Change to the correct working directory and check GPU availability\n",
    "import os\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "\n",
    "import torch\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49d3f8",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Exam Answers\n",
    "\n",
    "## Student Answer Notebook\n",
    "\n",
    "This notebook contains answers to all exam questions based strictly on the provided documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971d79b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Primary Research Objective (Multiple Choice)\n",
    "\n",
    "**Question:** What is the primary research objective of the IOI circuit analysis?\n",
    "\n",
    "**Choices:**\n",
    "- A. Train GPT2-small to perform better on name identification tasks\n",
    "- B. Identify a precise circuit in GPT2-small that implements IOI behavior within a dimension budget\n",
    "- C. Compare IOI performance across different transformer models\n",
    "- D. Develop a new attention mechanism for indirect object identification\n",
    "\n",
    "**Reasoning:** According to Section 1 (Goal) of the documentation, the Research Objective is stated as: \"Identify a precise circuit in GPT2-small that implements the **Indirect Object Identification (IOI)** behavior while adhering to strict residual write-budget constraints (≤ 11,200 dimensions).\"\n",
    "\n",
    "This directly matches option B. The study is not about training the model (A), comparing models (C), or developing new mechanisms (D).\n",
    "\n",
    "**Answer: B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f1919",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Three Functional Components (Multiple Choice)\n",
    "\n",
    "**Question:** According to the IOI hypothesis, which of the following correctly describes the three functional components and their attention patterns?\n",
    "\n",
    "**Choices:**\n",
    "- A. Duplicate Token Heads (END→S1), S-Inhibition Heads (S2→END), Name-Mover Heads (IO→END)\n",
    "- B. Duplicate Token Heads (S1→S2), S-Inhibition Heads (S2→END), Name-Mover Heads (END→IO)\n",
    "- C. Duplicate Token Heads (S2→S1), S-Inhibition Heads (END→S2), Name-Mover Heads (END→IO)\n",
    "- D. Duplicate Token Heads (S2→S1), S-Inhibition Heads (IO→S2), Name-Mover Heads (END→S1)\n",
    "\n",
    "**Reasoning:** According to Section 1 (Hypothesis) of the documentation:\n",
    "1. **Duplicate Token Heads**: \"Active at S2, attending to S1\" → S2→S1\n",
    "2. **S-Inhibition Heads**: \"Active at END, attending to S2\" → END→S2\n",
    "3. **Name-Mover Heads**: \"Active at END, attending to IO position\" → END→IO\n",
    "\n",
    "This matches option C exactly. The documentation clearly states these attention patterns in Section 3.2 (Attention Pattern Analysis) where it describes:\n",
    "- \"Duplicate Token Heads (S2 → S1 attention)\"\n",
    "- \"S-Inhibition Heads (END → S2 attention)\"\n",
    "- \"Name-Mover Heads (END → IO attention)\"\n",
    "\n",
    "**Answer: C**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5bc12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Position Identification (Free Generation)\n",
    "\n",
    "**Question:** Given the example sentence 'As Carl and Maria left the consulate, Carl gave a fridge to', identify the S1, S2, IO, and END positions. Explain why Carl appears twice and what role each position plays in the IOI circuit hypothesis.\n",
    "\n",
    "**Reasoning:** According to Section 2 (Data) of the documentation, for the example sentence \"As Carl and Maria left the consulate, Carl gave a fridge to\":\n",
    "\n",
    "**Key Positions:**\n",
    "- **S1 (position 2)**: \"Carl\" - First mention of subject\n",
    "- **S2 (position 9)**: \"Carl\" - Second mention of subject  \n",
    "- **IO (position 4)**: \"Maria\" - Indirect object\n",
    "- **END (position 13)**: \"to\" - Final position\n",
    "\n",
    "**Why Carl appears twice:**\n",
    "Carl appears twice because the subject name is repeated in the sentence structure - first as part of the initial phrase (\"As Carl and Maria left...\") and then as the actor performing the action (\"Carl gave...\"). This repetition is a key feature of the IOI task that the circuit must detect.\n",
    "\n",
    "**Role of each position in the IOI circuit hypothesis:**\n",
    "\n",
    "1. **S1 and S2**: These positions allow Duplicate Token Heads to signal token duplication. The heads at S2 attend to S1 to detect that the same name (Carl) has been repeated, encoding this duplication through position features.\n",
    "\n",
    "2. **S2**: S-Inhibition Heads at END position attend to S2 to inhibit Name-Mover attention to subject positions. This prevents the model from incorrectly predicting \"Carl\" as the answer.\n",
    "\n",
    "3. **IO**: Name-Mover Heads at END attend to the IO position to copy the indirect object token (Maria) to the residual stream for prediction.\n",
    "\n",
    "4. **END**: This is where the prediction happens. S-Inhibition and Name-Mover heads are active here, working together to predict the correct indirect object (Maria).\n",
    "\n",
    "**Answer:** \n",
    "- S1 = position 2 (\"Carl\", first mention)\n",
    "- S2 = position 9 (\"Carl\", second mention)  \n",
    "- IO = position 4 (\"Maria\")\n",
    "- END = position 13 (\"to\")\n",
    "\n",
    "Carl appears twice to create the duplicate token pattern that the circuit must detect to distinguish between the subject (repeated name) and the indirect object (unique name). S1/S2 enable duplication detection, S2 is used for inhibition signaling, IO provides the target for name moving, and END is where the final prediction occurs through the coordinated action of all three component types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ad775",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Attention Head Dimensions (Multiple Choice)\n",
    "\n",
    "**Question:** In GPT2-small, how many dimensions does each attention head write to the residual stream?\n",
    "\n",
    "**Choices:**\n",
    "- A. 768 dimensions\n",
    "- B. 64 dimensions\n",
    "- C. 3,072 dimensions\n",
    "- D. 12 dimensions\n",
    "\n",
    "**Reasoning:** According to Section 3.2 (Write Budget Constraints) of the documentation:\n",
    "- \"Each attention head writes: 64 dimensions (d_model / n_heads)\"\n",
    "- This is derived from d_model (768) / n_heads (12) = 64 dimensions per head\n",
    "\n",
    "The other values in the choices are:\n",
    "- 768: This is d_model (what each MLP writes)\n",
    "- 3,072: This is d_mlp\n",
    "- 12: This is the number of heads per layer\n",
    "\n",
    "**Answer: B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac3b72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Budget Calculation Verification (Free Generation)\n",
    "\n",
    "**Question:** The final IOI circuit contains 31 attention heads and 12 MLPs. Show the calculation to verify that this circuit uses exactly 11,200 dimensions and achieves 100% budget utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7423b32d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget Verification Calculation:\n",
      "========================================\n",
      "Attention heads: 31 × 64 = 1984 dimensions\n",
      "MLPs: 12 × 768 = 9216 dimensions\n",
      "Total: 1984 + 9216 = 11200 dimensions\n",
      "Budget limit: 11200 dimensions\n",
      "Utilization: 11200/11200 = 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Helper calculation for Question 5\n",
    "\n",
    "# From documentation:\n",
    "# - Each attention head writes: 64 dimensions\n",
    "# - Each MLP writes: 768 dimensions\n",
    "# - Budget limit: 11,200 dimensions\n",
    "\n",
    "num_attention_heads = 31\n",
    "dims_per_head = 64\n",
    "num_mlps = 12\n",
    "dims_per_mlp = 768\n",
    "budget_limit = 11200\n",
    "\n",
    "# Calculate dimensions\n",
    "attention_dims = num_attention_heads * dims_per_head\n",
    "mlp_dims = num_mlps * dims_per_mlp\n",
    "total_dims = attention_dims + mlp_dims\n",
    "\n",
    "# Calculate utilization\n",
    "utilization = (total_dims / budget_limit) * 100\n",
    "\n",
    "print(\"Budget Verification Calculation:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Attention heads: {num_attention_heads} × {dims_per_head} = {attention_dims} dimensions\")\n",
    "print(f\"MLPs: {num_mlps} × {dims_per_mlp} = {mlp_dims} dimensions\")\n",
    "print(f\"Total: {attention_dims} + {mlp_dims} = {total_dims} dimensions\")\n",
    "print(f\"Budget limit: {budget_limit} dimensions\")\n",
    "print(f\"Utilization: {total_dims}/{budget_limit} = {utilization:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416312b",
   "metadata": {},
   "source": [
    "**Reasoning:** According to Section 3.2 of the documentation:\n",
    "- Each attention head writes 64 dimensions\n",
    "- Each MLP writes 768 dimensions\n",
    "- Budget limit is 11,200 dimensions\n",
    "\n",
    "**Calculation:**\n",
    "- Attention heads: 31 × 64 = 1,984 dimensions\n",
    "- MLPs: 12 × 768 = 9,216 dimensions\n",
    "- Total: 1,984 + 9,216 = 11,200 dimensions\n",
    "- Utilization: 11,200 / 11,200 = 100.0%\n",
    "\n",
    "**Answer:** The circuit uses exactly 11,200 dimensions (1,984 from attention heads + 9,216 from MLPs), achieving 100% budget utilization. This matches the Budget Verification table in Section 4 (Results) of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b9866",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6: Highest Name-Mover Attention (Multiple Choice)\n",
    "\n",
    "**Question:** Which attention head showed the highest average attention score for Name-Mover behavior (END→IO attention)?\n",
    "\n",
    "**Choices:**\n",
    "- A. a3.h0 with 0.7191 average attention\n",
    "- B. a8.h6 with 0.7441 average attention\n",
    "- C. a10.h7 with 0.7829 average attention\n",
    "- D. a9.h9 with 0.7998 average attention\n",
    "\n",
    "**Reasoning:** According to Section 3.3 (Attention Pattern Analysis) under \"Name-Mover Heads (END → IO attention)\", the top 5 heads are listed as:\n",
    "1. a9.h9: 0.7998\n",
    "2. a10.h7: 0.7829\n",
    "3. a9.h6: 0.7412\n",
    "4. a11.h10: 0.6369\n",
    "5. a10.h0: 0.3877\n",
    "\n",
    "The highest is a9.h9 with 0.7998 average attention. \n",
    "\n",
    "Note that:\n",
    "- a3.h0 (0.7191) is the top Duplicate Token Head, not Name-Mover\n",
    "- a8.h6 (0.7441) is the top S-Inhibition Head, not Name-Mover\n",
    "\n",
    "**Answer: D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67bf68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7: Layered Distribution Explanation (Free Generation)\n",
    "\n",
    "**Question:** The documentation shows that Duplicate Token Heads are found in early layers (0-3), S-Inhibition Heads in middle layers (7-8), and Name-Mover Heads in late layers (9-11). Explain why this layered distribution makes sense from an information processing perspective.\n",
    "\n",
    "**Reasoning:** According to Section 5 (Analysis) under \"Key Observations\" point 1 (Layered Processing), the documentation explicitly addresses this:\n",
    "\n",
    "> \"The circuit exhibits clear stratification:\n",
    "> - Early layers (0-3): Duplicate token detection\n",
    "> - Middle layers (7-8): Subject inhibition\n",
    "> - Late layers (9-11): Name moving and prediction\"\n",
    "\n",
    "**Why this makes sense from an information processing perspective:**\n",
    "\n",
    "1. **Sequential Information Flow**: Transformers process information sequentially through layers. Early layers must detect low-level patterns first before higher-level reasoning can occur.\n",
    "\n",
    "2. **Dependency Chain**: The three components have a logical dependency:\n",
    "   - First, the model needs to DETECT that a token is duplicated (Duplicate Token Heads in early layers)\n",
    "   - Then, using this duplication information, it needs to INHIBIT attention to the repeated subject positions (S-Inhibition Heads in middle layers)\n",
    "   - Finally, with the subject inhibited, it can MOVE the correct name (indirect object) to the output (Name-Mover Heads in late layers)\n",
    "\n",
    "3. **Feature Detection to Action**: This follows a natural computational pattern:\n",
    "   - Early layers: Pattern detection (detecting \"Carl\" appears twice)\n",
    "   - Middle layers: Feature integration and control (signaling which positions to avoid)\n",
    "   - Late layers: Task completion (copying the correct answer)\n",
    "\n",
    "4. **Positional vs. Semantic Processing**: The documentation notes that Duplicate Token Heads work with \"position features\" (early/simple processing), while Name-Mover Heads are \"ideal for final token prediction\" (late/output-focused processing).\n",
    "\n",
    "**Answer:** The layered distribution makes sense because information processing in the IOI circuit follows a sequential pipeline: (1) early layers detect the basic pattern of token duplication, (2) middle layers use this information to inhibit attention to the repeated subject, and (3) late layers perform the final task of copying the correct indirect object to the prediction. This reflects a natural progression from pattern detection → interference control → output generation, which aligns with how transformers build up increasingly abstract representations through successive layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b20b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 8: Baseline Accuracy (Multiple Choice)\n",
    "\n",
    "**Question:** What baseline accuracy did GPT2-small achieve on the IOI task before any circuit interventions?\n",
    "\n",
    "**Choices:**\n",
    "- A. 100%\n",
    "- B. 84%\n",
    "- C. 94%\n",
    "- D. 74%\n",
    "\n",
    "**Reasoning:** According to Section 3.3 (Step 1: Baseline Evaluation) of the documentation:\n",
    "\"**Result**: 94.00% accuracy (94/100 correct)\"\n",
    "\n",
    "This is also confirmed in Section 4 (Performance Metrics):\n",
    "\"**Baseline Model Accuracy**: 94.00% (94/100 examples)\"\n",
    "\n",
    "And in Section 7 (Main Takeaways) point 4:\n",
    "\"**High Baseline Performance**: GPT2-small achieves 94% accuracy on IOI\"\n",
    "\n",
    "**Answer: C**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96999843",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 9: Head Count Distribution Explanation (Free Generation)\n",
    "\n",
    "**Question:** The final circuit includes 6 Duplicate Token Heads, 12 S-Inhibition Heads, and 15 Name-Mover Heads (31 total). Why might there be more Name-Mover Heads than Duplicate Token Heads in the circuit?\n",
    "\n",
    "**Reasoning:** Based on the documentation, several factors explain this distribution:\n",
    "\n",
    "1. **Complexity of the Task**: According to Section 5 (Analysis), the documentation notes that \"Multiple heads per category suggest robustness through redundancy.\" Name-Mover Heads perform the final, most critical step of copying the correct token to the output. This is the primary task output, so having more heads dedicated to this function provides greater reliability.\n",
    "\n",
    "2. **Layer Distribution**: The documentation shows:\n",
    "   - Duplicate Token Heads: primarily in layers 0, 1, 3 (early layers)\n",
    "   - Name-Mover Heads: primarily in layers 9, 10, 11 (late layers)\n",
    "   \n",
    "   Looking at the Layer Distribution in Section 4:\n",
    "   - Early layers (0-3): 4 + 1 + 2 = 7 heads total\n",
    "   - Late layers (9-11): 5 + 7 + 4 = 16 heads total\n",
    "   \n",
    "   There are simply more heads selected from later layers, where Name-Mover functionality is concentrated.\n",
    "\n",
    "3. **Attention Pattern Analysis**: The top 5 Name-Mover heads have high attention scores (0.7998, 0.7829, 0.7412, 0.6369, 0.3877), but even more heads may show significant END→IO attention patterns that qualified them for inclusion.\n",
    "\n",
    "4. **Functional Requirements**: Duplicate token detection is a simpler positional matching task (detecting the same token appearing twice). Name moving requires actually copying semantic content (the specific name token) to influence the final prediction, which may require more distributed processing.\n",
    "\n",
    "5. **Selection Strategy**: According to Section 3.3 (Step 3), the strategy was to \"maximize circuit expressiveness\" with remaining budget after selecting top heads. This may have favored including more late-layer heads that directly impact output.\n",
    "\n",
    "**Answer:** There are more Name-Mover Heads (15) than Duplicate Token Heads (6) likely because: (1) name moving is the primary output task requiring greater redundancy and reliability, (2) more heads in late layers showed significant END→IO attention patterns, (3) detecting token duplication is computationally simpler than copying semantic content for prediction, and (4) the selection strategy prioritized circuit expressiveness, favoring heads that directly impact the final output. The documentation notes that multiple heads per category suggest \"robustness through redundancy,\" and this redundancy may be more critical for the final prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82c5e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 10: Dataset Size (Multiple Choice)\n",
    "\n",
    "**Question:** How many examples from the IOI dataset were used for the analysis in this study?\n",
    "\n",
    "**Choices:**\n",
    "- A. 10,000 examples\n",
    "- B. 100 examples\n",
    "- C. 1,000 examples\n",
    "- D. 50 examples\n",
    "\n",
    "**Reasoning:** According to Section 2 (Data) under \"Dataset\":\n",
    "- \"**Size**: 10,000 examples (100 used for analysis)\"\n",
    "\n",
    "And confirmed in Section 3.3 (Step 1: Baseline Evaluation):\n",
    "- \"Tokenized 100 IOI examples\"\n",
    "\n",
    "And in Section 4 (Performance Metrics):\n",
    "- \"**Sample Size**: 100 examples from training set\"\n",
    "\n",
    "While the full dataset contains 10,000 examples, only 100 were used for the actual analysis.\n",
    "\n",
    "**Answer: B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6375d4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 11: Hypothesis Evaluation (Free Generation)\n",
    "\n",
    "**Question:** Based on the results, evaluate whether the three-component IOI hypothesis was supported. What specific evidence from the attention pattern analysis supports or contradicts the hypothesis?\n",
    "\n",
    "**Reasoning:** According to Section 5 (Analysis), the documentation explicitly states: \"The analysis **strongly supports** the three-component IOI hypothesis.\"\n",
    "\n",
    "**Specific Evidence Supporting the Hypothesis:**\n",
    "\n",
    "1. **Duplicate Token Heads Identified:**\n",
    "   - Found 6 heads with strong S2→S1 attention\n",
    "   - Top head: a3.h0 with 0.72 average attention (high selectivity)\n",
    "   - Located in early-to-middle layers (0, 1, 3) - consistent with positional feature detection\n",
    "   - Evidence: High attention scores (0.7191, 0.6613, 0.6080 for top 3)\n",
    "\n",
    "2. **S-Inhibition Heads Identified:**\n",
    "   - Found 12 heads with strong END→S2 attention\n",
    "   - Top head: a8.h6 with 0.74 average attention (high selectivity)\n",
    "   - Located in middle-to-late layers (7, 8, 9) - appropriate for suppressing subject interference\n",
    "   - Evidence: High attention scores (0.7441, 0.5079, 0.3037 for top 3)\n",
    "\n",
    "3. **Name-Mover Heads Identified:**\n",
    "   - Found 15 heads with strong END→IO attention\n",
    "   - Top head: a9.h9 with 0.80 average attention (high selectivity)\n",
    "   - Concentrated in late layers (9, 10, 11) - ideal for final token prediction\n",
    "   - Evidence: High attention scores (0.7998, 0.7829, 0.7412 for top 3)\n",
    "\n",
    "**Key Supporting Observations from Documentation:**\n",
    "\n",
    "1. \"Top heads show very strong attention patterns (>0.7) to their hypothesized targets, indicating specialized functionality\"\n",
    "\n",
    "2. The layered processing pattern (early→middle→late) matches the hypothesized information flow\n",
    "\n",
    "3. \"High Selectivity\": The top heads in each category show attention >0.7, suggesting they are specifically tuned for their hypothesized functions\n",
    "\n",
    "**No Contradictory Evidence Mentioned:**\n",
    "The documentation does not present any evidence that contradicts the hypothesis. All findings align with the three-component model.\n",
    "\n",
    "**Answer:** The three-component IOI hypothesis was **strongly supported** by the evidence. Specific supporting evidence includes:\n",
    "- Duplicate Token Heads: 6 heads with strong S2→S1 attention (top: a3.h0 at 0.72) in early layers (0-3)\n",
    "- S-Inhibition Heads: 12 heads with strong END→S2 attention (top: a8.h6 at 0.74) in middle layers (7-8)\n",
    "- Name-Mover Heads: 15 heads with strong END→IO attention (top: a9.h9 at 0.80) in late layers (9-11)\n",
    "\n",
    "The high selectivity (>0.7 attention) of top heads and the clear layer stratification matching the hypothesized information flow (detection → inhibition → prediction) provide strong support. No contradictory evidence was found in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8b892",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 12: Total Nodes in Circuit (Multiple Choice)\n",
    "\n",
    "**Question:** What is the total number of nodes in the final IOI circuit?\n",
    "\n",
    "**Choices:**\n",
    "- A. 31 nodes\n",
    "- B. 12 nodes\n",
    "- C. 43 nodes\n",
    "- D. 44 nodes\n",
    "\n",
    "**Reasoning:** According to Section 4 (Results) under \"Final Circuit Composition\":\n",
    "\n",
    "> \"**Total Nodes**: 44\n",
    "> - 1 input node\n",
    "> - 31 attention heads\n",
    "> - 12 MLPs\"\n",
    "\n",
    "Calculation: 1 + 31 + 12 = 44 nodes\n",
    "\n",
    "Note that 31 is just the attention heads, and 12 is just the MLPs. The total must include the input node.\n",
    "\n",
    "**Answer: D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f52b55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 13: Cross-Dataset Validation Experiment (Free Generation)\n",
    "\n",
    "**Question:** The documentation mentions 'Cross-Dataset Validation' as a potential extension. Describe a specific experiment that could test whether the identified IOI circuit generalizes to other name-based tasks, and what results would support or refute generalization.\n",
    "\n",
    "**Reasoning:** According to Section 6 (Next Steps) under \"Potential Extensions\" point 4:\n",
    "> \"**Cross-Dataset Validation**: Test if identified heads generalize to other name-based tasks\"\n",
    "\n",
    "**Specific Experiment Design:**\n",
    "\n",
    "**Task:** Test the identified IOI circuit on a different name-based task, such as a \"Name Attribution\" task where the model must identify who performed an action.\n",
    "\n",
    "**Dataset Example:**\n",
    "- Sentences like: \"After John gave the book to Mary, she thanked ___ for the gift\"\n",
    "- Correct answer: John\n",
    "- This requires similar name tracking but with different syntactic structure\n",
    "\n",
    "**Alternative Dataset:** Could also use:\n",
    "- Gendered pronoun resolution (e.g., \"John told Mary that he/she would...\")\n",
    "- Name coreference resolution\n",
    "- Subject-verb agreement with names\n",
    "\n",
    "**Experimental Procedure:**\n",
    "1. Create or obtain a dataset of 100+ examples of the alternative name-based task\n",
    "2. Run the same attention pattern analysis on the new task\n",
    "3. For each of the 44 identified IOI circuit nodes:\n",
    "   - Measure if they show similar functional attention patterns\n",
    "   - For Duplicate Token Heads: Do they still attend to repeated tokens?\n",
    "   - For S-Inhibition Heads: Do they still inhibit attention to specific positions?\n",
    "   - For Name-Mover Heads: Do they still move name information to prediction positions?\n",
    "4. Perform ablation studies: Remove the IOI circuit nodes and measure performance drop on the new task\n",
    "\n",
    "**Results Supporting Generalization:**\n",
    "- The same heads (e.g., a3.h0, a8.h6, a9.h9) show high attention scores for analogous patterns in the new task\n",
    "- Ablating the IOI circuit significantly degrades performance on the new task\n",
    "- Similar layer stratification is observed (early detection, middle inhibition, late output)\n",
    "\n",
    "**Results Refuting Generalization:**\n",
    "- The identified heads show random or weak attention patterns on the new task\n",
    "- Ablating the IOI circuit has minimal effect on new task performance\n",
    "- Different heads show strong task-relevant attention patterns\n",
    "- The layer distribution of important heads differs significantly\n",
    "\n",
    "**Answer:** A cross-dataset validation experiment could test the IOI circuit on a \"Name Attribution\" task (e.g., \"John gave the book to Mary, she thanked ___ for the gift\" → John). The experiment would measure whether the same 44 circuit nodes show similar functional attention patterns (Duplicate Token, S-Inhibition, Name-Mover) and perform ablation studies.\n",
    "\n",
    "Results supporting generalization would include: (1) the same heads showing strong attention to analogous positions, (2) significant performance drop when ablating the IOI circuit, and (3) similar layer stratification.\n",
    "\n",
    "Results refuting generalization would include: (1) weak or random attention patterns from IOI circuit heads, (2) minimal performance impact from ablation, and (3) different heads emerging as important for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c545176",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 14 (CQ1): Budget Calculation Verification (Code Required)\n",
    "\n",
    "**Question:** Write code to verify the IOI circuit budget calculation. Given the circuit contains 31 attention heads (each writing 64 dimensions) and 12 MLPs (each writing 768 dimensions), calculate:\n",
    "1. Total dimensions used by attention heads\n",
    "2. Total dimensions used by MLPs  \n",
    "3. Total circuit dimensions\n",
    "4. Budget utilization percentage (budget limit is 11,200)\n",
    "\n",
    "Print each value with appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1165691",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOI Circuit Budget Verification\n",
      "==================================================\n",
      "1. Total dimensions used by attention heads: 1984\n",
      "   (31 heads × 64 dims/head)\n",
      "\n",
      "2. Total dimensions used by MLPs: 9216\n",
      "   (12 MLPs × 768 dims/MLP)\n",
      "\n",
      "3. Total circuit dimensions: 11200\n",
      "   (1984 + 9216)\n",
      "\n",
      "4. Budget utilization percentage: 100.0%\n",
      "   (11200 / 11200 × 100)\n"
     ]
    }
   ],
   "source": [
    "# CQ1: Budget Calculation Verification\n",
    "\n",
    "# Circuit parameters from documentation\n",
    "num_attention_heads = 31\n",
    "dims_per_attention_head = 64\n",
    "\n",
    "num_mlps = 12\n",
    "dims_per_mlp = 768\n",
    "\n",
    "budget_limit = 11200\n",
    "\n",
    "# Calculate dimensions\n",
    "total_attention_dims = num_attention_heads * dims_per_attention_head\n",
    "total_mlp_dims = num_mlps * dims_per_mlp\n",
    "total_circuit_dims = total_attention_dims + total_mlp_dims\n",
    "\n",
    "# Calculate utilization\n",
    "budget_utilization = (total_circuit_dims / budget_limit) * 100\n",
    "\n",
    "# Print results with appropriate labels\n",
    "print(\"IOI Circuit Budget Verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"1. Total dimensions used by attention heads: {total_attention_dims}\")\n",
    "print(f\"   ({num_attention_heads} heads × {dims_per_attention_head} dims/head)\")\n",
    "print()\n",
    "print(f\"2. Total dimensions used by MLPs: {total_mlp_dims}\")\n",
    "print(f\"   ({num_mlps} MLPs × {dims_per_mlp} dims/MLP)\")\n",
    "print()\n",
    "print(f\"3. Total circuit dimensions: {total_circuit_dims}\")\n",
    "print(f\"   ({total_attention_dims} + {total_mlp_dims})\")\n",
    "print()\n",
    "print(f\"4. Budget utilization percentage: {budget_utilization:.1f}%\")\n",
    "print(f\"   ({total_circuit_dims} / {budget_limit} × 100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86e88a",
   "metadata": {},
   "source": [
    "**Reasoning:** Using the values from Section 3.2 of the documentation:\n",
    "- Each attention head writes 64 dimensions (d_model / n_heads = 768 / 12)\n",
    "- Each MLP writes 768 dimensions (d_model)\n",
    "- Budget limit is 11,200 dimensions\n",
    "\n",
    "The code calculates each component and verifies the budget utilization.\n",
    "\n",
    "**Answer:** \n",
    "1. Total attention head dimensions: 1,984 (31 × 64)\n",
    "2. Total MLP dimensions: 9,216 (12 × 768)\n",
    "3. Total circuit dimensions: 11,200\n",
    "4. Budget utilization: 100.0%\n",
    "\n",
    "This confirms the circuit uses exactly the budget limit with no waste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f2bbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 15 (CQ2): Layer Distribution Analysis (Code Required)\n",
    "\n",
    "**Question:** Write code to analyze the layer distribution of the IOI circuit heads. Given the following selected attention heads:\n",
    "['a0.h1', 'a0.h10', 'a0.h5', 'a0.h6', 'a1.h11', 'a3.h0', 'a3.h6', 'a6.h0', 'a7.h3', 'a7.h9', 'a8.h10', 'a8.h2', 'a8.h3', 'a8.h5', 'a8.h6', 'a9.h0', 'a9.h2', 'a9.h6', 'a9.h7', 'a9.h8', 'a9.h9', 'a10.h0', 'a10.h1', 'a10.h10', 'a10.h2', 'a10.h3', 'a10.h6', 'a10.h7', 'a11.h10', 'a11.h6', 'a11.h8']\n",
    "\n",
    "Calculate and print:\n",
    "1. Number of heads per layer (0-11)\n",
    "2. Total number of heads\n",
    "3. Which layer has the most heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45218f9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Distribution of IOI Circuit Attention Heads\n",
      "==================================================\n",
      "\n",
      "1. Number of heads per layer:\n",
      "   Layer  0: 4 heads ████\n",
      "   Layer  1: 1 heads █\n",
      "   Layer  2: 0 heads \n",
      "   Layer  3: 2 heads ██\n",
      "   Layer  4: 0 heads \n",
      "   Layer  5: 0 heads \n",
      "   Layer  6: 1 heads █\n",
      "   Layer  7: 2 heads ██\n",
      "   Layer  8: 5 heads █████\n",
      "   Layer  9: 6 heads ██████\n",
      "   Layer 10: 7 heads ███████\n",
      "   Layer 11: 3 heads ███\n",
      "\n",
      "2. Total number of heads: 31\n",
      "\n",
      "3. Layer with most heads: Layer 10 (7 heads)\n"
     ]
    }
   ],
   "source": [
    "# CQ2: Layer Distribution Analysis\n",
    "\n",
    "# Selected attention heads from the IOI circuit\n",
    "attention_heads = [\n",
    "    'a0.h1', 'a0.h10', 'a0.h5', 'a0.h6', \n",
    "    'a1.h11', \n",
    "    'a3.h0', 'a3.h6', \n",
    "    'a6.h0', \n",
    "    'a7.h3', 'a7.h9', \n",
    "    'a8.h10', 'a8.h2', 'a8.h3', 'a8.h5', 'a8.h6', \n",
    "    'a9.h0', 'a9.h2', 'a9.h6', 'a9.h7', 'a9.h8', 'a9.h9', \n",
    "    'a10.h0', 'a10.h1', 'a10.h10', 'a10.h2', 'a10.h3', 'a10.h6', 'a10.h7', \n",
    "    'a11.h10', 'a11.h6', 'a11.h8'\n",
    "]\n",
    "\n",
    "# Initialize count dictionary for all layers (0-11)\n",
    "layer_counts = {i: 0 for i in range(12)}\n",
    "\n",
    "# Count heads per layer\n",
    "for head in attention_heads:\n",
    "    # Extract layer number from head name (e.g., 'a0.h1' -> 0)\n",
    "    layer = int(head.split('.')[0][1:])\n",
    "    layer_counts[layer] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Layer Distribution of IOI Circuit Attention Heads\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "# 1. Number of heads per layer\n",
    "print(\"1. Number of heads per layer:\")\n",
    "for layer in range(12):\n",
    "    count = layer_counts[layer]\n",
    "    bar = '█' * count\n",
    "    print(f\"   Layer {layer:2d}: {count} heads {bar}\")\n",
    "print()\n",
    "\n",
    "# 2. Total number of heads\n",
    "total_heads = sum(layer_counts.values())\n",
    "print(f\"2. Total number of heads: {total_heads}\")\n",
    "print()\n",
    "\n",
    "# 3. Layer with most heads\n",
    "max_count = max(layer_counts.values())\n",
    "layers_with_max = [layer for layer, count in layer_counts.items() if count == max_count]\n",
    "print(f\"3. Layer with most heads: Layer {layers_with_max[0]} ({max_count} heads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3842acc",
   "metadata": {},
   "source": [
    "**Reasoning:** The code parses each attention head name to extract the layer number (e.g., 'a10.h7' → layer 10) and counts heads per layer. According to the documentation's Layer Distribution in Section 4:\n",
    "- Layer 0: 4 heads\n",
    "- Layer 1: 1 head\n",
    "- Layer 3: 2 heads\n",
    "- Layer 6: 1 head\n",
    "- Layer 7: 2 heads\n",
    "- Layer 8: 5 heads\n",
    "- Layer 9: 5 heads\n",
    "- Layer 10: 7 heads\n",
    "- Layer 11: 4 heads\n",
    "\n",
    "Note: The code output shows Layer 9 has 6 heads and Layer 11 has 3 heads, which differs slightly from the documentation. This is because the provided list contains 31 heads as stated, but the distribution may have a minor discrepancy with the documentation's stated distribution.\n",
    "\n",
    "**Answer:**\n",
    "1. Heads per layer: Layer 0: 4, Layer 1: 1, Layer 2: 0, Layer 3: 2, Layer 4: 0, Layer 5: 0, Layer 6: 1, Layer 7: 2, Layer 8: 5, Layer 9: 6, Layer 10: 7, Layer 11: 3\n",
    "2. Total number of heads: 31\n",
    "3. Layer with most heads: Layer 10 with 7 heads\n",
    "\n",
    "The late layers (8-11) contain the majority of heads (21 out of 31), consistent with the importance of S-Inhibition and Name-Mover Heads in those layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000fffb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 16 (CQ3): Attention Score Rankings Verification (Code Required)\n",
    "\n",
    "**Question:** Write code to verify the attention score rankings for the three head types. Given the following attention scores:\n",
    "\n",
    "Duplicate Token Heads (S2→S1):\n",
    "- a3.h0: 0.7191, a1.h11: 0.6613, a0.h5: 0.6080, a0.h1: 0.5152, a0.h10: 0.2359\n",
    "\n",
    "S-Inhibition Heads (END→S2):\n",
    "- a8.h6: 0.7441, a7.h9: 0.5079, a8.h10: 0.3037, a8.h5: 0.2852, a9.h7: 0.2557\n",
    "\n",
    "Name-Mover Heads (END→IO):\n",
    "- a9.h9: 0.7998, a10.h7: 0.7829, a9.h6: 0.7412, a11.h10: 0.6369, a10.h0: 0.3877\n",
    "\n",
    "For each head type, calculate and print:\n",
    "1. The mean attention score of the top 5 heads\n",
    "2. The head with highest attention score\n",
    "3. Whether the top head has attention > 0.7 (strong selectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92c4659",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Score Analysis for IOI Circuit Head Types\n",
      "============================================================\n",
      "\n",
      "Duplicate Token Heads (S2→S1):\n",
      "  1. Mean attention score of top 5 heads: 0.5479\n",
      "  2. Head with highest attention score: a3.h0 (0.7191)\n",
      "  3. Strong selectivity (top head > 0.7): Yes\n",
      "\n",
      "S-Inhibition Heads (END→S2):\n",
      "  1. Mean attention score of top 5 heads: 0.4193\n",
      "  2. Head with highest attention score: a8.h6 (0.7441)\n",
      "  3. Strong selectivity (top head > 0.7): Yes\n",
      "\n",
      "Name-Mover Heads (END→IO):\n",
      "  1. Mean attention score of top 5 heads: 0.6697\n",
      "  2. Head with highest attention score: a9.h9 (0.7998)\n",
      "  3. Strong selectivity (top head > 0.7): Yes\n",
      "\n",
      "Summary:\n",
      "------------------------------------------------------------\n",
      "All three head types have top heads with strong selectivity (>0.7): \n",
      "  - Duplicate Token: True\n",
      "  - S-Inhibition: True\n",
      "  - Name-Mover: True\n"
     ]
    }
   ],
   "source": [
    "# CQ3: Attention Score Rankings Verification\n",
    "\n",
    "# Attention scores from documentation\n",
    "duplicate_token_heads = {\n",
    "    'a3.h0': 0.7191,\n",
    "    'a1.h11': 0.6613,\n",
    "    'a0.h5': 0.6080,\n",
    "    'a0.h1': 0.5152,\n",
    "    'a0.h10': 0.2359\n",
    "}\n",
    "\n",
    "s_inhibition_heads = {\n",
    "    'a8.h6': 0.7441,\n",
    "    'a7.h9': 0.5079,\n",
    "    'a8.h10': 0.3037,\n",
    "    'a8.h5': 0.2852,\n",
    "    'a9.h7': 0.2557\n",
    "}\n",
    "\n",
    "name_mover_heads = {\n",
    "    'a9.h9': 0.7998,\n",
    "    'a10.h7': 0.7829,\n",
    "    'a9.h6': 0.7412,\n",
    "    'a11.h10': 0.6369,\n",
    "    'a10.h0': 0.3877\n",
    "}\n",
    "\n",
    "def analyze_head_type(name, heads_dict):\n",
    "    \"\"\"Analyze attention scores for a head type.\"\"\"\n",
    "    scores = list(heads_dict.values())\n",
    "    head_names = list(heads_dict.keys())\n",
    "    \n",
    "    # Find max\n",
    "    max_score = max(scores)\n",
    "    max_head = head_names[scores.index(max_score)]\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    \n",
    "    # Check strong selectivity\n",
    "    strong_selectivity = max_score > 0.7\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  1. Mean attention score of top 5 heads: {mean_score:.4f}\")\n",
    "    print(f\"  2. Head with highest attention score: {max_head} ({max_score:.4f})\")\n",
    "    print(f\"  3. Strong selectivity (top head > 0.7): {'Yes' if strong_selectivity else 'No'}\")\n",
    "    print()\n",
    "    \n",
    "    return mean_score, max_head, max_score, strong_selectivity\n",
    "\n",
    "print(\"Attention Score Analysis for IOI Circuit Head Types\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Analyze each head type\n",
    "dt_results = analyze_head_type(\"Duplicate Token Heads (S2→S1)\", duplicate_token_heads)\n",
    "si_results = analyze_head_type(\"S-Inhibition Heads (END→S2)\", s_inhibition_heads)\n",
    "nm_results = analyze_head_type(\"Name-Mover Heads (END→IO)\", name_mover_heads)\n",
    "\n",
    "# Summary\n",
    "print(\"Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"All three head types have top heads with strong selectivity (>0.7): \")\n",
    "print(f\"  - Duplicate Token: {dt_results[3]}\")\n",
    "print(f\"  - S-Inhibition: {si_results[3]}\")\n",
    "print(f\"  - Name-Mover: {nm_results[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332d2b3",
   "metadata": {},
   "source": [
    "**Reasoning:** The code stores the attention scores from Section 3.3 of the documentation and calculates summary statistics for each head type. The analysis confirms the patterns described in Section 5 (Analysis) under \"High Selectivity\" where it states \"Top heads show very strong attention patterns (>0.7) to their hypothesized targets.\"\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Duplicate Token Heads (S2→S1):**\n",
    "1. Mean attention score: 0.5479\n",
    "2. Highest: a3.h0 (0.7191)\n",
    "3. Strong selectivity: Yes\n",
    "\n",
    "**S-Inhibition Heads (END→S2):**\n",
    "1. Mean attention score: 0.4193\n",
    "2. Highest: a8.h6 (0.7441)\n",
    "3. Strong selectivity: Yes\n",
    "\n",
    "**Name-Mover Heads (END→IO):**\n",
    "1. Mean attention score: 0.6697\n",
    "2. Highest: a9.h9 (0.7998)\n",
    "3. Strong selectivity: Yes\n",
    "\n",
    "All three head types have their top head with attention >0.7, confirming strong selectivity and specialized functionality for their hypothesized roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe973dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Exam\n",
    "\n",
    "All questions have been answered based strictly on the provided documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-19-22-43_IOI_Exam_Answers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
