{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351c7a11",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Question Documentation\n",
    "\n",
    "This notebook contains the key knowledge points and exam questions for evaluating understanding of the IOI Circuit Analysis research.\n",
    "\n",
    "**Note**: Students should only use the documentation.md file to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2482c22",
   "metadata": {},
   "source": [
    "\n",
    "## Key Knowledge Points for IOI Circuit Analysis\n",
    "\n",
    "### 1. Research Goal & Hypothesis\n",
    "- Research objective: Identify circuit in GPT2-small for IOI task within 11,200 dimension budget\n",
    "- Three-component hypothesis:\n",
    "  - Duplicate Token Heads: S2 → S1 attention\n",
    "  - S-Inhibition Heads: END → S2 attention\n",
    "  - Name-Mover Heads: END → IO attention\n",
    "\n",
    "### 2. Dataset Understanding\n",
    "- Source: mib-bench/ioi (Hugging Face)\n",
    "- Size: 10,000 examples total, 100 used for analysis\n",
    "- Key positions: S1, S2, IO, END\n",
    "- Metadata: subject, indirect_object, object, place\n",
    "\n",
    "### 3. Model Configuration\n",
    "- Model: GPT2-small (TransformerLens)\n",
    "- Architecture: 12 layers, 12 heads, d_model=768, d_head=64, d_mlp=3072\n",
    "- Device: CUDA (A100)\n",
    "\n",
    "### 4. Write Budget Constraints\n",
    "- Head: 64 dimensions (d_model / n_heads)\n",
    "- MLP: 768 dimensions (d_model)\n",
    "- Total budget: ≤ 11,200 dimensions\n",
    "\n",
    "### 5. Analysis Pipeline\n",
    "- Baseline evaluation → Attention pattern analysis → Node selection → Validation\n",
    "- Baseline accuracy: 94%\n",
    "\n",
    "### 6. Head Detection Methods\n",
    "- Duplicate Token Heads: Attention S2 → S1 (top: a3.h0 with 0.7191)\n",
    "- S-Inhibition Heads: Attention END → S2 (top: a8.h6 with 0.7441)\n",
    "- Name-Mover Heads: Attention END → IO (top: a9.h9 with 0.7998)\n",
    "\n",
    "### 7. Circuit Composition\n",
    "- Total: 44 nodes (1 input + 31 heads + 12 MLPs)\n",
    "- Budget: 31×64 + 12×768 = 1,984 + 9,216 = 11,200 (100% utilization)\n",
    "\n",
    "### 8. Key Findings\n",
    "- Layered processing: Early (0-3) → Middle (7-8) → Late (9-11)\n",
    "- High selectivity (>0.7 attention for top heads)\n",
    "- Efficient representation (10.1% of total model capacity)\n",
    "\n",
    "### 9. Attention Head Categories\n",
    "- 6 Duplicate Token Heads (layers 0, 1, 3)\n",
    "- 12 S-Inhibition Heads (layers 7, 8, 9)\n",
    "- 15 Name-Mover Heads (layers 9, 10, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bea35e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exam Questions\n",
    "\n",
    "The following questions test both factual understanding and ability to apply, reason, and generalize from the documentation.\n",
    "\n",
    "**Question Distribution**: \n",
    "- Multiple Choice: 8 questions\n",
    "- Free Generation: 10 questions  \n",
    "- Code-Based: 3 questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624f51c",
   "metadata": {},
   "source": [
    "### Question 1 (Multiple Choice)\n",
    "\n",
    "What is the primary research objective of the IOI circuit analysis experiment?\n",
    "\n",
    "A) Train GPT2-small to perform the IOI task from scratch\n",
    "B) Compare different transformer architectures on the IOI task\n",
    "C) Identify a precise circuit in GPT2-small that implements IOI behavior within a write budget constraint\n",
    "D) Optimize GPT2-small's performance on the IOI task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fba83",
   "metadata": {},
   "source": [
    "### Question 2 (Multiple Choice)\n",
    "\n",
    "According to the three-component hypothesis, what is the function of S-Inhibition Heads?\n",
    "\n",
    "A) Attend from S2 to S1 to detect duplicate tokens\n",
    "B) Attend from END to S2 to inhibit Name-Mover attention to subject positions\n",
    "C) Attend from END to IO to copy the indirect object\n",
    "D) Attend from S1 to END to establish positional context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ace29",
   "metadata": {},
   "source": [
    "### Question 3 (Free Generation)\n",
    "\n",
    "What dataset was used for this analysis, and how many examples were used? Identify the four key positions in the IOI sentence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba19eb",
   "metadata": {},
   "source": [
    "### Question 4 (Multiple Choice)\n",
    "\n",
    "What is the d_head dimension in GPT2-small, and how is it calculated?\n",
    "\n",
    "A) 768, calculated as d_model\n",
    "B) 64, calculated as d_model / n_heads\n",
    "C) 3072, calculated as 4 × d_model\n",
    "D) 12, calculated as n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f92b91",
   "metadata": {},
   "source": [
    "### Question 5 (Free Generation)\n",
    "\n",
    "Explain the write budget allocation in this experiment. How many dimensions does each attention head write? How many dimensions does each MLP write?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a51743",
   "metadata": {},
   "source": [
    "### Question 6 (Multiple Choice)\n",
    "\n",
    "What was GPT2-small's baseline accuracy on the IOI task?\n",
    "\n",
    "A) 100%\n",
    "B) 72%\n",
    "C) 80%\n",
    "D) 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9013bf",
   "metadata": {},
   "source": [
    "### Question 7 (Free Generation)\n",
    "\n",
    "Which attention head had the highest average S2→S1 attention score for duplicate token detection, and what was its score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c7b6f",
   "metadata": {},
   "source": [
    "### Question 8 (Multiple Choice)\n",
    "\n",
    "How many total nodes are in the final circuit, and what is their breakdown?\n",
    "\n",
    "A) 31 nodes: 19 attention heads and 12 MLPs\n",
    "B) 43 nodes: 31 attention heads and 12 MLPs\n",
    "C) 44 nodes: 1 input, 31 attention heads, and 12 MLPs\n",
    "D) 44 nodes: 32 attention heads and 12 MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74e5ec",
   "metadata": {},
   "source": [
    "### Question 9 (Free Generation)\n",
    "\n",
    "Describe the layer distribution pattern of the three types of attention heads in the identified circuit. Which layers contain each type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9863130",
   "metadata": {},
   "source": [
    "### Question 10 (Multiple Choice)\n",
    "\n",
    "According to the final circuit composition, how many attention heads belong to each functional category?\n",
    "\n",
    "A) 6 Duplicate Token Heads, 12 S-Inhibition Heads, 15 Name-Mover Heads\n",
    "B) 10 Duplicate Token Heads, 10 S-Inhibition Heads, 11 Name-Mover Heads\n",
    "C) 8 Duplicate Token Heads, 8 S-Inhibition Heads, 15 Name-Mover Heads\n",
    "D) 5 Duplicate Token Heads, 15 S-Inhibition Heads, 11 Name-Mover Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604f24c",
   "metadata": {},
   "source": [
    "### Question 11 (Free Generation)\n",
    "\n",
    "Given the final circuit composition of 31 attention heads and 12 MLPs, calculate the total write budget used. Show your work and explain why this configuration achieves 100% budget utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af558fd",
   "metadata": {},
   "source": [
    "### Question 12 (Free Generation)\n",
    "\n",
    "Explain the causal mechanism by which the three types of heads work together to solve the IOI task. How does information flow from S1 to the final prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964505d",
   "metadata": {},
   "source": [
    "### Question 13 (Multiple Choice)\n",
    "\n",
    "If the IOI task were modified so that the sentence structure changed from 'A and B ... A gave to __' to 'B and A ... A gave to __' (swapping the order of first appearance), which component of the hypothesis would need to be reconsidered first?\n",
    "\n",
    "A) Duplicate Token Heads, because S1 and S2 positions would change relative to IO position\n",
    "B) S-Inhibition Heads, because they wouldn't know which token to inhibit\n",
    "C) Name-Mover Heads, because they would attend to the wrong position\n",
    "D) MLPs, because they would fail to transform features correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73810afc",
   "metadata": {},
   "source": [
    "### Question 14 (Free Generation)\n",
    "\n",
    "The documentation mentions that the IOI circuit uses only 10.1% of total model capacity. Calculate the total possible dimensions in GPT2-small and explain what this sparse representation implies about how the model implements the IOI task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7ff17",
   "metadata": {},
   "source": [
    "### Question 15 (Multiple Choice)\n",
    "\n",
    "Why was attention averaging across all 100 examples used to identify functional head types rather than analyzing individual examples?\n",
    "\n",
    "A) To reduce computational cost\n",
    "B) To increase the number of data points\n",
    "C) To find heads with consistent, specialized behavior across contexts rather than context-dependent heads\n",
    "D) To enable statistical significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c6647",
   "metadata": {},
   "source": [
    "### Question 16 (Free Generation)\n",
    "\n",
    "Explain why all 12 MLPs were included in the circuit even though the main focus was on attention head analysis. What role do MLPs play in the IOI circuit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256a9f3",
   "metadata": {},
   "source": [
    "### Question 17 (Free Generation)\n",
    "\n",
    "The documentation mentions 'Negative Name Movers' as an alternative hypothesis to explore. Based on your understanding of the circuit, what might be the function of Negative Name Mover heads and how would they complement the existing three-component hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5db35d",
   "metadata": {},
   "source": [
    "### Question 18 (Multiple Choice)\n",
    "\n",
    "If ablation studies were performed and removing a8.h6 (the top S-Inhibition Head with 0.74 attention score) caused minimal performance drop, what would be the most likely explanation?\n",
    "\n",
    "A) The S-Inhibition hypothesis is incorrect\n",
    "B) The circuit has redundant S-Inhibition heads that compensate for the ablation\n",
    "C) S-Inhibition is not necessary for the IOI task\n",
    "D) The attention score measurement was inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53034f",
   "metadata": {},
   "source": [
    "### Question 19 (Code Question (CQ1))\n",
    "\n",
    "Write code to verify the write budget of the IOI circuit. Given the circuit nodes list, count the attention heads and MLPs, then calculate the total budget used and utilization percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c46ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ1: Circuit Budget Verification\n",
    "# Your task is to verify the write budget of the identified IOI circuit.\n",
    "\n",
    "# Given circuit data:\n",
    "circuit_nodes = [\n",
    "    \"input\",\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\",\n",
    "    \"m0\", \"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"m10\", \"m11\"\n",
    "]\n",
    "\n",
    "# Model configuration\n",
    "D_HEAD = 64  # dimensions per attention head\n",
    "D_MODEL = 768  # dimensions per MLP\n",
    "BUDGET_LIMIT = 11200  # maximum allowed dimensions\n",
    "\n",
    "# TODO: Count the number of attention heads (nodes starting with 'a')\n",
    "n_attention_heads = None  # Your code here\n",
    "\n",
    "# TODO: Count the number of MLPs (nodes starting with 'm')\n",
    "n_mlps = None  # Your code here\n",
    "\n",
    "# TODO: Calculate total write budget used\n",
    "total_budget = None  # Your code here\n",
    "\n",
    "# TODO: Calculate budget utilization percentage\n",
    "utilization_percentage = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Attention heads: {n_attention_heads}\")\n",
    "print(f\"MLPs: {n_mlps}\")\n",
    "print(f\"Total budget used: {total_budget}\")\n",
    "print(f\"Budget utilization: {utilization_percentage:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93eeca4",
   "metadata": {},
   "source": [
    "### Question 20 (Code Question (CQ2))\n",
    "\n",
    "Write code to analyze the layer distribution of attention heads in the circuit. Parse each head name to extract its layer, count heads per layer, find the layer with most heads, and count heads in early (0-3) and late (9-11) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ2: Layer Distribution Analysis\n",
    "# Your task is to analyze the layer distribution of attention heads in the circuit.\n",
    "\n",
    "# Given attention head nodes (extracted from circuit):\n",
    "attention_heads = [\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\"\n",
    "]\n",
    "\n",
    "# TODO: Parse the layer number from each head name (format: a{layer}.h{head})\n",
    "# Create a dictionary counting heads per layer\n",
    "layer_counts = {}  # Your code here\n",
    "\n",
    "# TODO: Find which layer has the maximum number of heads\n",
    "max_layer = None  # Your code here\n",
    "max_count = None  # Your code here\n",
    "\n",
    "# TODO: Calculate the total number of heads in early layers (0-3)\n",
    "early_layer_count = None  # Your code here\n",
    "\n",
    "# TODO: Calculate the total number of heads in late layers (9-11)\n",
    "late_layer_count = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Heads per layer: {dict(sorted(layer_counts.items()))}\")\n",
    "print(f\"Layer with most heads: {max_layer} ({max_count} heads)\")\n",
    "print(f\"Early layer heads (0-3): {early_layer_count}\")\n",
    "print(f\"Late layer heads (9-11): {late_layer_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e923db",
   "metadata": {},
   "source": [
    "### Question 21 (Code Question (CQ3))\n",
    "\n",
    "Write code to analyze the attention scores across the three head types. Calculate mean scores for each type, identify which type has the highest mean, count heads with high selectivity (>0.5), and find the overall top head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ3: Attention Score Ranking Analysis\n",
    "# Your task is to analyze and compare attention scores across the three head types.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Top attention scores from the documentation\n",
    "duplicate_token_scores = {\n",
    "    \"a3.h0\": 0.7191,\n",
    "    \"a1.h11\": 0.6613,\n",
    "    \"a0.h5\": 0.6080,\n",
    "    \"a0.h1\": 0.5152,\n",
    "    \"a0.h10\": 0.2359\n",
    "}\n",
    "\n",
    "s_inhibition_scores = {\n",
    "    \"a8.h6\": 0.7441,\n",
    "    \"a7.h9\": 0.5079,\n",
    "    \"a8.h10\": 0.3037,\n",
    "    \"a8.h5\": 0.2852,\n",
    "    \"a9.h7\": 0.2557\n",
    "}\n",
    "\n",
    "name_mover_scores = {\n",
    "    \"a9.h9\": 0.7998,\n",
    "    \"a10.h7\": 0.7829,\n",
    "    \"a9.h6\": 0.7412,\n",
    "    \"a11.h10\": 0.6369,\n",
    "    \"a10.h0\": 0.3877\n",
    "}\n",
    "\n",
    "# TODO: Calculate the mean attention score for each head type\n",
    "mean_duplicate = None  # Your code here\n",
    "mean_s_inhibition = None  # Your code here\n",
    "mean_name_mover = None  # Your code here\n",
    "\n",
    "# TODO: Determine which head type has the highest mean attention score\n",
    "highest_mean_type = None  # Your code here\n",
    "\n",
    "# TODO: Count how many heads in each category have attention > 0.5 (high selectivity)\n",
    "high_selectivity_duplicate = None  # Your code here\n",
    "high_selectivity_s_inhibition = None  # Your code here\n",
    "high_selectivity_name_mover = None  # Your code here\n",
    "\n",
    "# TODO: Find the overall top head across all categories\n",
    "all_heads = {**duplicate_token_scores, **s_inhibition_scores, **name_mover_scores}\n",
    "top_head = None  # Your code here\n",
    "top_score = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean duplicate token score: {mean_duplicate:.4f}\")\n",
    "print(f\"Mean S-inhibition score: {mean_s_inhibition:.4f}\")\n",
    "print(f\"Mean name mover score: {mean_name_mover:.4f}\")\n",
    "print(f\"Highest mean type: {highest_mean_type}\")\n",
    "print(f\"High selectivity counts (>0.5): Duplicate={high_selectivity_duplicate}, S-Inhibition={high_selectivity_s_inhibition}, Name-Mover={high_selectivity_name_mover}\")\n",
    "print(f\"Top head overall: {top_head} ({top_score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
