{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169c17b7",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Code Question Solutions\n",
    "\n",
    "This notebook contains the code questions with their solutions and auto-check cells.\n",
    "\n",
    "**Instructions**: Run each cell from top to bottom. The auto-check cells validate the expected outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9af94f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CQ1: Write code to verify the write budget of the IOI circuit. Given the circuit nodes list, count the attention heads and MLPs, then calculate the total budget used and utilization percentage.\n",
    "\n",
    "**Reference**: Section 4. Results - Budget Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00b260",
   "metadata": {},
   "source": [
    "### Student Stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ1: Circuit Budget Verification\n",
    "# Your task is to verify the write budget of the identified IOI circuit.\n",
    "\n",
    "# Given circuit data:\n",
    "circuit_nodes = [\n",
    "    \"input\",\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\",\n",
    "    \"m0\", \"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"m10\", \"m11\"\n",
    "]\n",
    "\n",
    "# Model configuration\n",
    "D_HEAD = 64  # dimensions per attention head\n",
    "D_MODEL = 768  # dimensions per MLP\n",
    "BUDGET_LIMIT = 11200  # maximum allowed dimensions\n",
    "\n",
    "# TODO: Count the number of attention heads (nodes starting with 'a')\n",
    "n_attention_heads = None  # Your code here\n",
    "\n",
    "# TODO: Count the number of MLPs (nodes starting with 'm')\n",
    "n_mlps = None  # Your code here\n",
    "\n",
    "# TODO: Calculate total write budget used\n",
    "total_budget = None  # Your code here\n",
    "\n",
    "# TODO: Calculate budget utilization percentage\n",
    "utilization_percentage = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Attention heads: {n_attention_heads}\")\n",
    "print(f\"MLPs: {n_mlps}\")\n",
    "print(f\"Total budget used: {total_budget}\")\n",
    "print(f\"Budget utilization: {utilization_percentage:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa0ce0",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ1: Circuit Budget Verification - SOLUTION\n",
    "\n",
    "# Given circuit data:\n",
    "circuit_nodes = [\n",
    "    \"input\",\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\",\n",
    "    \"m0\", \"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"m10\", \"m11\"\n",
    "]\n",
    "\n",
    "# Model configuration\n",
    "D_HEAD = 64  # dimensions per attention head\n",
    "D_MODEL = 768  # dimensions per MLP\n",
    "BUDGET_LIMIT = 11200  # maximum allowed dimensions\n",
    "\n",
    "# Count the number of attention heads (nodes starting with 'a')\n",
    "n_attention_heads = sum(1 for node in circuit_nodes if node.startswith('a'))\n",
    "\n",
    "# Count the number of MLPs (nodes starting with 'm')\n",
    "n_mlps = sum(1 for node in circuit_nodes if node.startswith('m'))\n",
    "\n",
    "# Calculate total write budget used\n",
    "total_budget = n_attention_heads * D_HEAD + n_mlps * D_MODEL\n",
    "\n",
    "# Calculate budget utilization percentage\n",
    "utilization_percentage = (total_budget / BUDGET_LIMIT) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Attention heads: {n_attention_heads}\")\n",
    "print(f\"MLPs: {n_mlps}\")\n",
    "print(f\"Total budget used: {total_budget}\")\n",
    "print(f\"Budget utilization: {utilization_percentage:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9fa03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CQ2: Write code to analyze the layer distribution of attention heads in the circuit. Parse each head name to extract its layer, count heads per layer, find the layer with most heads, and count heads in early (0-3) and late (9-11) layers.\n",
    "\n",
    "**Reference**: Section 4. Results - Layer Distribution and Section 5. Analysis - Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d370988",
   "metadata": {},
   "source": [
    "### Student Stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf95a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ2: Layer Distribution Analysis\n",
    "# Your task is to analyze the layer distribution of attention heads in the circuit.\n",
    "\n",
    "# Given attention head nodes (extracted from circuit):\n",
    "attention_heads = [\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\"\n",
    "]\n",
    "\n",
    "# TODO: Parse the layer number from each head name (format: a{layer}.h{head})\n",
    "# Create a dictionary counting heads per layer\n",
    "layer_counts = {}  # Your code here\n",
    "\n",
    "# TODO: Find which layer has the maximum number of heads\n",
    "max_layer = None  # Your code here\n",
    "max_count = None  # Your code here\n",
    "\n",
    "# TODO: Calculate the total number of heads in early layers (0-3)\n",
    "early_layer_count = None  # Your code here\n",
    "\n",
    "# TODO: Calculate the total number of heads in late layers (9-11)\n",
    "late_layer_count = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Heads per layer: {dict(sorted(layer_counts.items()))}\")\n",
    "print(f\"Layer with most heads: {max_layer} ({max_count} heads)\")\n",
    "print(f\"Early layer heads (0-3): {early_layer_count}\")\n",
    "print(f\"Late layer heads (9-11): {late_layer_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edfd35",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ2: Layer Distribution Analysis - SOLUTION\n",
    "\n",
    "# Given attention head nodes (extracted from circuit):\n",
    "attention_heads = [\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\"\n",
    "]\n",
    "\n",
    "# Parse the layer number from each head name (format: a{layer}.h{head})\n",
    "layer_counts = {}\n",
    "for head in attention_heads:\n",
    "    layer = int(head.split('.')[0][1:])  # Extract layer number\n",
    "    layer_counts[layer] = layer_counts.get(layer, 0) + 1\n",
    "\n",
    "# Find which layer has the maximum number of heads\n",
    "max_layer = max(layer_counts.keys(), key=lambda x: layer_counts[x])\n",
    "max_count = layer_counts[max_layer]\n",
    "\n",
    "# Calculate the total number of heads in early layers (0-3)\n",
    "early_layer_count = sum(layer_counts.get(i, 0) for i in range(4))\n",
    "\n",
    "# Calculate the total number of heads in late layers (9-11)\n",
    "late_layer_count = sum(layer_counts.get(i, 0) for i in range(9, 12))\n",
    "\n",
    "# Print results\n",
    "print(f\"Heads per layer: {dict(sorted(layer_counts.items()))}\")\n",
    "print(f\"Layer with most heads: {max_layer} ({max_count} heads)\")\n",
    "print(f\"Early layer heads (0-3): {early_layer_count}\")\n",
    "print(f\"Late layer heads (9-11): {late_layer_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3956982",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CQ3: Write code to analyze the attention scores across the three head types. Calculate mean scores for each type, identify which type has the highest mean, count heads with high selectivity (>0.5), and find the overall top head.\n",
    "\n",
    "**Reference**: Section 3.3 Analysis Pipeline - Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f96dbe",
   "metadata": {},
   "source": [
    "### Student Stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1431c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ3: Attention Score Ranking Analysis\n",
    "# Your task is to analyze and compare attention scores across the three head types.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Top attention scores from the documentation\n",
    "duplicate_token_scores = {\n",
    "    \"a3.h0\": 0.7191,\n",
    "    \"a1.h11\": 0.6613,\n",
    "    \"a0.h5\": 0.6080,\n",
    "    \"a0.h1\": 0.5152,\n",
    "    \"a0.h10\": 0.2359\n",
    "}\n",
    "\n",
    "s_inhibition_scores = {\n",
    "    \"a8.h6\": 0.7441,\n",
    "    \"a7.h9\": 0.5079,\n",
    "    \"a8.h10\": 0.3037,\n",
    "    \"a8.h5\": 0.2852,\n",
    "    \"a9.h7\": 0.2557\n",
    "}\n",
    "\n",
    "name_mover_scores = {\n",
    "    \"a9.h9\": 0.7998,\n",
    "    \"a10.h7\": 0.7829,\n",
    "    \"a9.h6\": 0.7412,\n",
    "    \"a11.h10\": 0.6369,\n",
    "    \"a10.h0\": 0.3877\n",
    "}\n",
    "\n",
    "# TODO: Calculate the mean attention score for each head type\n",
    "mean_duplicate = None  # Your code here\n",
    "mean_s_inhibition = None  # Your code here\n",
    "mean_name_mover = None  # Your code here\n",
    "\n",
    "# TODO: Determine which head type has the highest mean attention score\n",
    "highest_mean_type = None  # Your code here\n",
    "\n",
    "# TODO: Count how many heads in each category have attention > 0.5 (high selectivity)\n",
    "high_selectivity_duplicate = None  # Your code here\n",
    "high_selectivity_s_inhibition = None  # Your code here\n",
    "high_selectivity_name_mover = None  # Your code here\n",
    "\n",
    "# TODO: Find the overall top head across all categories\n",
    "all_heads = {**duplicate_token_scores, **s_inhibition_scores, **name_mover_scores}\n",
    "top_head = None  # Your code here\n",
    "top_score = None  # Your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean duplicate token score: {mean_duplicate:.4f}\")\n",
    "print(f\"Mean S-inhibition score: {mean_s_inhibition:.4f}\")\n",
    "print(f\"Mean name mover score: {mean_name_mover:.4f}\")\n",
    "print(f\"Highest mean type: {highest_mean_type}\")\n",
    "print(f\"High selectivity counts (>0.5): Duplicate={high_selectivity_duplicate}, S-Inhibition={high_selectivity_s_inhibition}, Name-Mover={high_selectivity_name_mover}\")\n",
    "print(f\"Top head overall: {top_head} ({top_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a03552",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c62236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ3: Attention Score Ranking Analysis - SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Top attention scores from the documentation\n",
    "duplicate_token_scores = {\n",
    "    \"a3.h0\": 0.7191,\n",
    "    \"a1.h11\": 0.6613,\n",
    "    \"a0.h5\": 0.6080,\n",
    "    \"a0.h1\": 0.5152,\n",
    "    \"a0.h10\": 0.2359\n",
    "}\n",
    "\n",
    "s_inhibition_scores = {\n",
    "    \"a8.h6\": 0.7441,\n",
    "    \"a7.h9\": 0.5079,\n",
    "    \"a8.h10\": 0.3037,\n",
    "    \"a8.h5\": 0.2852,\n",
    "    \"a9.h7\": 0.2557\n",
    "}\n",
    "\n",
    "name_mover_scores = {\n",
    "    \"a9.h9\": 0.7998,\n",
    "    \"a10.h7\": 0.7829,\n",
    "    \"a9.h6\": 0.7412,\n",
    "    \"a11.h10\": 0.6369,\n",
    "    \"a10.h0\": 0.3877\n",
    "}\n",
    "\n",
    "# Calculate the mean attention score for each head type\n",
    "mean_duplicate = np.mean(list(duplicate_token_scores.values()))\n",
    "mean_s_inhibition = np.mean(list(s_inhibition_scores.values()))\n",
    "mean_name_mover = np.mean(list(name_mover_scores.values()))\n",
    "\n",
    "# Determine which head type has the highest mean attention score\n",
    "means = {\"Duplicate Token\": mean_duplicate, \"S-Inhibition\": mean_s_inhibition, \"Name Mover\": mean_name_mover}\n",
    "highest_mean_type = max(means.keys(), key=lambda x: means[x])\n",
    "\n",
    "# Count how many heads in each category have attention > 0.5 (high selectivity)\n",
    "high_selectivity_duplicate = sum(1 for v in duplicate_token_scores.values() if v > 0.5)\n",
    "high_selectivity_s_inhibition = sum(1 for v in s_inhibition_scores.values() if v > 0.5)\n",
    "high_selectivity_name_mover = sum(1 for v in name_mover_scores.values() if v > 0.5)\n",
    "\n",
    "# Find the overall top head across all categories\n",
    "all_heads = {**duplicate_token_scores, **s_inhibition_scores, **name_mover_scores}\n",
    "top_head = max(all_heads.keys(), key=lambda x: all_heads[x])\n",
    "top_score = all_heads[top_head]\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean duplicate token score: {mean_duplicate:.4f}\")\n",
    "print(f\"Mean S-inhibition score: {mean_s_inhibition:.4f}\")\n",
    "print(f\"Mean name mover score: {mean_name_mover:.4f}\")\n",
    "print(f\"Highest mean type: {highest_mean_type}\")\n",
    "print(f\"High selectivity counts (>0.5): Duplicate={high_selectivity_duplicate}, S-Inhibition={high_selectivity_s_inhibition}, Name-Mover={high_selectivity_name_mover}\")\n",
    "print(f\"Top head overall: {top_head} ({top_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d908fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Check Validation\n",
    "\n",
    "The following cells validate the expected outputs from each code question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b295c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-check for CQ1: Circuit Budget Verification\n",
    "print(\"=== CQ1 Validation ===\")\n",
    "\n",
    "# Expected values\n",
    "expected_heads = 31\n",
    "expected_mlps = 12\n",
    "expected_budget = 11200\n",
    "expected_utilization = 100.0\n",
    "\n",
    "# Calculate actuals\n",
    "circuit_nodes = [\n",
    "    \"input\",\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\",\n",
    "    \"m0\", \"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"m10\", \"m11\"\n",
    "]\n",
    "\n",
    "actual_heads = sum(1 for node in circuit_nodes if node.startswith('a'))\n",
    "actual_mlps = sum(1 for node in circuit_nodes if node.startswith('m'))\n",
    "actual_budget = actual_heads * 64 + actual_mlps * 768\n",
    "actual_utilization = (actual_budget / 11200) * 100\n",
    "\n",
    "# Validate\n",
    "assert actual_heads == expected_heads, f\"Heads mismatch: {actual_heads} != {expected_heads}\"\n",
    "assert actual_mlps == expected_mlps, f\"MLPs mismatch: {actual_mlps} != {expected_mlps}\"\n",
    "assert actual_budget == expected_budget, f\"Budget mismatch: {actual_budget} != {expected_budget}\"\n",
    "assert abs(actual_utilization - expected_utilization) < 0.1, f\"Utilization mismatch: {actual_utilization} != {expected_utilization}\"\n",
    "\n",
    "print(\"✓ All CQ1 checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-check for CQ2: Layer Distribution Analysis\n",
    "print(\"=== CQ2 Validation ===\")\n",
    "\n",
    "# Expected values\n",
    "expected_max_layer = 10\n",
    "expected_max_count = 7\n",
    "expected_early_count = 7\n",
    "expected_late_count = 16\n",
    "\n",
    "# Calculate actuals\n",
    "attention_heads = [\n",
    "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
    "    \"a1.h11\",\n",
    "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
    "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
    "    \"a3.h0\", \"a3.h6\",\n",
    "    \"a6.h0\",\n",
    "    \"a7.h3\", \"a7.h9\",\n",
    "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
    "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\"\n",
    "]\n",
    "\n",
    "layer_counts = {}\n",
    "for head in attention_heads:\n",
    "    layer = int(head.split('.')[0][1:])\n",
    "    layer_counts[layer] = layer_counts.get(layer, 0) + 1\n",
    "\n",
    "actual_max_layer = max(layer_counts.keys(), key=lambda x: layer_counts[x])\n",
    "actual_max_count = layer_counts[actual_max_layer]\n",
    "actual_early_count = sum(layer_counts.get(i, 0) for i in range(4))\n",
    "actual_late_count = sum(layer_counts.get(i, 0) for i in range(9, 12))\n",
    "\n",
    "# Validate\n",
    "assert actual_max_layer == expected_max_layer, f\"Max layer mismatch: {actual_max_layer} != {expected_max_layer}\"\n",
    "assert actual_max_count == expected_max_count, f\"Max count mismatch: {actual_max_count} != {expected_max_count}\"\n",
    "assert actual_early_count == expected_early_count, f\"Early count mismatch: {actual_early_count} != {expected_early_count}\"\n",
    "assert actual_late_count == expected_late_count, f\"Late count mismatch: {actual_late_count} != {expected_late_count}\"\n",
    "\n",
    "print(\"✓ All CQ2 checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-check for CQ3: Attention Score Ranking Analysis\n",
    "import numpy as np\n",
    "print(\"=== CQ3 Validation ===\")\n",
    "\n",
    "# Expected values (with tolerance)\n",
    "expected_mean_duplicate = 0.5479\n",
    "expected_mean_s_inhibition = 0.4193\n",
    "expected_mean_name_mover = 0.6697\n",
    "expected_highest_type = \"Name Mover\"\n",
    "expected_top_head = \"a9.h9\"\n",
    "expected_top_score = 0.7998\n",
    "\n",
    "# Calculate actuals\n",
    "duplicate_token_scores = {\"a3.h0\": 0.7191, \"a1.h11\": 0.6613, \"a0.h5\": 0.6080, \"a0.h1\": 0.5152, \"a0.h10\": 0.2359}\n",
    "s_inhibition_scores = {\"a8.h6\": 0.7441, \"a7.h9\": 0.5079, \"a8.h10\": 0.3037, \"a8.h5\": 0.2852, \"a9.h7\": 0.2557}\n",
    "name_mover_scores = {\"a9.h9\": 0.7998, \"a10.h7\": 0.7829, \"a9.h6\": 0.7412, \"a11.h10\": 0.6369, \"a10.h0\": 0.3877}\n",
    "\n",
    "actual_mean_duplicate = np.mean(list(duplicate_token_scores.values()))\n",
    "actual_mean_s_inhibition = np.mean(list(s_inhibition_scores.values()))\n",
    "actual_mean_name_mover = np.mean(list(name_mover_scores.values()))\n",
    "\n",
    "means = {\"Duplicate Token\": actual_mean_duplicate, \"S-Inhibition\": actual_mean_s_inhibition, \"Name Mover\": actual_mean_name_mover}\n",
    "actual_highest_type = max(means.keys(), key=lambda x: means[x])\n",
    "\n",
    "all_heads = {**duplicate_token_scores, **s_inhibition_scores, **name_mover_scores}\n",
    "actual_top_head = max(all_heads.keys(), key=lambda x: all_heads[x])\n",
    "actual_top_score = all_heads[actual_top_head]\n",
    "\n",
    "# Validate (with tolerance for floating point)\n",
    "tolerance = 0.001\n",
    "assert abs(actual_mean_duplicate - expected_mean_duplicate) < tolerance, f\"Duplicate mean mismatch\"\n",
    "assert abs(actual_mean_s_inhibition - expected_mean_s_inhibition) < tolerance, f\"S-inhibition mean mismatch\"\n",
    "assert abs(actual_mean_name_mover - expected_mean_name_mover) < tolerance, f\"Name mover mean mismatch\"\n",
    "assert actual_highest_type == expected_highest_type, f\"Highest type mismatch: {actual_highest_type} != {expected_highest_type}\"\n",
    "assert actual_top_head == expected_top_head, f\"Top head mismatch: {actual_top_head} != {expected_top_head}\"\n",
    "assert abs(actual_top_score - expected_top_score) < tolerance, f\"Top score mismatch\"\n",
    "\n",
    "print(\"✓ All CQ3 checks passed!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
