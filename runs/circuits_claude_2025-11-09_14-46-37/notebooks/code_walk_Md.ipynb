{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd25b5af",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Code Walkthrough\n",
    "\n",
    "## Overview\n",
    "\n",
    "This document provides a detailed walkthrough of the implementation used to identify the IOI circuit in GPT2-small.\n",
    "\n",
    "## Setup and Initialization\n",
    "\n",
    "### 1. Environment Configuration\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "\n",
    "# Check for GPU availability\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "\n",
    "**Purpose**: Set working directory and configure compute device. Using CUDA significantly speeds up model inference and activation caching.\n",
    "\n",
    "### 2. Load Model\n",
    "\n",
    "```python\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)\n",
    "```\n",
    "\n",
    "**Key Configuration**:\n",
    "- `n_layers`: 12\n",
    "- `n_heads`: 12  \n",
    "- `d_model`: 768\n",
    "- `d_head`: 64\n",
    "\n",
    "**Why TransformerLens**: Provides easy access to activation caching and intervention capabilities needed for mechanistic interpretability.\n",
    "\n",
    "### 3. Load Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mib-bench/ioi\")\n",
    "ioi_data = dataset['train']\n",
    "```\n",
    "\n",
    "**Dataset Structure**:\n",
    "- Each example has a `prompt`, `choices`, `answerKey`, and `metadata`\n",
    "- Metadata includes `subject`, `indirect_object`, `object`, and `place`\n",
    "- Multiple counterfactual variants provided for each example\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "### 4. Position Identification\n",
    "\n",
    "```python\n",
    "def find_positions(prompt_idx):\n",
    "    tokens_str = model.to_str_tokens(prompts[prompt_idx])\n",
    "    s_name = s_names[prompt_idx]\n",
    "    \n",
    "    s1_pos = None\n",
    "    s2_pos = None\n",
    "    end_pos = len(tokens_str) - 1\n",
    "    \n",
    "    for i, token in enumerate(tokens_str):\n",
    "        if s_name in token:\n",
    "            if s1_pos is None:\n",
    "                s1_pos = i\n",
    "            else:\n",
    "                s2_pos = i\n",
    "                break\n",
    "    \n",
    "    return s1_pos, s2_pos, end_pos, tokens_str\n",
    "```\n",
    "\n",
    "**Purpose**: Locate critical positions in each sentence:\n",
    "- **S1**: First subject mention (typically position 2)\n",
    "- **S2**: Second subject mention (varies by sentence structure)\n",
    "- **END**: Last token position (where model predicts next token)\n",
    "\n",
    "**Challenge**: Names are tokenized differently, so we search for name substrings in tokens rather than exact matches.\n",
    "\n",
    "### 5. Baseline Evaluation\n",
    "\n",
    "```python\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    _, _, end_pos, _ = find_positions(i)\n",
    "    end_logits = logits[i, end_pos, :]\n",
    "    \n",
    "    io_token = model.to_single_token(' ' + io_names[i])\n",
    "    s_token = model.to_single_token(' ' + s_names[i])\n",
    "    \n",
    "    predicted_io = end_logits[io_token] > end_logits[s_token]\n",
    "```\n",
    "\n",
    "**Purpose**: Measure model's baseline performance by comparing logits for IO vs. Subject tokens.\n",
    "\n",
    "**Result**: 94% accuracy demonstrates model has learned IOI behavior.\n",
    "\n",
    "## Attention Pattern Analysis\n",
    "\n",
    "### 6. Duplicate Token Head Detection\n",
    "\n",
    "```python\n",
    "duplicate_token_scores = np.zeros((n_layers, n_heads))\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    s1_pos, s2_pos, _, _ = find_positions(i)\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
    "        \n",
    "        for head in range(n_heads):\n",
    "            attn_s2_to_s1 = attn_pattern[head, s2_pos, s1_pos].item()\n",
    "            duplicate_token_scores[layer, head] += attn_s2_to_s1\n",
    "\n",
    "duplicate_token_scores /= len(prompts)\n",
    "```\n",
    "\n",
    "**Mechanism**:\n",
    "1. Access cached attention patterns from each head\n",
    "2. Extract attention weight from S2 position to S1 position\n",
    "3. Average across all examples\n",
    "4. Higher scores indicate heads that specialize in duplicate token detection\n",
    "\n",
    "**Top Result**: a3.h0 with 0.72 average attention from S2→S1\n",
    "\n",
    "### 7. S-Inhibition Head Detection\n",
    "\n",
    "```python\n",
    "s_inhibition_scores = np.zeros((n_layers, n_heads))\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    s1_pos, s2_pos, end_pos, _ = find_positions(i)\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
    "        \n",
    "        for head in range(n_heads):\n",
    "            attn_end_to_s2 = attn_pattern[head, end_pos, s2_pos].item()\n",
    "            s_inhibition_scores[layer, head] += attn_end_to_s2\n",
    "\n",
    "s_inhibition_scores /= len(prompts)\n",
    "```\n",
    "\n",
    "**Mechanism**:\n",
    "1. Extract attention weight from END position to S2 position\n",
    "2. These heads attend to the subject to inhibit it from being predicted\n",
    "3. Works in conjunction with Name-Mover heads\n",
    "\n",
    "**Top Result**: a8.h6 with 0.74 average attention from END→S2\n",
    "\n",
    "### 8. Name-Mover Head Detection\n",
    "\n",
    "```python\n",
    "name_mover_scores = np.zeros((n_layers, n_heads))\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    tokens_str = model.to_str_tokens(prompts[i])\n",
    "    s1_pos, s2_pos, end_pos, _ = find_positions(i)\n",
    "    \n",
    "    # Find IO position\n",
    "    io_name = io_names[i]\n",
    "    io_pos = None\n",
    "    for j, token in enumerate(tokens_str):\n",
    "        if io_name in token and j != s1_pos and j != s2_pos:\n",
    "            io_pos = j\n",
    "            break\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][i]\n",
    "        \n",
    "        for head in range(n_heads):\n",
    "            attn_end_to_io = attn_pattern[head, end_pos, io_pos].item()\n",
    "            name_mover_scores[layer, head] += attn_end_to_io\n",
    "\n",
    "name_mover_scores /= len(prompts)\n",
    "```\n",
    "\n",
    "**Mechanism**:\n",
    "1. First locate IO position (different from S1/S2)\n",
    "2. Extract attention weight from END to IO\n",
    "3. These heads copy the IO token to the output\n",
    "\n",
    "**Top Result**: a9.h9 with 0.80 average attention from END→IO\n",
    "\n",
    "## Circuit Selection\n",
    "\n",
    "### 9. Head Selection Strategy\n",
    "\n",
    "```python\n",
    "# Select top heads from each category\n",
    "duplicate_heads_to_include = [\n",
    "    (layer, head) for _, layer, head in top_duplicate_heads[:3]\n",
    "]\n",
    "\n",
    "s_inhibition_heads_to_include = [\n",
    "    (layer, head) for _, layer, head in top_s_inhibition_heads[:3]\n",
    "]\n",
    "\n",
    "name_mover_heads_to_include = [\n",
    "    (layer, head) for _, layer, head in top_name_mover_heads[:4]\n",
    "]\n",
    "\n",
    "selected_heads = list(set(\n",
    "    duplicate_heads_to_include +\n",
    "    s_inhibition_heads_to_include +\n",
    "    name_mover_heads_to_include\n",
    "))\n",
    "```\n",
    "\n",
    "**Strategy**:\n",
    "1. Start with top performers from each category\n",
    "2. Remove duplicates (some heads rank high in multiple categories)\n",
    "3. Calculate budget used so far\n",
    "\n",
    "### 10. MLP Selection\n",
    "\n",
    "```python\n",
    "head_layers = sorted(set([layer for layer, _ in selected_heads]))\n",
    "\n",
    "# Include MLPs from layers with selected heads plus supporting layers\n",
    "selected_mlps = [0, 1]  # Early layers for feature extraction\n",
    "selected_mlps.extend(head_layers)  # Layers with attention heads\n",
    "selected_mlps.extend([2, 4, 5, 6])  # Middle layers for transformation\n",
    "\n",
    "selected_mlps = sorted(set(selected_mlps))\n",
    "```\n",
    "\n",
    "**Rationale**:\n",
    "- MLPs provide nonlinear transformations essential for circuit computation\n",
    "- Include early layers for basic feature extraction\n",
    "- Include layers with selected heads for local computation\n",
    "- Include middle layers for feature transformation\n",
    "\n",
    "### 11. Budget Maximization\n",
    "\n",
    "```python\n",
    "remaining_budget = 11200 - (len(selected_heads) * 64 + len(selected_mlps) * 768)\n",
    "max_additional_heads = remaining_budget // 64\n",
    "\n",
    "# Combine and sort all candidates\n",
    "all_important_heads = []\n",
    "for score, layer, head in top_duplicate_heads[:15]:\n",
    "    if (layer, head) not in selected_heads:\n",
    "        all_important_heads.append((score, layer, head, 'duplicate'))\n",
    "# ... repeat for other categories\n",
    "\n",
    "all_important_heads.sort(reverse=True)\n",
    "\n",
    "# Add top additional heads\n",
    "for i in range(max_additional_heads):\n",
    "    score, layer, head, category = all_important_heads[i]\n",
    "    selected_heads.append((layer, head))\n",
    "```\n",
    "\n",
    "**Purpose**: Maximize circuit expressiveness by using all available budget.\n",
    "\n",
    "**Result**: Added 21 additional heads, achieving exact 11,200-dimension budget usage.\n",
    "\n",
    "## Validation and Output\n",
    "\n",
    "### 12. Constraint Validation\n",
    "\n",
    "```python\n",
    "# Validate all nodes are in src_nodes\n",
    "for node in circuit_nodes:\n",
    "    if node not in src_nodes:\n",
    "        invalid_nodes.append(node)\n",
    "\n",
    "# Validate naming convention\n",
    "if node.startswith('a'):\n",
    "    # Check format: a{layer}.h{head}\n",
    "    parts = node.split('.')\n",
    "    # Validation logic...\n",
    "\n",
    "# Validate budget\n",
    "total_budget = len(selected_heads) * 64 + len(selected_mlps) * 768\n",
    "assert total_budget <= 11200\n",
    "```\n",
    "\n",
    "**Checks**:\n",
    "1. All nodes exist in allowed source nodes\n",
    "2. Naming follows convention (a{layer}.h{head}, m{layer})\n",
    "3. Total write budget ≤ 11,200 dimensions\n",
    "\n",
    "### 13. Save Circuit\n",
    "\n",
    "```python\n",
    "circuit_data = {\n",
    "    \"nodes\": circuit_nodes\n",
    "}\n",
    "\n",
    "with open('real_circuits_1.json', 'w') as f:\n",
    "    json.dump(circuit_data, f, indent=2)\n",
    "```\n",
    "\n",
    "**Output Format**: JSON with single \"nodes\" key containing ordered list of node names.\n",
    "\n",
    "## Key Implementation Details\n",
    "\n",
    "### Attention Pattern Caching\n",
    "\n",
    "TransformerLens's `run_with_cache` stores all intermediate activations:\n",
    "- `cache['blocks.{layer}.attn.hook_pattern']`: Attention probabilities after softmax\n",
    "- Shape: `[batch, n_heads, seq_len_q, seq_len_k]`\n",
    "- Values range [0, 1] and sum to 1 over key dimension\n",
    "\n",
    "### Averaging Across Examples\n",
    "\n",
    "We average attention scores across examples to find heads with consistent behavior:\n",
    "- Consistent high attention → specialized functionality\n",
    "- Inconsistent attention → general-purpose or context-dependent\n",
    "\n",
    "### Budget Calculation\n",
    "\n",
    "```\n",
    "Head budget = n_heads × (d_model / n_heads_per_layer)\n",
    "            = 31 × 64 = 1,984\n",
    "\n",
    "MLP budget = n_mlps × d_model  \n",
    "           = 12 × 768 = 9,216\n",
    "\n",
    "Total = 1,984 + 9,216 = 11,200 ✓\n",
    "```\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### GPU Acceleration\n",
    "- Model and data moved to CUDA\n",
    "- Batch processing of 100 examples\n",
    "- Activation caching done in single forward pass\n",
    "\n",
    "### Memory Management\n",
    "- Used subset of 100 examples (vs. full 10,000)\n",
    "- Cached activations for all layers at once\n",
    "- Total memory usage: ~2-3 GB GPU RAM\n",
    "\n",
    "### Computational Complexity\n",
    "- Forward pass: O(batch_size × seq_len²)\n",
    "- Attention analysis: O(n_layers × n_heads × n_examples)\n",
    "- Total runtime: ~2-3 minutes on A100\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "### Random Seeds\n",
    "No random operations used - results are fully deterministic given:\n",
    "1. Model weights (GPT2-small from Hugging Face)\n",
    "2. Dataset (mib-bench/ioi)\n",
    "3. Sample indices (first 100 examples)\n",
    "\n",
    "### Dependencies\n",
    "- `transformer_lens`: For model loading and activation caching\n",
    "- `datasets`: For loading IOI dataset\n",
    "- `torch`: For GPU computation\n",
    "- `numpy`: For numerical operations\n",
    "\n",
    "---\n",
    "\n",
    "**Code Organization**:\n",
    "- All analysis done in single Jupyter notebook\n",
    "- Modular functions for position finding and scoring\n",
    "- Clear separation of analysis phases (exploration → analysis → selection → validation)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
