[
  {
    "question_type": "multiple_choice",
    "question": "What is the primary goal of the IOI (Indirect Object Identification) task?",
    "choice": [
      "To predict the indirect object at the end of a sentence where two names appear, with one name repeated.",
      "To identify which subject performed an action in a sentence.",
      "To determine the direct object being transferred in a sentence.",
      "To classify whether a sentence contains duplicate tokens."
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "In the example sentence 'As Carl and Maria left the consulate, Carl gave a fridge to ___', what are the positions S1, S2, and IO respectively?",
    "choice": [
      "S1 = position 2 (first 'Carl'), S2 = position 9 (second 'Carl'), IO = position 4 ('Maria')",
      "S1 = position 4 ('Maria'), S2 = position 9 (second 'Carl'), IO = position 2 (first 'Carl')",
      "S1 = position 2 (first 'Carl'), S2 = position 4 ('Maria'), IO = position 9 (second 'Carl')",
      "S1 = position 9 (second 'Carl'), S2 = position 2 (first 'Carl'), IO = position 13 ('to')"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Describe the three functional components hypothesized to comprise the IOI circuit, including what positions they attend from/to and their proposed function.",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "How many dimensions does each attention head write to the residual stream in GPT2-small?",
    "choice": [
      "64 dimensions",
      "768 dimensions",
      "3,072 dimensions",
      "12 dimensions"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "What was the baseline accuracy of GPT2-small on the IOI task (100 example sample)?",
    "choice": [
      "94.00%",
      "87.50%",
      "99.00%",
      "72.00%"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Which attention head showed the highest average attention from S2 to S1 (Duplicate Token Head behavior)?",
    "choice": [
      "a3.h0 (0.7191)",
      "a1.h11 (0.6613)",
      "a8.h6 (0.7441)",
      "a9.h9 (0.7998)"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Which attention head showed the highest average attention from END to IO (Name-Mover Head behavior)?",
    "choice": [
      "a9.h9 (0.7998)",
      "a10.h7 (0.7829)",
      "a9.h6 (0.7412)",
      "a8.h6 (0.7441)"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "How many total nodes (including input, attention heads, and MLPs) are in the final identified circuit?",
    "choice": [
      "44 nodes (1 input + 31 heads + 12 MLPs)",
      "43 nodes (31 heads + 12 MLPs)",
      "31 nodes (attention heads only)",
      "55 nodes (1 input + 31 heads + 12 MLPs + 11 additional)"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Explain why the identified circuit exhibits 'layered processing' and what functional role each layer group plays in the IOI task.",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The documentation states that the circuit achieves 100% budget utilization with exactly 11,200 dimensions. If the researchers wanted to add 5 more attention heads to the circuit, how many MLPs would they need to remove to stay within budget? Show your calculation.",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Why does the documentation suggest that finding multiple heads per functional category indicates 'robustness through redundancy'?",
    "choice": [
      "Multiple heads performing similar functions provide backup pathways, so if one head fails or is ablated, others can compensate, making the circuit more resilient to perturbations.",
      "Having more heads increases the total budget utilization, making the circuit more efficient.",
      "Redundant heads allow the model to process multiple sentences in parallel.",
      "Multiple heads with the same function indicate that the circuit was overfit to the training data."
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Based on the methodology described, propose a concrete experiment to test whether the S-Inhibition Heads are causally necessary for the IOI task. What would you measure and what result would confirm their necessity?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "The circuit uses only 11,200 of 110,592 possible dimensions (10.1% of total model capacity). What does this suggest about how GPT2-small implements the IOI behavior?",
    "choice": [
      "IOI is implemented by a relatively sparse, specialized subcircuit rather than requiring the full model capacity, suggesting modular functional organization.",
      "The model is inefficient and wastes 90% of its capacity on irrelevant computations.",
      "IOI is a simple task that doesn't require sophisticated neural processing.",
      "The remaining 90% of capacity is used for error correction and robustness."
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Suppose you found a new attention head a5.h7 that shows strong attention from END to S1 (not S2). How would you classify this head relative to the three hypothesized categories, and what function might it serve?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Write code to verify the budget constraint calculation. Given the final circuit composition (31 attention heads and 12 MLPs), compute the total dimensional write and verify it equals 11,200. Also calculate what percentage of the budget is used by attention heads vs MLPs.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ1"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to analyze the layer distribution of the 31 attention heads in the circuit. Count how many heads are in each layer (0-11) and identify which layer has the most heads. Then determine if there's a trend showing more heads in later layers.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ2"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to simulate what would happen if you wanted to create a minimal IOI circuit using only the top-1 head from each functional category (Duplicate Token, S-Inhibition, Name-Mover) plus all 12 MLPs. Calculate the total budget used and how much budget remains unused compared to the full circuit.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ3"
  }
]