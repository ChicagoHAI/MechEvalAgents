{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e1ce19",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/smallyan/critic_model_mechinterp\n",
      "\n",
      "Repository path exists: True\n",
      "\n",
      "Repository structure:\n",
      "  evaluation/README.md\n",
      "  evaluation/code_critic_evaluation.ipynb\n",
      "  evaluation/eval_summary_self.ipynb\n",
      "  evaluation/eval_summary_ts.ipynb\n",
      "  evaluation/evaluation_summary.png\n",
      "  evaluation/goal_matching.ipynb\n",
      "  evaluation/hidden_test.ipynb\n",
      "  evaluation/matching_report.ipynb\n",
      "  evaluation/replications/circuits_replication_2025-11-09_18-55-54/README.md\n",
      "  evaluation/replications/circuits_replication_2025-11-09_18-55-54/documentation_replication.md\n",
      "  evaluation/replications/circuits_replication_2025-11-09_18-55-54/evaluation_replication.md\n",
      "  evaluation/replications/circuits_replication_2025-11-09_18-55-54/real_circuits_1.json\n",
      "  evaluation/replications/circuits_replication_2025-11-09_18-55-54/replication.ipynb\n",
      "  evaluation/self_matching.ipynb\n",
      "  logs/circuit_prompt_ioi_claude.log\n",
      "  logs/code_walk.md\n",
      "  logs/documentation.md\n",
      "  logs/plan.md\n",
      "  notebooks/2025-11-09-14-46_IOI_Circuit_Analysis.ipynb\n",
      "  notebooks/code_walk_Md.ipynb\n",
      "  notebooks/documentation_Md.ipynb\n",
      "  notebooks/plan_Md.ipynb\n",
      "  results/circuit_visualization.png\n",
      "  results/real_circuits_1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Define the repository path\n",
    "repo_path = Path('/home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-09_14-46-37')\n",
    "print(f\"\\nRepository path exists: {repo_path.exists()}\")\n",
    "\n",
    "# List all files in the repository\n",
    "if repo_path.exists():\n",
    "    print(\"\\nRepository structure:\")\n",
    "    for item in sorted(repo_path.rglob('*')):\n",
    "        if item.is_file():\n",
    "            relative_path = item.relative_to(repo_path)\n",
    "            print(f\"  {relative_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008f2ad6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTATION (Student-facing):\n",
      "================================================================================\n",
      "# IOI Circuit Analysis - Documentation\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "### Research Objective\n",
      "Identify a precise circuit in GPT2-small that implements the **Indirect Object Identification (IOI)** behavior while adhering to strict residual write-budget constraints (≤ 11,200 dimensions).\n",
      "\n",
      "### Hypothesis\n",
      "The IOI circuit comprises three functional components:\n",
      "\n",
      "1. **Duplicate Token Heads**: Active at S2, attending to S1, signaling token duplication through position features\n",
      "2. **S-Inhibition Heads**: Active at END, attending to S2, inhibiting Name-Mover attention to subject positions\n",
      "3. **Name-Mover Heads**: Active at END, attending to IO position, copying the indirect object token to the residual stream\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset\n",
      "- **Source**: mib-bench/ioi (Hugging Face)\n",
      "- **Size**: 10,000 examples (100 used for analysis)\n",
      "- **Task**: Predict the indirect object at the end of a sentence\n",
      "\n",
      "### Example Sentence Structure\n",
      "```\n",
      "\"As Carl and Maria left the consulate, Carl gave a fridge to ___\"\n",
      "```\n",
      "\n",
      "**Key Positions**:\n",
      "- **S1**: First mention of subject (position 2: \"Carl\")\n",
      "- **S2**: Second mention of subject (position 9: \"Carl\")\n",
      "- **IO**: Indirect object (position 4: \"Maria\")\n",
      "- **END**: Final position (position 13: \"to\")\n",
      "\n",
      "**Correct Answer**: Maria (the indirect object)\n",
      "\n",
      "### Metadata Structure\n",
      "Each example contains:\n",
      "- `subject` (S): The repeated name (e.g., \"Carl\")\n",
      "- `indirect_object` (IO): The other name (e.g., \"Maria\")\n",
      "- `object`: The item being given (e.g., \"fridge\")\n",
      "- `place`: The location (e.g., \"consulate\")\n",
      "\n",
      "### Sample Examples\n",
      "\n",
      "**Example 1**:\n",
      "- Prompt: \"As Carl and Maria left the consulate, Carl gave a fridge to\"\n",
      "- Choices: ['Maria', 'Carl']\n",
      "- Correct: Maria (index 0)\n",
      "\n",
      "**Example 2**:\n",
      "- Prompt: \"After Kevin and Bob spent some time at the racecourse, Kevin offered a duster to\"\n",
      "- Choices: ['Bob', 'Kevin']\n",
      "- Correct: Bob (index 0)\n",
      "\n",
      "**Example 3**:\n",
      "- Prompt: \"After Brian and Matt spent some time at the vet, Brian offered a button to\"\n",
      "- Choices: ['Matt', 'Brian']\n",
      "- Correct: Matt (index 0)\n",
      "\n",
      "## 3. Met\n",
      "\n",
      "... [Total length: 9271 characters]\n"
     ]
    }
   ],
   "source": [
    "# Read the documentation (student-facing)\n",
    "documentation_path = repo_path / 'logs' / 'documentation.md'\n",
    "with open(documentation_path, 'r') as f:\n",
    "    documentation = f.read()\n",
    "\n",
    "print(\"DOCUMENTATION (Student-facing):\")\n",
    "print(\"=\" * 80)\n",
    "print(documentation[:2000])  # Preview first 2000 chars\n",
    "print(f\"\\n... [Total length: {len(documentation)} characters]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4620226d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL DOCUMENTATION:\n",
      "================================================================================\n",
      "# IOI Circuit Analysis - Documentation\n",
      "\n",
      "## 1. Goal\n",
      "\n",
      "### Research Objective\n",
      "Identify a precise circuit in GPT2-small that implements the **Indirect Object Identification (IOI)** behavior while adhering to strict residual write-budget constraints (≤ 11,200 dimensions).\n",
      "\n",
      "### Hypothesis\n",
      "The IOI circuit comprises three functional components:\n",
      "\n",
      "1. **Duplicate Token Heads**: Active at S2, attending to S1, signaling token duplication through position features\n",
      "2. **S-Inhibition Heads**: Active at END, attending to S2, inhibiting Name-Mover attention to subject positions\n",
      "3. **Name-Mover Heads**: Active at END, attending to IO position, copying the indirect object token to the residual stream\n",
      "\n",
      "## 2. Data\n",
      "\n",
      "### Dataset\n",
      "- **Source**: mib-bench/ioi (Hugging Face)\n",
      "- **Size**: 10,000 examples (100 used for analysis)\n",
      "- **Task**: Predict the indirect object at the end of a sentence\n",
      "\n",
      "### Example Sentence Structure\n",
      "```\n",
      "\"As Carl and Maria left the consulate, Carl gave a fridge to ___\"\n",
      "```\n",
      "\n",
      "**Key Positions**:\n",
      "- **S1**: First mention of subject (position 2: \"Carl\")\n",
      "- **S2**: Second mention of subject (position 9: \"Carl\")\n",
      "- **IO**: Indirect object (position 4: \"Maria\")\n",
      "- **END**: Final position (position 13: \"to\")\n",
      "\n",
      "**Correct Answer**: Maria (the indirect object)\n",
      "\n",
      "### Metadata Structure\n",
      "Each example contains:\n",
      "- `subject` (S): The repeated name (e.g., \"Carl\")\n",
      "- `indirect_object` (IO): The other name (e.g., \"Maria\")\n",
      "- `object`: The item being given (e.g., \"fridge\")\n",
      "- `place`: The location (e.g., \"consulate\")\n",
      "\n",
      "### Sample Examples\n",
      "\n",
      "**Example 1**:\n",
      "- Prompt: \"As Carl and Maria left the consulate, Carl gave a fridge to\"\n",
      "- Choices: ['Maria', 'Carl']\n",
      "- Correct: Maria (index 0)\n",
      "\n",
      "**Example 2**:\n",
      "- Prompt: \"After Kevin and Bob spent some time at the racecourse, Kevin offered a duster to\"\n",
      "- Choices: ['Bob', 'Kevin']\n",
      "- Correct: Bob (index 0)\n",
      "\n",
      "**Example 3**:\n",
      "- Prompt: \"After Brian and Matt spent some time at the vet, Brian offered a button to\"\n",
      "- Choices: ['Matt', 'Brian']\n",
      "- Correct: Matt (index 0)\n",
      "\n",
      "## 3. Method\n",
      "\n",
      "### 3.1 Model Configuration\n",
      "- **Model**: GPT2-small via TransformerLens\n",
      "- **Device**: CUDA (NVIDIA A100 80GB PCIe)\n",
      "- **Architecture**:\n",
      "  - Layers: 12\n",
      "  - Heads per layer: 12\n",
      "  - d_model: 768\n",
      "  - d_head: 64\n",
      "  - d_mlp: 3,072\n",
      "\n",
      "### 3.2 Write Budget Constraints\n",
      "- Each attention head writes: 64 dimensions (d_model / n_heads)\n",
      "- Each MLP writes: 768 dimensions (d_model)\n",
      "- **Total budget**: ≤ 11,200 dimensions\n",
      "\n",
      "### 3.3 Analysis Pipeline\n",
      "\n",
      "#### Step 1: Baseline Evaluation\n",
      "- Tokenized 100 IOI examples\n",
      "- Ran model with activation caching\n",
      "- Evaluated baseline accuracy by comparing logits for IO vs. S tokens\n",
      "- **Result**: 94.00% accuracy (94/100 correct)\n",
      "\n",
      "#### Step 2: Attention Pattern Analysis\n",
      "\n",
      "**Duplicate Token Heads** (S2 → S1 attention):\n",
      "- For each example, identified S1 and S2 positions\n",
      "- Calculated attention weight from S2 to S1 for each head\n",
      "- Averaged across all examples\n",
      "- Top 5 heads:\n",
      "  1. a3.h0: 0.7191\n",
      "  2. a1.h11: 0.6613\n",
      "  3. a0.h5: 0.6080\n",
      "  4. a0.h1: 0.5152\n",
      "  5. a0.h10: 0.2359\n",
      "\n",
      "**S-Inhibition Heads** (END → S2 attention):\n",
      "- Calculated attention weight from END to S2 for each head\n",
      "- Averaged across examples\n",
      "- Top 5 heads:\n",
      "  1. a8.h6: 0.7441\n",
      "  2. a7.h9: 0.5079\n",
      "  3. a8.h10: 0.3037\n",
      "  4. a8.h5: 0.2852\n",
      "  5. a9.h7: 0.2557\n",
      "\n",
      "**Name-Mover Heads** (END → IO attention):\n",
      "- Identified IO position in each example\n",
      "- Calculated attention weight from END to IO\n",
      "- Averaged across examples\n",
      "- Top 5 heads:\n",
      "  1. a9.h9: 0.7998\n",
      "  2. a10.h7: 0.7829\n",
      "  3. a9.h6: 0.7412\n",
      "  4. a11.h10: 0.6369\n",
      "  5. a10.h0: 0.3877\n",
      "\n",
      "#### Step 3: Circuit Node Selection\n",
      "\n",
      "**Strategy**:\n",
      "1. Started with top heads from each category (10 heads total)\n",
      "2. Included all 12 MLPs for feature extraction and transformation\n",
      "3. Calculated remaining budget: 11,200 - (10×64 + 12×768) = 1,344 dims\n",
      "4. Added 21 additional high-scoring heads to maximize circuit expressiveness\n",
      "5. Achieved exact budget utilization: 11,200 dimensions\n",
      "\n",
      "**Final Selection**:\n",
      "- **31 attention heads** (1,984 dimensions)\n",
      "- **12 MLPs** (9,216 dimensions)\n",
      "- **Total**: 11,200 dimensions (100.0% budget utilization)\n",
      "\n",
      "## 4. Results\n",
      "\n",
      "### Final Circuit Composition\n",
      "\n",
      "**Total Nodes**: 44\n",
      "- 1 input node\n",
      "- 31 attention heads\n",
      "- 12 MLPs\n",
      "\n",
      "**Attention Head Breakdown by Function**:\n",
      "- Duplicate Token Heads: 6 heads\n",
      "- S-Inhibition Heads: 12 heads\n",
      "- Name-Mover Heads: 15 heads\n",
      "\n",
      "**Layer Distribution**:\n",
      "- Layer 0: 4 heads\n",
      "- Layer 1: 1 head\n",
      "- Layer 3: 2 heads\n",
      "- Layer 6: 1 head\n",
      "- Layer 7: 2 heads\n",
      "- Layer 8: 5 heads\n",
      "- Layer 9: 5 heads\n",
      "- Layer 10: 7 heads\n",
      "- Layer 11: 4 heads\n",
      "\n",
      "**Selected Nodes** (partial list):\n",
      "```json\n",
      "{\n",
      "  \"nodes\": [\n",
      "    \"input\",\n",
      "    \"a0.h1\", \"a0.h10\", \"a0.h5\", \"a0.h6\",\n",
      "    \"a1.h11\",\n",
      "    \"a10.h0\", \"a10.h1\", \"a10.h10\", \"a10.h2\", \"a10.h3\", \"a10.h6\", \"a10.h7\",\n",
      "    \"a11.h10\", \"a11.h6\", \"a11.h8\",\n",
      "    \"a3.h0\", \"a3.h6\",\n",
      "    \"a6.h0\",\n",
      "    \"a7.h3\", \"a7.h9\",\n",
      "    \"a8.h10\", \"a8.h2\", \"a8.h3\", \"a8.h5\", \"a8.h6\",\n",
      "    \"a9.h0\", \"a9.h2\", \"a9.h6\", \"a9.h7\", \"a9.h8\", \"a9.h9\",\n",
      "    \"m0\", \"m1\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"m7\", \"m8\", \"m9\", \"m10\", \"m11\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "### Budget Verification\n",
      "\n",
      "✓ All nodes are valid (in src_nodes)\n",
      "✓ All node names follow correct convention\n",
      "✓ Within budget constraint\n",
      "\n",
      "| Component | Count | Dims/Unit | Total Dims |\n",
      "|-----------|-------|-----------|------------|\n",
      "| Attention Heads | 31 | 64 | 1,984 |\n",
      "| MLPs | 12 | 768 | 9,216 |\n",
      "| **Total** | **44** | - | **11,200** |\n",
      "| Budget Limit | - | - | 11,200 |\n",
      "| **Utilization** | - | - | **100.0%** |\n",
      "\n",
      "### Performance Metrics\n",
      "\n",
      "- **Baseline Model Accuracy**: 94.00% (94/100 examples)\n",
      "- **Sample Size**: 100 examples from training set\n",
      "- **All constraint validations passed**: ✓\n",
      "\n",
      "## 5. Analysis\n",
      "\n",
      "### Support for Hypothesis\n",
      "\n",
      "The analysis **strongly supports** the three-component IOI hypothesis:\n",
      "\n",
      "1. **Duplicate Token Heads Identified**: \n",
      "   - Found 6 heads with strong S2→S1 attention (e.g., a3.h0 with 0.72 avg attention)\n",
      "   - These heads are predominantly in early-to-middle layers (0, 1, 3), consistent with positional feature detection\n",
      "\n",
      "2. **S-Inhibition Heads Identified**:\n",
      "   - Found 12 heads with strong END→S2 attention (e.g., a8.h6 with 0.74 avg attention)\n",
      "   - These heads are in middle-to-late layers (7, 8, 9), appropriate for suppressing subject interference\n",
      "\n",
      "3. **Name-Mover Heads Identified**:\n",
      "   - Found 15 heads with strong END→IO attention (e.g., a9.h9 with 0.80 avg attention)\n",
      "   - These heads are concentrated in late layers (9, 10, 11), ideal for final token prediction\n",
      "\n",
      "### Key Observations\n",
      "\n",
      "1. **Layered Processing**: The circuit exhibits clear stratification:\n",
      "   - Early layers (0-3): Duplicate token detection\n",
      "   - Middle layers (7-8): Subject inhibition\n",
      "   - Late layers (9-11): Name moving and prediction\n",
      "\n",
      "2. **High Selectivity**: Top heads show very strong attention patterns (>0.7) to their hypothesized targets, indicating specialized functionality\n",
      "\n",
      "3. **Efficient Budget Usage**: By including all MLPs and strategically selecting heads, we achieved 100% budget utilization while covering all three functional categories\n",
      "\n",
      "4. **Redundancy**: Multiple heads per category suggest robustness through redundancy, which is common in neural networks\n",
      "\n",
      "## 6. Next Steps\n",
      "\n",
      "### Potential Extensions\n",
      "\n",
      "1. **Ablation Studies**: Remove individual heads or head categories to measure performance impact\n",
      "2. **Larger Sample Analysis**: Run on full 10,000-example dataset to validate patterns\n",
      "3. **Activation Patching**: Directly test causal role of each component\n",
      "4. **Cross-Dataset Validation**: Test if identified heads generalize to other name-based tasks\n",
      "5. **Circuit Refinement**: Use causal intervention to identify minimal sufficient circuit\n",
      "\n",
      "### Alternative Hypotheses to Explore\n",
      "\n",
      "1. **Negative Name Movers**: Heads that move S tokens to suppress them\n",
      "2. **Backup Pathways**: Alternative circuits that activate when primary circuit is ablated\n",
      "3. **MLP Specialization**: Investigate which MLPs support specific head types\n",
      "4. **Position-Dependent Behavior**: Analyze if heads behave differently based on sentence length\n",
      "\n",
      "## 7. Main Takeaways\n",
      "\n",
      "1. **IOI Circuit Successfully Identified**: We identified 44 nodes (31 heads + 12 MLPs + input) that align with the three-component hypothesis within exact budget constraints\n",
      "\n",
      "2. **Clear Functional Specialization**: Attention heads show strong evidence of specialized roles in duplicate token detection, subject inhibition, and name moving\n",
      "\n",
      "3. **Layer Hierarchy Matters**: The circuit exhibits clear layered processing, with early layers detecting patterns, middle layers inhibiting interference, and late layers performing final prediction\n",
      "\n",
      "4. **High Baseline Performance**: GPT2-small achieves 94% accuracy on IOI, indicating strong learned behavior for this task\n",
      "\n",
      "5. **Efficient Representation**: The circuit uses only 11,200 of 110,592 possible dimensions (10.1% of total model capacity), suggesting IOI is implemented by a relatively sparse subcircuit\n",
      "\n",
      "6. **Hypothesis Validated**: The three-component circuit proposal (Duplicate Token → S-Inhibition → Name-Mover) is empirically supported by attention pattern analysis\n",
      "\n",
      "7. **Methodology Generalizable**: This attention-pattern-based circuit discovery approach can be applied to other interpretability tasks and model behaviors\n",
      "\n",
      "---\n",
      "\n",
      "**Experiment Completed**: 2025-11-09  \n",
      "**Model**: GPT2-small  \n",
      "**Dataset**: mib-bench/ioi  \n",
      "**Status**: All deliverables completed ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the complete documentation\n",
    "print(\"FULL DOCUMENTATION:\")\n",
    "print(\"=\" * 80)\n",
    "print(documentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba893b2d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN:\n",
      "================================================================================\n",
      "# IOI Circuit Analysis - Research Plan\n",
      "\n",
      "## Objective\n",
      "\n",
      "Identify a precise circuit in GPT2-small that implements the Indirect Object Identification (IOI) task while staying within a write budget of 11,200 dimensions.\n",
      "\n",
      "## Hypothesis\n",
      "\n",
      "The IOI circuit consists of three main components:\n",
      "\n",
      "1. **Duplicate Token Heads**: Attend from S2 to S1, signaling token duplication\n",
      "2. **S-Inhibition Heads**: Attend from END to S2, inhibiting attention to the subject\n",
      "3. **Name-Mover Heads**: Attend from END to IO, copying the indirect object to the output\n",
      "\n",
      "## Methodology\n",
      "\n",
      "### Phase 1: Data Exploration\n",
      "- Load GPT2-small model via TransformerLens\n",
      "- Load mib-bench/ioi dataset\n",
      "- Analyze dataset structure and identify key positions (S1, S2, IO, END)\n",
      "- Establish baseline model performance on IOI task\n",
      "\n",
      "### Phase 2: Attention Pattern Analysis\n",
      "- Run model with activation caching\n",
      "- For each layer and head, calculate attention patterns:\n",
      "  - **Duplicate Token Heads**: Measure attention from S2 → S1\n",
      "  - **S-Inhibition Heads**: Measure attention from END → S2\n",
      "  - **Name-Mover Heads**: Measure attention from END → IO\n",
      "- Rank heads by their alignment with hypothesized behavior\n",
      "\n",
      "### Phase 3: Circuit Selection\n",
      "- Select top-k heads from each category\n",
      "- Include supporting MLPs from relevant layers\n",
      "- Ensure total write budget ≤ 11,200 dimensions\n",
      "  - Each head writes: d_model / n_heads = 768 / 12 = 64 dims\n",
      "  - Each MLP writes: d_model = 768 dims\n",
      "\n",
      "### Phase 4: Validation\n",
      "- Verify all nodes are in allowed src_nodes\n",
      "- Verify naming conventions (a{layer}.h{head}, m{layer})\n",
      "- Verify budget constraints\n",
      "- Document circuit composition and performance\n",
      "\n",
      "## Expected Outcomes\n",
      "\n",
      "- A JSON file (real_circuits_1.json) containing the selected circuit nodes\n",
      "- Comprehensive documentation of findings\n",
      "- Validation that the circuit stays within budget\n",
      "- Analysis of whether findings support the IOI hypothesis\n",
      "\n",
      "## Success Criteria\n",
      "\n",
      "1. Circuit contains ≤ 11,200 dimensional writes\n",
      "2. All nodes follow naming conventions\n",
      "3. Circuit includes representatives from all three hypothesized head types\n",
      "4. Documentation clearly explains methodology and results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the plan\n",
    "plan_path = repo_path / 'logs' / 'plan.md'\n",
    "with open(plan_path, 'r') as f:\n",
    "    plan = f.read()\n",
    "\n",
    "print(\"PLAN:\")\n",
    "print(\"=\" * 80)\n",
    "print(plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed69227",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE WALKTHROUGH (first 3000 chars):\n",
      "================================================================================\n",
      "# IOI Circuit Analysis - Code Walkthrough\n",
      "\n",
      "## Overview\n",
      "\n",
      "This document provides a detailed walkthrough of the implementation used to identify the IOI circuit in GPT2-small.\n",
      "\n",
      "## Setup and Initialization\n",
      "\n",
      "### 1. Environment Configuration\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.chdir('/home/smallyan/critic_model_mechinterp')\n",
      "\n",
      "# Check for GPU availability\n",
      "import torch\n",
      "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
      "```\n",
      "\n",
      "**Purpose**: Set working directory and configure compute device. Using CUDA significantly speeds up model inference and activation caching.\n",
      "\n",
      "### 2. Load Model\n",
      "\n",
      "```python\n",
      "from transformer_lens import HookedTransformer\n",
      "\n",
      "model = HookedTransformer.from_pretrained('gpt2-small', device=device)\n",
      "```\n",
      "\n",
      "**Key Configuration**:\n",
      "- `n_layers`: 12\n",
      "- `n_heads`: 12  \n",
      "- `d_model`: 768\n",
      "- `d_head`: 64\n",
      "\n",
      "**Why TransformerLens**: Provides easy access to activation caching and intervention capabilities needed for mechanistic interpretability.\n",
      "\n",
      "### 3. Load Dataset\n",
      "\n",
      "```python\n",
      "from datasets import load_dataset\n",
      "\n",
      "dataset = load_dataset(\"mib-bench/ioi\")\n",
      "ioi_data = dataset['train']\n",
      "```\n",
      "\n",
      "**Dataset Structure**:\n",
      "- Each example has a `prompt`, `choices`, `answerKey`, and `metadata`\n",
      "- Metadata includes `subject`, `indirect_object`, `object`, and `place`\n",
      "- Multiple counterfactual variants provided for each example\n",
      "\n",
      "## Data Processing\n",
      "\n",
      "### 4. Position Identification\n",
      "\n",
      "```python\n",
      "def find_positions(prompt_idx):\n",
      "    tokens_str = model.to_str_tokens(prompts[prompt_idx])\n",
      "    s_name = s_names[prompt_idx]\n",
      "    \n",
      "    s1_pos = None\n",
      "    s2_pos = None\n",
      "    end_pos = len(tokens_str) - 1\n",
      "    \n",
      "    for i, token in enumerate(tokens_str):\n",
      "        if s_name in token:\n",
      "            if s1_pos is None:\n",
      "                s1_pos = i\n",
      "            else:\n",
      "                s2_pos = i\n",
      "                break\n",
      "    \n",
      "    return s1_pos, s2_pos, end_pos, tokens_str\n",
      "```\n",
      "\n",
      "**Purpose**: Locate critical positions in each sentence:\n",
      "- **S1**: First subject mention (typically position 2)\n",
      "- **S2**: Second subject mention (varies by sentence structure)\n",
      "- **END**: Last token position (where model predicts next token)\n",
      "\n",
      "**Challenge**: Names are tokenized differently, so we search for name substrings in tokens rather than exact matches.\n",
      "\n",
      "### 5. Baseline Evaluation\n",
      "\n",
      "```python\n",
      "logits, cache = model.run_with_cache(tokens)\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    _, _, end_pos, _ = find_positions(i)\n",
      "    end_logits = logits[i, end_pos, :]\n",
      "    \n",
      "    io_token = model.to_single_token(' ' + io_names[i])\n",
      "    s_token = model.to_single_token(' ' + s_names[i])\n",
      "    \n",
      "    predicted_io = end_logits[io_token] > end_logits[s_token]\n",
      "```\n",
      "\n",
      "**Purpose**: Measure model's baseline performance by comparing logits for IO vs. Subject tokens.\n",
      "\n",
      "**Result**: 94% accuracy demonstrates model has learned IOI behavior.\n",
      "\n",
      "## Attention Pattern Analysis\n",
      "\n",
      "### 6. Duplicate Token Head Detection\n",
      "\n",
      "```python\n",
      "duplicate_token_scores = np.zeros((n_layers, n_heads))\n",
      "\n",
      "for i in range(len(prompts)):\n",
      "    s1_pos, s2_pos, _, _ = find_positions(i)\n",
      "    \n",
      "    for layer in range(n_layers):\n",
      "        attn_patter\n",
      "\n",
      "... [Total length: 10259 characters]\n"
     ]
    }
   ],
   "source": [
    "# Read the code walkthrough\n",
    "code_walk_path = repo_path / 'logs' / 'code_walk.md'\n",
    "with open(code_walk_path, 'r') as f:\n",
    "    code_walk = f.read()\n",
    "\n",
    "print(\"CODE WALKTHROUGH (first 3000 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(code_walk[:3000])\n",
    "print(f\"\\n... [Total length: {len(code_walk)} characters]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c457c0ff",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation notebook exists: True\n",
      "\n",
      "Circuit Results JSON:\n",
      "{\n",
      "  \"nodes\": [\n",
      "    \"input\",\n",
      "    \"a0.h1\",\n",
      "    \"a0.h10\",\n",
      "    \"a0.h5\",\n",
      "    \"a0.h6\",\n",
      "    \"a1.h11\",\n",
      "    \"a10.h0\",\n",
      "    \"a10.h1\",\n",
      "    \"a10.h10\",\n",
      "    \"a10.h2\",\n",
      "    \"a10.h3\",\n",
      "    \"a10.h6\",\n",
      "    \"a10.h7\",\n",
      "    \"a11.h10\",\n",
      "    \"a11.h6\",\n",
      "    \"a11.h8\",\n",
      "    \"a3.h0\",\n",
      "    \"a3.h6\",\n",
      "    \"a6.h0\",\n",
      "    \"a7.h3\",\n",
      "    \"a7.h9\",\n",
      "    \"a8.h10\",\n",
      "    \"a8.h2\",\n",
      "    \"a8.h3\",\n",
      "    \"a8.h5\",\n",
      "    \"a8.h6\",\n",
      "    \"a9.h0\",\n",
      "    \"a9.h2\",\n",
      "    \"a9.h6\",\n",
      "    \"a9.h7\",\n",
      "    \"a9.h8\",\n",
      "    \"a9.h9\",\n",
      "    \"m0\",\n",
      "    \"m1\",\n",
      "    \"m10\",\n",
      "    \"m11\",\n",
      "    \"m2\",\n",
      "    \"m3\",\n",
      "    \"m4\",\n",
      "    \"m5\",\n",
      "    \"m6\",\n",
      "    \"m7\",\n",
      "    \"m8\",\n",
      "    \"m9\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read the actual implementation notebook\n",
    "notebook_path = repo_path / 'notebooks' / '2025-11-09-14-46_IOI_Circuit_Analysis.ipynb'\n",
    "\n",
    "# Since it's a notebook, let's just check if it exists\n",
    "print(f\"Implementation notebook exists: {notebook_path.exists()}\")\n",
    "\n",
    "# Also check the results JSON\n",
    "results_json_path = repo_path / 'results' / 'real_circuits_1.json'\n",
    "with open(results_json_path, 'r') as f:\n",
    "    circuit_results = json.load(f)\n",
    "\n",
    "print(\"\\nCircuit Results JSON:\")\n",
    "print(json.dumps(circuit_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986fd4c",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Exam Design\n",
    "\n",
    "## Key Knowledge Points Extracted from Documentation\n",
    "\n",
    "Based on thorough analysis of the documentation, here are the central concepts students must understand:\n",
    "\n",
    "### 1. **Task Definition & Setup**\n",
    "- IOI (Indirect Object Identification) task definition\n",
    "- Sentence structure: S1, S2, IO, END positions\n",
    "- Dataset source and size (mib-bench/ioi)\n",
    "- Example sentence patterns\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "- GPT2-small specifications (12 layers, 12 heads, d_model=768, d_head=64)\n",
    "- TransformerLens framework usage\n",
    "- Dimensional constraints per component\n",
    "\n",
    "### 3. **Research Hypothesis**\n",
    "- Three-component circuit theory:\n",
    "  - Duplicate Token Heads (S2→S1)\n",
    "  - S-Inhibition Heads (END→S2)\n",
    "  - Name-Mover Heads (END→IO)\n",
    "- Layer hierarchy and processing flow\n",
    "\n",
    "### 4. **Budget Constraints**\n",
    "- Write budget: ≤11,200 dimensions\n",
    "- Attention head contribution: 64 dims each\n",
    "- MLP contribution: 768 dims each\n",
    "- Budget utilization strategy\n",
    "\n",
    "### 5. **Analysis Methodology**\n",
    "- Baseline evaluation approach\n",
    "- Attention pattern analysis for each head type\n",
    "- Top-k selection strategy\n",
    "- Validation criteria\n",
    "\n",
    "### 6. **Results & Findings**\n",
    "- Final circuit composition (31 heads + 12 MLPs)\n",
    "- Layer distribution of functional components\n",
    "- Performance metrics (94% baseline accuracy)\n",
    "- Budget utilization (100%)\n",
    "\n",
    "### 7. **Interpretability Insights**\n",
    "- Layered processing patterns\n",
    "- Functional specialization evidence\n",
    "- Redundancy in circuit design\n",
    "- Sparse subcircuit representation\n",
    "\n",
    "### 8. **Generalizable Concepts**\n",
    "- Mechanistic interpretability methodology\n",
    "- Attention-pattern-based circuit discovery\n",
    "- Causal intervention techniques (next steps)\n",
    "- Circuit validation approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6c16ed",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 questions so far...\n"
     ]
    }
   ],
   "source": [
    "# Design comprehensive exam questions\n",
    "# Mix of factual recall and applied reasoning\n",
    "\n",
    "exam_questions = []\n",
    "\n",
    "# ========== SECTION 1: Task Understanding & Setup ==========\n",
    "\n",
    "# Q1 - Factual recall\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"In the IOI task, what does the model need to predict at the END position?\",\n",
    "    \"choices\": [\n",
    "        \"A) The subject (S) who performs the action\",\n",
    "        \"B) The indirect object (IO) who receives the item\",\n",
    "        \"C) The object being given\",\n",
    "        \"D) The location where the action takes place\"\n",
    "    ],\n",
    "    \"answer\": \"B) The indirect object (IO) who receives the item\",\n",
    "    \"reference\": \"Section 2 (Data) - The task is to predict the indirect object at the end of a sentence. Example: 'As Carl and Maria left the consulate, Carl gave a fridge to ___' → Answer: Maria\"\n",
    "})\n",
    "\n",
    "# Q2 - Applied reasoning\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Consider a new sentence: 'When Alice and Bob arrived at the museum, Alice handed a notebook to ___'. Identify the S1, S2, IO, and END positions. Explain what makes this sentence follow the IOI pattern.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"S1 would be at the first mention of 'Alice' (position varies by tokenization), S2 would be at the second mention of 'Alice', IO would be at 'Bob', and END would be at 'to' (the final token). This follows the IOI pattern because: (1) it has a repeated subject name (Alice), (2) it has a distinct indirect object name (Bob), (3) the subject appears first, then both names appear together, then the subject repeats before performing an action directed at the IO, and (4) the model must predict the IO name at the end.\",\n",
    "    \"reference\": \"Section 2 (Data) - Key Positions and Example Sentence Structure\"\n",
    "})\n",
    "\n",
    "# Q3 - Multiple choice - comprehension\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"What is the primary dataset used for this IOI circuit analysis?\",\n",
    "    \"choices\": [\n",
    "        \"A) GPT-2 training corpus\",\n",
    "        \"B) mib-bench/ioi from Hugging Face\",\n",
    "        \"C) Custom synthetic IOI dataset\",\n",
    "        \"D) Common Crawl subset\"\n",
    "    ],\n",
    "    \"answer\": \"B) mib-bench/ioi from Hugging Face\",\n",
    "    \"reference\": \"Section 2 (Data) - Dataset subsection clearly states Source: mib-bench/ioi (Hugging Face), Size: 10,000 examples\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 2: Hypothesis & Circuit Components ==========\n",
    "\n",
    "# Q4 - Factual + reasoning\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The hypothesis proposes three types of attention heads. For each type, describe: (a) what positions it attends between, and (b) what functional role it plays in solving the IOI task.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"(1) Duplicate Token Heads: Attend from S2 to S1, detecting that the same name appears twice to signal token duplication through position features. (2) S-Inhibition Heads: Attend from END to S2, inhibiting the Name-Mover heads from attending to the subject position, preventing the model from incorrectly predicting the subject. (3) Name-Mover Heads: Attend from END to IO, copying the indirect object token to the residual stream so it can be predicted at the output.\",\n",
    "    \"reference\": \"Section 1 (Goal) - Hypothesis, and Section 3.2 (Attention Pattern Analysis)\"\n",
    "})\n",
    "\n",
    "# Q5 - Mechanistic reasoning\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Why is the S-Inhibition mechanism necessary for the IOI circuit?\",\n",
    "    \"choices\": [\n",
    "        \"A) To amplify the signal from the indirect object\",\n",
    "        \"B) To prevent the Name-Mover heads from incorrectly attending to and copying the subject name\",\n",
    "        \"C) To detect duplicate tokens in the input sequence\",\n",
    "        \"D) To increase the overall model accuracy\"\n",
    "    ],\n",
    "    \"answer\": \"B) To prevent the Name-Mover heads from incorrectly attending to and copying the subject name\",\n",
    "    \"reference\": \"Section 1 (Goal) - Hypothesis describes S-Inhibition Heads as 'inhibiting Name-Mover attention to subject positions'\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 3: Budget Constraints & Architecture ==========\n",
    "\n",
    "# Q6 - Calculation/Applied\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"If you select 25 attention heads and 10 MLPs for your circuit, how many dimensions would this consume? Show your calculation and state whether this fits within the budget constraint.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Attention heads: 25 heads × 64 dims/head = 1,600 dims. MLPs: 10 MLPs × 768 dims/MLP = 7,680 dims. Total: 1,600 + 7,680 = 9,280 dimensions. This DOES fit within the budget constraint of ≤11,200 dimensions. Remaining budget: 11,200 - 9,280 = 1,920 dimensions.\",\n",
    "    \"reference\": \"Section 3.2 (Write Budget Constraints) - Each attention head writes 64 dimensions, each MLP writes 768 dimensions, total budget ≤11,200 dimensions\"\n",
    "})\n",
    "\n",
    "# Q7 - Factual recall\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"What is the dimensionality of each attention head's output in GPT2-small?\",\n",
    "    \"choices\": [\n",
    "        \"A) 768 dimensions (d_model)\",\n",
    "        \"B) 3,072 dimensions (d_mlp)\",\n",
    "        \"C) 64 dimensions (d_head)\",\n",
    "        \"D) 12 dimensions (n_heads)\"\n",
    "    ],\n",
    "    \"answer\": \"C) 64 dimensions (d_head)\",\n",
    "    \"reference\": \"Section 3.1 (Model Configuration) and Section 3.2 (Write Budget Constraints) - d_head = 64, calculated as d_model / n_heads = 768 / 12\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 4: Methodology & Analysis ==========\n",
    "\n",
    "# Q8 - Methodological understanding\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Describe the methodology used to identify 'Duplicate Token Heads'. What metric was calculated, and what threshold or selection criterion was used?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"To identify Duplicate Token Heads, the researchers calculated the attention weight from position S2 to position S1 for each attention head across all examples, then averaged these weights. Heads were ranked by their average S2→S1 attention score. The top 5 heads with highest scores were identified (e.g., a3.h0 with 0.7191 average attention). The selection criterion was based on ranking - choosing heads with the strongest attention patterns to the hypothesized target positions.\",\n",
    "    \"reference\": \"Section 3.3 (Analysis Pipeline) - Step 2: Attention Pattern Analysis, specifically the Duplicate Token Heads subsection\"\n",
    "})\n",
    "\n",
    "# Q9 - Critical analysis\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"The baseline model achieved 94% accuracy on the IOI task. What does this tell us about the model's behavior?\",\n",
    "    \"choices\": [\n",
    "        \"A) The model has perfectly learned the IOI task\",\n",
    "        \"B) The model has strong learned behavior for IOI but makes some errors\",\n",
    "        \"C) The model is randomly guessing between two options\",\n",
    "        \"D) The model's circuit identification is incorrect\"\n",
    "    ],\n",
    "    \"answer\": \"B) The model has strong learned behavior for IOI but makes some errors\",\n",
    "    \"reference\": \"Section 4 (Results) - Performance Metrics and Section 7 (Main Takeaways) point 4: 'High Baseline Performance: GPT2-small achieves 94% accuracy on IOI, indicating strong learned behavior for this task'\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 5: Results Interpretation ==========\n",
    "\n",
    "# Q10 - Pattern recognition\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation shows that Duplicate Token Heads are in layers 0-3, S-Inhibition Heads in layers 7-8, and Name-Mover Heads in layers 9-11. What computational principle does this layered organization suggest about how the circuit processes information?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"This layered organization suggests hierarchical or sequential processing: early layers detect low-level patterns (duplicate tokens), middle layers perform intermediate computations (inhibiting interference from the subject), and late layers perform final high-level operations (selecting and copying the correct answer). This reflects a compositional processing strategy where complex tasks are broken down into stages, with earlier layers extracting features that later layers use for decision-making.\",\n",
    "    \"reference\": \"Section 5 (Analysis) - Key Observations point 1: 'Layered Processing', and Section 7 (Main Takeaways) point 3: 'Layer Hierarchy Matters'\"\n",
    "})\n",
    "\n",
    "print(f\"Created {len(exam_questions)} questions so far...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f7cd0f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions created: 18\n"
     ]
    }
   ],
   "source": [
    "# Continue with more advanced questions\n",
    "\n",
    "# ========== SECTION 6: Circuit Composition & Validation ==========\n",
    "\n",
    "# Q11 - Calculation\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"The final circuit contains 31 attention heads and 12 MLPs. What is the total dimensional write budget consumed?\",\n",
    "    \"choices\": [\n",
    "        \"A) 10,200 dimensions\",\n",
    "        \"B) 11,200 dimensions\",\n",
    "        \"C) 12,000 dimensions\",\n",
    "        \"D) 9,984 dimensions\"\n",
    "    ],\n",
    "    \"answer\": \"B) 11,200 dimensions\",\n",
    "    \"reference\": \"Section 4 (Results) - Budget Verification table shows: 31 heads × 64 = 1,984 dims + 12 MLPs × 768 = 9,216 dims = 11,200 total dimensions\"\n",
    "})\n",
    "\n",
    "# Q12 - Hypothetical transfer\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Suppose you wanted to adapt this IOI circuit discovery methodology to identify circuits for a different task: detecting whether a pronoun refers to the first or second mentioned person in a sentence (pronoun resolution). What modifications would you make to the attention pattern analysis? Specifically, what new attention patterns would you measure?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"For pronoun resolution, you would measure: (1) Attention from the pronoun position to each candidate antecedent position (to identify which heads attend to potential referents), (2) Attention patterns that discriminate between first and second mentions based on recency or positional encoding, (3) Attention from the pronoun to syntactic markers (e.g., gender cues, number agreement) that constrain reference, (4) Potential inhibition patterns that suppress incorrect antecedents. The core methodology would remain similar—identify positions of interest, measure attention patterns, rank heads by alignment with hypothesized behavior—but the specific positions and patterns would change to match the pronoun resolution task structure.\",\n",
    "    \"reference\": \"Section 3.3 (Analysis Pipeline) - Attention Pattern Analysis methodology, and Section 7 (Main Takeaways) point 7 about generalizable methodology\"\n",
    "})\n",
    "\n",
    "# Q13 - Error detection\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"A student claims: 'The circuit uses 10.1% of the model's capacity, which means 89.9% of GPT2-small's parameters are unnecessary and could be removed.' Identify the flaw in this reasoning.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"The flaw is conflating task-specific capacity usage with overall model utility. The 10.1% figure refers to the dimensions used for the IOI task specifically, not the entire model. GPT2-small performs many different tasks beyond IOI (language modeling, various NLP tasks), and different circuits likely implement different capabilities. The remaining 89.9% is not 'unnecessary'—it's used for other linguistic computations. Additionally, there may be redundant or backup pathways, and some capacity may be used for feature extraction that supports multiple tasks. Removing 89.9% of the model would destroy its ability to perform other tasks.\",\n",
    "    \"reference\": \"Section 7 (Main Takeaways) point 5: 'The circuit uses only 11,200 of 110,592 possible dimensions (10.1% of total model capacity), suggesting IOI is implemented by a relatively sparse subcircuit' - this is about task-specific circuits, not model redundancy\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 7: Advanced Reasoning & Experimental Design ==========\n",
    "\n",
    "# Q14 - Experimental design\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation suggests 'Ablation Studies' as a next step to measure performance impact. Design a specific ablation experiment to test whether S-Inhibition Heads are causally necessary for the IOI circuit. What would you ablate, what would you measure, and what result would support their causal necessity?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Experimental design: (1) Ablation: Zero out or remove the outputs of all identified S-Inhibition Heads (e.g., a8.h6, a7.h9, etc.) while keeping all other circuit components active. (2) Measurement: Evaluate model accuracy on the IOI task - specifically, measure how often the model incorrectly predicts the subject (S) instead of the indirect object (IO). Also measure the attention patterns of Name-Mover Heads to see if they now attend more to S2 position. (3) Expected result supporting causal necessity: Accuracy should drop significantly (if these heads are necessary), and there should be an increase in incorrect S predictions. If Name-Mover Heads start attending more to S2 when S-Inhibition Heads are ablated, this would provide strong evidence that S-Inhibition Heads causally suppress subject attention.\",\n",
    "    \"reference\": \"Section 6 (Next Steps) - Potential Extensions point 1 (Ablation Studies) and the hypothesis about S-Inhibition function in Section 1\"\n",
    "})\n",
    "\n",
    "# Q15 - Comparative analysis\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Which attention head has the highest average attention score to its hypothesized target position?\",\n",
    "    \"choices\": [\n",
    "        \"A) a3.h0 (Duplicate Token Head) with 0.72 attention from S2 to S1\",\n",
    "        \"B) a8.h6 (S-Inhibition Head) with 0.74 attention from END to S2\",\n",
    "        \"C) a9.h9 (Name-Mover Head) with 0.80 attention from END to IO\",\n",
    "        \"D) All heads have approximately equal attention scores\"\n",
    "    ],\n",
    "    \"answer\": \"C) a9.h9 (Name-Mover Head) with 0.80 attention from END to IO\",\n",
    "    \"reference\": \"Section 3.3 (Analysis Pipeline) Step 2 - Name-Mover Heads subsection lists a9.h9 with 0.7998 (≈0.80) as the top head\"\n",
    "})\n",
    "\n",
    "# Q16 - Creative synthesis\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation mentions 'Backup Pathways' as an alternative hypothesis to explore. Propose a concrete experiment to test whether backup pathways exist in the IOI circuit. What would constitute evidence for backup pathways?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Experiment: Perform progressive ablation of Name-Mover Heads in order of their attention strength (starting with a9.h9, then a10.h7, etc.). After each ablation, measure task accuracy. Evidence for backup pathways would include: (1) Graceful degradation - accuracy decreases gradually rather than catastrophically when ablating individual heads, (2) Attention pattern shifts - when top Name-Mover Heads are ablated, other heads (not originally classified as Name-Movers) begin showing increased END→IO attention, (3) Maintained above-chance performance - even after ablating the top 3-5 Name-Mover Heads, the model still performs significantly better than random guessing (>50%), (4) Recovery through alternative routes - analysis shows information flow through different layers or MLPs when primary heads are removed. The presence of 15 Name-Mover Heads (redundancy) already suggests backup pathways may exist.\",\n",
    "    \"reference\": \"Section 6 (Next Steps) - Alternative Hypotheses point 2 (Backup Pathways), and Section 5 (Analysis) - Key Observations point 4 about redundancy\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 8: Technical Details & Limitations ==========\n",
    "\n",
    "# Q17 - Technical understanding\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"What is the purpose of using TransformerLens for this analysis?\",\n",
    "    \"choices\": [\n",
    "        \"A) It provides better model accuracy than standard implementations\",\n",
    "        \"B) It enables activation caching and easy access to internal model states\",\n",
    "        \"C) It reduces computational requirements for running GPT2\",\n",
    "        \"D) It automatically identifies circuits without manual analysis\"\n",
    "    ],\n",
    "    \"answer\": \"B) It enables activation caching and easy access to internal model states\",\n",
    "    \"reference\": \"Section 3.1 (Model Configuration) mentions using TransformerLens, and the code walkthrough would explain its utility for mechanistic interpretability through activation access\"\n",
    "})\n",
    "\n",
    "# Q18 - Limitation awareness\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The analysis used only 100 examples from a dataset of 10,000. What are two potential limitations of this choice, and what could be done to address them?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Limitation 1: Statistical reliability - With only 100 examples, attention pattern averages might not be representative of the full distribution. Some heads might show high attention by chance on this small sample. Solution: Analyze the full 10,000 examples or use statistical significance testing to validate findings. Limitation 2: Pattern variability - Different sentence structures or name tokenizations in the unused 9,900 examples might reveal additional important heads or different attention patterns. Solution: Analyze performance across different subsets and validate that identified heads are consistent across diverse examples. The documentation acknowledges this by suggesting 'Larger Sample Analysis' as a next step.\",\n",
    "    \"reference\": \"Section 2 (Data) - Size: 10,000 examples (100 used for analysis), and Section 6 (Next Steps) - Potential Extensions point 2 (Larger Sample Analysis)\"\n",
    "})\n",
    "\n",
    "print(f\"Total questions created: {len(exam_questions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f200be66",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions created: 25\n"
     ]
    }
   ],
   "source": [
    "# Add more diverse question types to reach comprehensive coverage\n",
    "\n",
    "# ========== SECTION 9: Conceptual Synthesis ==========\n",
    "\n",
    "# Q19 - Interpretation\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation states that top attention heads show 'very strong attention patterns (>0.7)' to their targets. Why is high attention selectivity evidence for 'specialized functionality' rather than just random correlation?\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"High attention selectivity (>0.7) is evidence for specialized functionality because: (1) These heads consistently attend to specific positions across different examples with varying sentence structures and names, showing they respond to the abstract pattern (e.g., 'second mention of subject') rather than surface features, (2) The attention is concentrated on functionally relevant positions (S1, S2, IO) rather than distributed randomly, suggesting the heads have learned task-relevant features, (3) Different head types show high attention to different position pairs, indicating functional differentiation rather than general attention, (4) The high values mean these heads are dedicating most of their attention capacity to these specific positions, implying this is their primary computational role. Random correlation would show more variable, lower, and less position-specific attention scores.\",\n",
    "    \"reference\": \"Section 5 (Analysis) - Key Observations point 2: 'High Selectivity: Top heads show very strong attention patterns (>0.7) to their hypothesized targets, indicating specialized functionality'\"\n",
    "})\n",
    "\n",
    "# Q20 - Reasoning about efficiency\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"Why did the researchers include all 12 MLPs in the circuit rather than selecting only the most relevant ones?\",\n",
    "    \"choices\": [\n",
    "        \"A) MLPs don't contribute meaningfully to the circuit, so their inclusion doesn't matter\",\n",
    "        \"B) The documentation doesn't explain this choice\",\n",
    "        \"C) Including all MLPs ensures comprehensive feature extraction and transformation while still fitting within the budget\",\n",
    "        \"D) MLPs were included by mistake\"\n",
    "    ],\n",
    "    \"answer\": \"C) Including all MLPs ensures comprehensive feature extraction and transformation while still fitting within the budget\",\n",
    "    \"reference\": \"Section 3.3 (Analysis Pipeline) Step 3 - Circuit Node Selection: 'Included all 12 MLPs for feature extraction and transformation' and Section 5 (Analysis) mentions this as part of 'Efficient Budget Usage'\"\n",
    "})\n",
    "\n",
    "# Q21 - Causal reasoning\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Consider this scenario: You ablate all Duplicate Token Heads but the model's accuracy remains at 93%. What would this result suggest about the role of Duplicate Token Heads in the circuit? Provide two possible interpretations.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Interpretation 1: Duplicate Token Heads may not be causally necessary for IOI performance. The circuit might rely more heavily on S-Inhibition and Name-Mover Heads, with positional information available through other means (e.g., positional embeddings, or MLPs processing position features). Interpretation 2: Backup pathways exist - other heads or MLPs compensate for the ablated Duplicate Token Heads by detecting the duplicate token pattern through alternative mechanisms. This would align with the 'Backup Pathways' hypothesis mentioned in Next Steps. Both interpretations highlight the difference between correlation (high attention patterns) and causation (necessary for task performance), which is why the documentation suggests ablation studies and activation patching as critical next steps to establish causal roles.\",\n",
    "    \"reference\": \"Section 6 (Next Steps) - points 1 (Ablation Studies) and point 2 under Alternative Hypotheses (Backup Pathways)\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 10: Generalization & Transfer ==========\n",
    "\n",
    "# Q22 - Transfer learning\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"The documentation suggests testing if identified heads 'generalize to other name-based tasks'. Describe a specific different task where you might expect the same Name-Mover Heads to be useful, and explain why.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"A suitable task would be 'Direct Object Identification' - predicting who receives an action in sentences like 'The teacher praised ___ and reprimanded John' (answer: Mary, if mentioned earlier). The same Name-Mover Heads might be useful because: (1) The core mechanism (attending to a specific name and copying it to the output) is similar, (2) The task requires distinguishing between two names and selecting the contextually appropriate one, (3) It involves similar positional reasoning about which name is the target of an action. However, you might need different S-Inhibition patterns since the logic of what to suppress differs. Testing this would involve measuring whether heads like a9.h9 and a10.h7 show high attention from the prediction position to the correct name in this new task.\",\n",
    "    \"reference\": \"Section 6 (Next Steps) - Potential Extensions point 4: 'Cross-Dataset Validation: Test if identified heads generalize to other name-based tasks'\"\n",
    "})\n",
    "\n",
    "# Q23 - Methodological critique\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"The circuit selection strategy involved 'adding 21 additional high-scoring heads to maximize circuit expressiveness'. What potential problem does this approach have?\",\n",
    "    \"choices\": [\n",
    "        \"A) It might include heads that don't contribute causally to IOI performance\",\n",
    "        \"B) It violates the budget constraint\",\n",
    "        \"C) It doesn't include enough MLPs\",\n",
    "        \"D) It focuses too much on early layers\"\n",
    "    ],\n",
    "    \"answer\": \"A) It might include heads that don't contribute causally to IOI performance\",\n",
    "    \"reference\": \"Section 3.3 Step 3 describes adding heads to 'maximize circuit expressiveness' and achieve budget utilization, but Section 6 (Next Steps) emphasizes the need for ablation studies and 'Circuit Refinement: Use causal intervention to identify minimal sufficient circuit', suggesting the current circuit may include non-causal components\"\n",
    "})\n",
    "\n",
    "# Q24 - Advanced application\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"free_generation\",\n",
    "    \"question\": \"Propose how 'Activation Patching' (mentioned in Next Steps) could be used to validate the hypothesis that a3.h0 is a Duplicate Token Head. Describe the patching procedure and the expected result if the hypothesis is correct.\",\n",
    "    \"choices\": None,\n",
    "    \"answer\": \"Activation Patching procedure: (1) Run the model on an IOI example and cache the activation of a3.h0, (2) Run the model on a 'counterfactual' example where the subject does NOT repeat (e.g., 'As Carl and Maria left...'), (3) During this second run, replace ('patch') the activation of a3.h0 with the cached activation from the original IOI example, (4) Measure whether this causes the model's behavior to change - specifically, does it make the model treat the non-repeated name as if it were repeated? Expected result if hypothesis is correct: The patched activation should cause downstream components (S-Inhibition Heads and Name-Mover Heads) to behave as if there IS a duplicate token, potentially causing the model to show IOI-like attention patterns even in the non-IOI sentence. This would demonstrate that a3.h0 causally contributes duplicate-token information to the circuit.\",\n",
    "    \"reference\": \"Section 6 (Next Steps) - Potential Extensions point 3: 'Activation Patching: Directly test causal role of each component' and the overall hypothesis about Duplicate Token Heads in Section 1\"\n",
    "})\n",
    "\n",
    "# ========== SECTION 11: Data & Examples ==========\n",
    "\n",
    "# Q25 - Data understanding\n",
    "exam_questions.append({\n",
    "    \"question_type\": \"multiple_choice\",\n",
    "    \"question\": \"According to the metadata structure, which of the following is NOT a field in each IOI example?\",\n",
    "    \"choices\": [\n",
    "        \"A) subject (S)\",\n",
    "        \"B) indirect_object (IO)\",\n",
    "        \"C) verb (the action performed)\",\n",
    "        \"D) place (the location)\"\n",
    "    ],\n",
    "    \"answer\": \"C) verb (the action performed)\",\n",
    "    \"reference\": \"Section 2 (Data) - Metadata Structure lists: subject (S), indirect_object (IO), object, and place. Verb is not mentioned as a metadata field.\"\n",
    "})\n",
    "\n",
    "print(f\"Total questions created: {len(exam_questions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb13482f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created exam directory at: /home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-09_14-46-37/exam\n",
      "\n",
      "Saved exam JSON to: /home/smallyan/critic_model_mechinterp/runs/circuits_claude_2025-11-09_14-46-37/exam/exam_ioi_circuit.json\n",
      "Total questions in exam: 25\n",
      "\n",
      "Exam Statistics:\n",
      "  Multiple Choice: 11 (44.0%)\n",
      "  Free Generation: 14 (56.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create the exam directory and save the JSON file\n",
    "import os\n",
    "\n",
    "exam_dir = repo_path / 'exam'\n",
    "exam_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created exam directory at: {exam_dir}\")\n",
    "\n",
    "# Save the exam questions as JSON\n",
    "exam_json_path = exam_dir / 'exam_ioi_circuit.json'\n",
    "\n",
    "with open(exam_json_path, 'w') as f:\n",
    "    json.dump(exam_questions, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved exam JSON to: {exam_json_path}\")\n",
    "print(f\"Total questions in exam: {len(exam_questions)}\")\n",
    "\n",
    "# Display exam statistics\n",
    "mc_count = sum(1 for q in exam_questions if q['question_type'] == 'multiple_choice')\n",
    "fg_count = sum(1 for q in exam_questions if q['question_type'] == 'free_generation')\n",
    "\n",
    "print(f\"\\nExam Statistics:\")\n",
    "print(f\"  Multiple Choice: {mc_count} ({mc_count/len(exam_questions)*100:.1f}%)\")\n",
    "print(f\"  Free Generation: {fg_count} ({fg_count/len(exam_questions)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77527c6a",
   "metadata": {},
   "source": [
    "# IOI Circuit Analysis - Comprehensive Exam\n",
    "\n",
    "This exam is designed to assess student understanding of the IOI Circuit Analysis research documentation.\n",
    "\n",
    "## Exam Overview\n",
    "\n",
    "**Total Questions**: 25\n",
    "- **Multiple Choice**: 11 questions (44%)\n",
    "- **Free Generation**: 14 questions (56%)\n",
    "\n",
    "**Coverage**:\n",
    "- Factual Understanding: ~40%\n",
    "- Applied Reasoning & Transfer: ~60%\n",
    "\n",
    "**Estimated Time**: 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Exam Structure\n",
    "\n",
    "The exam is organized into thematic sections:\n",
    "\n",
    "1. **Task Understanding & Setup** (Q1-Q3)\n",
    "2. **Hypothesis & Circuit Components** (Q4-Q5)\n",
    "3. **Budget Constraints & Architecture** (Q6-Q7)\n",
    "4. **Methodology & Analysis** (Q8-Q9)\n",
    "5. **Results Interpretation** (Q10-Q11)\n",
    "6. **Circuit Composition & Validation** (Q12-Q13)\n",
    "7. **Advanced Reasoning & Experimental Design** (Q14-Q16)\n",
    "8. **Technical Details & Limitations** (Q17-Q18)\n",
    "9. **Conceptual Synthesis** (Q19-Q21)\n",
    "10. **Generalization & Transfer** (Q22-Q24)\n",
    "11. **Data & Examples** (Q25)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86dad418",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "IOI CIRCUIT ANALYSIS - EXAM QUESTIONS AND GOLD ANSWERS\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 1 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "In the IOI task, what does the model need to predict at the END position?\n",
      "\n",
      "CHOICES:\n",
      "  A) The subject (S) who performs the action\n",
      "  B) The indirect object (IO) who receives the item\n",
      "  C) The object being given\n",
      "  D) The location where the action takes place\n",
      "\n",
      "ANSWER:\n",
      "B) The indirect object (IO) who receives the item\n",
      "\n",
      "REFERENCE:\n",
      "Section 2 (Data) - The task is to predict the indirect object at the end of a sentence. Example: 'As Carl and Maria left the consulate, Carl gave a fridge to ___' → Answer: Maria\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 2 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "Consider a new sentence: 'When Alice and Bob arrived at the museum, Alice handed a notebook to ___'. Identify the S1, S2, IO, and END positions. Explain what makes this sentence follow the IOI pattern.\n",
      "\n",
      "ANSWER:\n",
      "S1 would be at the first mention of 'Alice' (position varies by tokenization), S2 would be at the second mention of 'Alice', IO would be at 'Bob', and END would be at 'to' (the final token). This follows the IOI pattern because: (1) it has a repeated subject name (Alice), (2) it has a distinct indirect object name (Bob), (3) the subject appears first, then both names appear together, then the subject repeats before performing an action directed at the IO, and (4) the model must predict the IO name at the end.\n",
      "\n",
      "REFERENCE:\n",
      "Section 2 (Data) - Key Positions and Example Sentence Structure\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 3 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "What is the primary dataset used for this IOI circuit analysis?\n",
      "\n",
      "CHOICES:\n",
      "  A) GPT-2 training corpus\n",
      "  B) mib-bench/ioi from Hugging Face\n",
      "  C) Custom synthetic IOI dataset\n",
      "  D) Common Crawl subset\n",
      "\n",
      "ANSWER:\n",
      "B) mib-bench/ioi from Hugging Face\n",
      "\n",
      "REFERENCE:\n",
      "Section 2 (Data) - Dataset subsection clearly states Source: mib-bench/ioi (Hugging Face), Size: 10,000 examples\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 4 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The hypothesis proposes three types of attention heads. For each type, describe: (a) what positions it attends between, and (b) what functional role it plays in solving the IOI task.\n",
      "\n",
      "ANSWER:\n",
      "(1) Duplicate Token Heads: Attend from S2 to S1, detecting that the same name appears twice to signal token duplication through position features. (2) S-Inhibition Heads: Attend from END to S2, inhibiting the Name-Mover heads from attending to the subject position, preventing the model from incorrectly predicting the subject. (3) Name-Mover Heads: Attend from END to IO, copying the indirect object token to the residual stream so it can be predicted at the output.\n",
      "\n",
      "REFERENCE:\n",
      "Section 1 (Goal) - Hypothesis, and Section 3.2 (Attention Pattern Analysis)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 5 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "Why is the S-Inhibition mechanism necessary for the IOI circuit?\n",
      "\n",
      "CHOICES:\n",
      "  A) To amplify the signal from the indirect object\n",
      "  B) To prevent the Name-Mover heads from incorrectly attending to and copying the subject name\n",
      "  C) To detect duplicate tokens in the input sequence\n",
      "  D) To increase the overall model accuracy\n",
      "\n",
      "ANSWER:\n",
      "B) To prevent the Name-Mover heads from incorrectly attending to and copying the subject name\n",
      "\n",
      "REFERENCE:\n",
      "Section 1 (Goal) - Hypothesis describes S-Inhibition Heads as 'inhibiting Name-Mover attention to subject positions'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 6 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "If you select 25 attention heads and 10 MLPs for your circuit, how many dimensions would this consume? Show your calculation and state whether this fits within the budget constraint.\n",
      "\n",
      "ANSWER:\n",
      "Attention heads: 25 heads × 64 dims/head = 1,600 dims. MLPs: 10 MLPs × 768 dims/MLP = 7,680 dims. Total: 1,600 + 7,680 = 9,280 dimensions. This DOES fit within the budget constraint of ≤11,200 dimensions. Remaining budget: 11,200 - 9,280 = 1,920 dimensions.\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.2 (Write Budget Constraints) - Each attention head writes 64 dimensions, each MLP writes 768 dimensions, total budget ≤11,200 dimensions\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 7 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "What is the dimensionality of each attention head's output in GPT2-small?\n",
      "\n",
      "CHOICES:\n",
      "  A) 768 dimensions (d_model)\n",
      "  B) 3,072 dimensions (d_mlp)\n",
      "  C) 64 dimensions (d_head)\n",
      "  D) 12 dimensions (n_heads)\n",
      "\n",
      "ANSWER:\n",
      "C) 64 dimensions (d_head)\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.1 (Model Configuration) and Section 3.2 (Write Budget Constraints) - d_head = 64, calculated as d_model / n_heads = 768 / 12\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 8 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "Describe the methodology used to identify 'Duplicate Token Heads'. What metric was calculated, and what threshold or selection criterion was used?\n",
      "\n",
      "ANSWER:\n",
      "To identify Duplicate Token Heads, the researchers calculated the attention weight from position S2 to position S1 for each attention head across all examples, then averaged these weights. Heads were ranked by their average S2→S1 attention score. The top 5 heads with highest scores were identified (e.g., a3.h0 with 0.7191 average attention). The selection criterion was based on ranking - choosing heads with the strongest attention patterns to the hypothesized target positions.\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.3 (Analysis Pipeline) - Step 2: Attention Pattern Analysis, specifically the Duplicate Token Heads subsection\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 9 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "The baseline model achieved 94% accuracy on the IOI task. What does this tell us about the model's behavior?\n",
      "\n",
      "CHOICES:\n",
      "  A) The model has perfectly learned the IOI task\n",
      "  B) The model has strong learned behavior for IOI but makes some errors\n",
      "  C) The model is randomly guessing between two options\n",
      "  D) The model's circuit identification is incorrect\n",
      "\n",
      "ANSWER:\n",
      "B) The model has strong learned behavior for IOI but makes some errors\n",
      "\n",
      "REFERENCE:\n",
      "Section 4 (Results) - Performance Metrics and Section 7 (Main Takeaways) point 4: 'High Baseline Performance: GPT2-small achieves 94% accuracy on IOI, indicating strong learned behavior for this task'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 10 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The documentation shows that Duplicate Token Heads are in layers 0-3, S-Inhibition Heads in layers 7-8, and Name-Mover Heads in layers 9-11. What computational principle does this layered organization suggest about how the circuit processes information?\n",
      "\n",
      "ANSWER:\n",
      "This layered organization suggests hierarchical or sequential processing: early layers detect low-level patterns (duplicate tokens), middle layers perform intermediate computations (inhibiting interference from the subject), and late layers perform final high-level operations (selecting and copying the correct answer). This reflects a compositional processing strategy where complex tasks are broken down into stages, with earlier layers extracting features that later layers use for decision-making.\n",
      "\n",
      "REFERENCE:\n",
      "Section 5 (Analysis) - Key Observations point 1: 'Layered Processing', and Section 7 (Main Takeaways) point 3: 'Layer Hierarchy Matters'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 11 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "The final circuit contains 31 attention heads and 12 MLPs. What is the total dimensional write budget consumed?\n",
      "\n",
      "CHOICES:\n",
      "  A) 10,200 dimensions\n",
      "  B) 11,200 dimensions\n",
      "  C) 12,000 dimensions\n",
      "  D) 9,984 dimensions\n",
      "\n",
      "ANSWER:\n",
      "B) 11,200 dimensions\n",
      "\n",
      "REFERENCE:\n",
      "Section 4 (Results) - Budget Verification table shows: 31 heads × 64 = 1,984 dims + 12 MLPs × 768 = 9,216 dims = 11,200 total dimensions\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 12 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "Suppose you wanted to adapt this IOI circuit discovery methodology to identify circuits for a different task: detecting whether a pronoun refers to the first or second mentioned person in a sentence (pronoun resolution). What modifications would you make to the attention pattern analysis? Specifically, what new attention patterns would you measure?\n",
      "\n",
      "ANSWER:\n",
      "For pronoun resolution, you would measure: (1) Attention from the pronoun position to each candidate antecedent position (to identify which heads attend to potential referents), (2) Attention patterns that discriminate between first and second mentions based on recency or positional encoding, (3) Attention from the pronoun to syntactic markers (e.g., gender cues, number agreement) that constrain reference, (4) Potential inhibition patterns that suppress incorrect antecedents. The core methodology would remain similar—identify positions of interest, measure attention patterns, rank heads by alignment with hypothesized behavior—but the specific positions and patterns would change to match the pronoun resolution task structure.\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.3 (Analysis Pipeline) - Attention Pattern Analysis methodology, and Section 7 (Main Takeaways) point 7 about generalizable methodology\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 13 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "A student claims: 'The circuit uses 10.1% of the model's capacity, which means 89.9% of GPT2-small's parameters are unnecessary and could be removed.' Identify the flaw in this reasoning.\n",
      "\n",
      "ANSWER:\n",
      "The flaw is conflating task-specific capacity usage with overall model utility. The 10.1% figure refers to the dimensions used for the IOI task specifically, not the entire model. GPT2-small performs many different tasks beyond IOI (language modeling, various NLP tasks), and different circuits likely implement different capabilities. The remaining 89.9% is not 'unnecessary'—it's used for other linguistic computations. Additionally, there may be redundant or backup pathways, and some capacity may be used for feature extraction that supports multiple tasks. Removing 89.9% of the model would destroy its ability to perform other tasks.\n",
      "\n",
      "REFERENCE:\n",
      "Section 7 (Main Takeaways) point 5: 'The circuit uses only 11,200 of 110,592 possible dimensions (10.1% of total model capacity), suggesting IOI is implemented by a relatively sparse subcircuit' - this is about task-specific circuits, not model redundancy\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 14 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The documentation suggests 'Ablation Studies' as a next step to measure performance impact. Design a specific ablation experiment to test whether S-Inhibition Heads are causally necessary for the IOI circuit. What would you ablate, what would you measure, and what result would support their causal necessity?\n",
      "\n",
      "ANSWER:\n",
      "Experimental design: (1) Ablation: Zero out or remove the outputs of all identified S-Inhibition Heads (e.g., a8.h6, a7.h9, etc.) while keeping all other circuit components active. (2) Measurement: Evaluate model accuracy on the IOI task - specifically, measure how often the model incorrectly predicts the subject (S) instead of the indirect object (IO). Also measure the attention patterns of Name-Mover Heads to see if they now attend more to S2 position. (3) Expected result supporting causal necessity: Accuracy should drop significantly (if these heads are necessary), and there should be an increase in incorrect S predictions. If Name-Mover Heads start attending more to S2 when S-Inhibition Heads are ablated, this would provide strong evidence that S-Inhibition Heads causally suppress subject attention.\n",
      "\n",
      "REFERENCE:\n",
      "Section 6 (Next Steps) - Potential Extensions point 1 (Ablation Studies) and the hypothesis about S-Inhibition function in Section 1\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 15 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "Which attention head has the highest average attention score to its hypothesized target position?\n",
      "\n",
      "CHOICES:\n",
      "  A) a3.h0 (Duplicate Token Head) with 0.72 attention from S2 to S1\n",
      "  B) a8.h6 (S-Inhibition Head) with 0.74 attention from END to S2\n",
      "  C) a9.h9 (Name-Mover Head) with 0.80 attention from END to IO\n",
      "  D) All heads have approximately equal attention scores\n",
      "\n",
      "ANSWER:\n",
      "C) a9.h9 (Name-Mover Head) with 0.80 attention from END to IO\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.3 (Analysis Pipeline) Step 2 - Name-Mover Heads subsection lists a9.h9 with 0.7998 (≈0.80) as the top head\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 16 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The documentation mentions 'Backup Pathways' as an alternative hypothesis to explore. Propose a concrete experiment to test whether backup pathways exist in the IOI circuit. What would constitute evidence for backup pathways?\n",
      "\n",
      "ANSWER:\n",
      "Experiment: Perform progressive ablation of Name-Mover Heads in order of their attention strength (starting with a9.h9, then a10.h7, etc.). After each ablation, measure task accuracy. Evidence for backup pathways would include: (1) Graceful degradation - accuracy decreases gradually rather than catastrophically when ablating individual heads, (2) Attention pattern shifts - when top Name-Mover Heads are ablated, other heads (not originally classified as Name-Movers) begin showing increased END→IO attention, (3) Maintained above-chance performance - even after ablating the top 3-5 Name-Mover Heads, the model still performs significantly better than random guessing (>50%), (4) Recovery through alternative routes - analysis shows information flow through different layers or MLPs when primary heads are removed. The presence of 15 Name-Mover Heads (redundancy) already suggests backup pathways may exist.\n",
      "\n",
      "REFERENCE:\n",
      "Section 6 (Next Steps) - Alternative Hypotheses point 2 (Backup Pathways), and Section 5 (Analysis) - Key Observations point 4 about redundancy\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 17 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "What is the purpose of using TransformerLens for this analysis?\n",
      "\n",
      "CHOICES:\n",
      "  A) It provides better model accuracy than standard implementations\n",
      "  B) It enables activation caching and easy access to internal model states\n",
      "  C) It reduces computational requirements for running GPT2\n",
      "  D) It automatically identifies circuits without manual analysis\n",
      "\n",
      "ANSWER:\n",
      "B) It enables activation caching and easy access to internal model states\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.1 (Model Configuration) mentions using TransformerLens, and the code walkthrough would explain its utility for mechanistic interpretability through activation access\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 18 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The analysis used only 100 examples from a dataset of 10,000. What are two potential limitations of this choice, and what could be done to address them?\n",
      "\n",
      "ANSWER:\n",
      "Limitation 1: Statistical reliability - With only 100 examples, attention pattern averages might not be representative of the full distribution. Some heads might show high attention by chance on this small sample. Solution: Analyze the full 10,000 examples or use statistical significance testing to validate findings. Limitation 2: Pattern variability - Different sentence structures or name tokenizations in the unused 9,900 examples might reveal additional important heads or different attention patterns. Solution: Analyze performance across different subsets and validate that identified heads are consistent across diverse examples. The documentation acknowledges this by suggesting 'Larger Sample Analysis' as a next step.\n",
      "\n",
      "REFERENCE:\n",
      "Section 2 (Data) - Size: 10,000 examples (100 used for analysis), and Section 6 (Next Steps) - Potential Extensions point 2 (Larger Sample Analysis)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 19 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The documentation states that top attention heads show 'very strong attention patterns (>0.7)' to their targets. Why is high attention selectivity evidence for 'specialized functionality' rather than just random correlation?\n",
      "\n",
      "ANSWER:\n",
      "High attention selectivity (>0.7) is evidence for specialized functionality because: (1) These heads consistently attend to specific positions across different examples with varying sentence structures and names, showing they respond to the abstract pattern (e.g., 'second mention of subject') rather than surface features, (2) The attention is concentrated on functionally relevant positions (S1, S2, IO) rather than distributed randomly, suggesting the heads have learned task-relevant features, (3) Different head types show high attention to different position pairs, indicating functional differentiation rather than general attention, (4) The high values mean these heads are dedicating most of their attention capacity to these specific positions, implying this is their primary computational role. Random correlation would show more variable, lower, and less position-specific attention scores.\n",
      "\n",
      "REFERENCE:\n",
      "Section 5 (Analysis) - Key Observations point 2: 'High Selectivity: Top heads show very strong attention patterns (>0.7) to their hypothesized targets, indicating specialized functionality'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 20 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "Why did the researchers include all 12 MLPs in the circuit rather than selecting only the most relevant ones?\n",
      "\n",
      "CHOICES:\n",
      "  A) MLPs don't contribute meaningfully to the circuit, so their inclusion doesn't matter\n",
      "  B) The documentation doesn't explain this choice\n",
      "  C) Including all MLPs ensures comprehensive feature extraction and transformation while still fitting within the budget\n",
      "  D) MLPs were included by mistake\n",
      "\n",
      "ANSWER:\n",
      "C) Including all MLPs ensures comprehensive feature extraction and transformation while still fitting within the budget\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.3 (Analysis Pipeline) Step 3 - Circuit Node Selection: 'Included all 12 MLPs for feature extraction and transformation' and Section 5 (Analysis) mentions this as part of 'Efficient Budget Usage'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 21 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "Consider this scenario: You ablate all Duplicate Token Heads but the model's accuracy remains at 93%. What would this result suggest about the role of Duplicate Token Heads in the circuit? Provide two possible interpretations.\n",
      "\n",
      "ANSWER:\n",
      "Interpretation 1: Duplicate Token Heads may not be causally necessary for IOI performance. The circuit might rely more heavily on S-Inhibition and Name-Mover Heads, with positional information available through other means (e.g., positional embeddings, or MLPs processing position features). Interpretation 2: Backup pathways exist - other heads or MLPs compensate for the ablated Duplicate Token Heads by detecting the duplicate token pattern through alternative mechanisms. This would align with the 'Backup Pathways' hypothesis mentioned in Next Steps. Both interpretations highlight the difference between correlation (high attention patterns) and causation (necessary for task performance), which is why the documentation suggests ablation studies and activation patching as critical next steps to establish causal roles.\n",
      "\n",
      "REFERENCE:\n",
      "Section 6 (Next Steps) - points 1 (Ablation Studies) and point 2 under Alternative Hypotheses (Backup Pathways)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 22 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "The documentation suggests testing if identified heads 'generalize to other name-based tasks'. Describe a specific different task where you might expect the same Name-Mover Heads to be useful, and explain why.\n",
      "\n",
      "ANSWER:\n",
      "A suitable task would be 'Direct Object Identification' - predicting who receives an action in sentences like 'The teacher praised ___ and reprimanded John' (answer: Mary, if mentioned earlier). The same Name-Mover Heads might be useful because: (1) The core mechanism (attending to a specific name and copying it to the output) is similar, (2) The task requires distinguishing between two names and selecting the contextually appropriate one, (3) It involves similar positional reasoning about which name is the target of an action. However, you might need different S-Inhibition patterns since the logic of what to suppress differs. Testing this would involve measuring whether heads like a9.h9 and a10.h7 show high attention from the prediction position to the correct name in this new task.\n",
      "\n",
      "REFERENCE:\n",
      "Section 6 (Next Steps) - Potential Extensions point 4: 'Cross-Dataset Validation: Test if identified heads generalize to other name-based tasks'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 23 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "The circuit selection strategy involved 'adding 21 additional high-scoring heads to maximize circuit expressiveness'. What potential problem does this approach have?\n",
      "\n",
      "CHOICES:\n",
      "  A) It might include heads that don't contribute causally to IOI performance\n",
      "  B) It violates the budget constraint\n",
      "  C) It doesn't include enough MLPs\n",
      "  D) It focuses too much on early layers\n",
      "\n",
      "ANSWER:\n",
      "A) It might include heads that don't contribute causally to IOI performance\n",
      "\n",
      "REFERENCE:\n",
      "Section 3.3 Step 3 describes adding heads to 'maximize circuit expressiveness' and achieve budget utilization, but Section 6 (Next Steps) emphasizes the need for ablation studies and 'Circuit Refinement: Use causal intervention to identify minimal sufficient circuit', suggesting the current circuit may include non-causal components\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 24 [FREE_GENERATION]\n",
      "====================================================================================================\n",
      "\n",
      "Propose how 'Activation Patching' (mentioned in Next Steps) could be used to validate the hypothesis that a3.h0 is a Duplicate Token Head. Describe the patching procedure and the expected result if the hypothesis is correct.\n",
      "\n",
      "ANSWER:\n",
      "Activation Patching procedure: (1) Run the model on an IOI example and cache the activation of a3.h0, (2) Run the model on a 'counterfactual' example where the subject does NOT repeat (e.g., 'As Carl and Maria left...'), (3) During this second run, replace ('patch') the activation of a3.h0 with the cached activation from the original IOI example, (4) Measure whether this causes the model's behavior to change - specifically, does it make the model treat the non-repeated name as if it were repeated? Expected result if hypothesis is correct: The patched activation should cause downstream components (S-Inhibition Heads and Name-Mover Heads) to behave as if there IS a duplicate token, potentially causing the model to show IOI-like attention patterns even in the non-IOI sentence. This would demonstrate that a3.h0 causally contributes duplicate-token information to the circuit.\n",
      "\n",
      "REFERENCE:\n",
      "Section 6 (Next Steps) - Potential Extensions point 3: 'Activation Patching: Directly test causal role of each component' and the overall hypothesis about Duplicate Token Heads in Section 1\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION 25 [MULTIPLE_CHOICE]\n",
      "====================================================================================================\n",
      "\n",
      "According to the metadata structure, which of the following is NOT a field in each IOI example?\n",
      "\n",
      "CHOICES:\n",
      "  A) subject (S)\n",
      "  B) indirect_object (IO)\n",
      "  C) verb (the action performed)\n",
      "  D) place (the location)\n",
      "\n",
      "ANSWER:\n",
      "C) verb (the action performed)\n",
      "\n",
      "REFERENCE:\n",
      "Section 2 (Data) - Metadata Structure lists: subject (S), indirect_object (IO), object, and place. Verb is not mentioned as a metadata field.\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "END OF EXAM\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display all exam questions with answers in a formatted way\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"IOI CIRCUIT ANALYSIS - EXAM QUESTIONS AND GOLD ANSWERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, q in enumerate(exam_questions, 1):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"QUESTION {i} [{q['question_type'].upper()}]\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"\\n{q['question']}\\n\")\n",
    "    \n",
    "    if q['choices']:\n",
    "        print(\"CHOICES:\")\n",
    "        for choice in q['choices']:\n",
    "            print(f\"  {choice}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"ANSWER:\\n{q['answer']}\\n\")\n",
    "    print(f\"REFERENCE:\\n{q['reference']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"END OF EXAM\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f4e11c",
   "metadata": {
    "execution_status": "pending"
   },
   "outputs": [],
   "source": [
    "# Save the current notebook to the exam directory as exam_documentation.ipynb\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current notebook path\n",
    "current_notebook = Path('/home/smallyan/critic_model_mechinterp/notebooks/2025-11-10-19-16_ExamDesign.ipynb')\n",
    "destination_notebook = exam_dir / 'exam_documentation.ipynb'\n",
    "\n",
    "# Copy the notebook\n",
    "shutil.copy(current_notebook, destination_notebook)\n",
    "\n",
    "print(f\"Saved exam documentation notebook to: {destination_notebook}\")\n",
    "print(f\"\\nExam deliverables completed:\")\n",
    "print(f\"  1. {exam_json_path}\")\n",
    "print(f\"  2. {destination_notebook}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-10-19-16_ExamDesign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
