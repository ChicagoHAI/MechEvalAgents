[
  {
    "question_id": 1,
    "question_type": "multiple_choice",
    "question": "What is the primary goal of the IOI (Indirect Object Identification) task?",
    "gold_answer": "To predict the indirect object at the end of a sentence where two names appear, with one name repeated.",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 1 (Goal) and Section 2 (Data) - describes the IOI task structure and objective",
    "external_reference": false
  },
  {
    "question_id": 2,
    "question_type": "multiple_choice",
    "question": "In the example sentence 'As Carl and Maria left the consulate, Carl gave a fridge to ___', what are the positions S1, S2, and IO respectively?",
    "gold_answer": "S1 = position 2 (first 'Carl'), S2 = position 9 (second 'Carl'), IO = position 4 ('Maria')",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 2 (Data) - Example Sentence Structure with Key Positions",
    "external_reference": false
  },
  {
    "question_id": 3,
    "question_type": "free_generation",
    "question": "Describe the three functional components hypothesized to comprise the IOI circuit, including what positions they attend from/to and their proposed function.",
    "gold_answer": "1. Duplicate Token Heads: Attend from S2 to S1, signaling token duplication through position features. 2. S-Inhibition Heads: Attend from END to S2, inhibiting Name-Mover attention to subject positions to prevent incorrect predictions. 3. Name-Mover Heads: Attend from END to IO position, copying the indirect object token to the residual stream for final prediction.",
    "student_answer": "The three functional components are:\n\n1. **Duplicate Token Heads**: Attend from S2 \u2192 S1; detect and signal token duplication through position features\n2. **S-Inhibition Heads**: Attend from END \u2192 S2; inhibit Name-Mover attention to subject positions  \n3. **Name-Mover Heads**: Attend from END \u2192 IO; copy the indirect object token to the residual stream for prediction",
    "score": 5.0,
    "feedback": "Correctly identified all three components (Duplicate Token Heads, S-Inhibition Heads, Name Mover Heads). Mentions attention patterns correctly.",
    "reference": "Section 1 (Goal) - Hypothesis subsection",
    "external_reference": false
  },
  {
    "question_id": 4,
    "question_type": "multiple_choice",
    "question": "How many dimensions does each attention head write to the residual stream in GPT2-small?",
    "gold_answer": "64 dimensions",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 3.2 (Write Budget Constraints) - each head writes d_model/n_heads = 768/12 = 64 dims",
    "external_reference": false
  },
  {
    "question_id": 5,
    "question_type": "multiple_choice",
    "question": "What was the baseline accuracy of GPT2-small on the IOI task (100 example sample)?",
    "gold_answer": "94.00%",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 3.3 (Analysis Pipeline) - Step 1: Baseline Evaluation",
    "external_reference": false
  },
  {
    "question_id": 6,
    "question_type": "multiple_choice",
    "question": "Which attention head showed the highest average attention from S2 to S1 (Duplicate Token Head behavior)?",
    "gold_answer": "a3.h0 (0.7191)",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 3.3 (Analysis Pipeline) - Step 2: Duplicate Token Heads ranking",
    "external_reference": false
  },
  {
    "question_id": 7,
    "question_type": "multiple_choice",
    "question": "Which attention head showed the highest average attention from END to IO (Name-Mover Head behavior)?",
    "gold_answer": "a9.h9 (0.7998)",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 3.3 (Analysis Pipeline) - Step 2: Name-Mover Heads ranking",
    "external_reference": false
  },
  {
    "question_id": 8,
    "question_type": "multiple_choice",
    "question": "How many total nodes (including input, attention heads, and MLPs) are in the final identified circuit?",
    "gold_answer": "44 nodes (1 input + 31 heads + 12 MLPs)",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 4 (Results) - Final Circuit Composition",
    "external_reference": false
  },
  {
    "question_id": 9,
    "question_type": "free_generation",
    "question": "Explain why the identified circuit exhibits 'layered processing' and what functional role each layer group plays in the IOI task.",
    "gold_answer": "The circuit shows clear stratification across layers: Early layers (0-3) contain Duplicate Token Heads that detect when the same name appears twice, identifying the S1/S2 relationship. Middle layers (7-8) contain S-Inhibition Heads that suppress attention to the subject position to prevent the model from incorrectly predicting the repeated name. Late layers (9-11) contain Name-Mover Heads that attend to the indirect object and copy it to the output position for final prediction. This hierarchical processing mirrors a compositional computation: detect pattern \u2192 suppress interference \u2192 output answer.",
    "student_answer": "The circuit exhibits 'layered processing' because different functional components are concentrated in different layers of the model, creating a hierarchical pipeline:\n\n1. **Early layers (0-3)**: Perform duplicate token detection. These layers identify that the subject token appears twice in the sequence (S1 and S2 positions) and extract positional features.\n\n2. **Middle layers (7-8)**: Handle subject inhibition. These layers suppress attention to the subject positions to prevent the model from i",
    "score": 5.0,
    "feedback": "Excellent answer with comprehensive coverage.",
    "reference": "Section 5 (Analysis) - Key Observations #1",
    "external_reference": false
  },
  {
    "question_id": 10,
    "question_type": "free_generation",
    "question": "The documentation states that the circuit achieves 100% budget utilization with exactly 11,200 dimensions. If the researchers wanted to add 5 more attention heads to the circuit, how many MLPs would they need to remove to stay within budget? Show your calculation.",
    "gold_answer": "Each attention head writes 64 dimensions, so 5 heads = 5 \u00d7 64 = 320 dimensions. Each MLP writes 768 dimensions. To stay within 11,200 total: 320/768 = 0.417, meaning they would need to remove at least 1 MLP (768 dims) to free up enough space. Removing 1 MLP frees 768 dims, allowing 5 heads (320 dims) with 448 dims to spare. Answer: 1 MLP.",
    "student_answer": "To add 5 attention heads while staying within the 11,200 dimension budget:\n\n**Calculation:**\n- 5 additional heads \u00d7 64 dims/head = 320 additional dimensions needed\n- 320 dimensions \u00f7 768 dims/MLP = 0.4167 MLPs\n- Since MLPs are indivisible units, we must remove at least 1 MLP\n\n**Answer: 1 MLP must be removed**\n\nVerification:\n- Original: 31 heads + 12 MLPs = 1,984 + 9,216 = 11,200 dims\n- Modified: 36 heads + 11 MLPs = 2,304 + 8,448 = 10,752 dims \u2713 (within budget)",
    "score": 4.0,
    "feedback": "Good answer with solid coverage of key concepts.",
    "reference": "Section 3.2 (Write Budget Constraints) and Section 4 (Budget Verification table)",
    "external_reference": false
  },
  {
    "question_id": 11,
    "question_type": "multiple_choice",
    "question": "Why does the documentation suggest that finding multiple heads per functional category indicates 'robustness through redundancy'?",
    "gold_answer": "Multiple heads performing similar functions provide backup pathways, so if one head fails or is ablated, others can compensate, making the circuit more resilient to perturbations.",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 5 (Analysis) - Key Observations #4",
    "external_reference": false
  },
  {
    "question_id": 12,
    "question_type": "free_generation",
    "question": "Based on the methodology described, propose a concrete experiment to test whether the S-Inhibition Heads are causally necessary for the IOI task. What would you measure and what result would confirm their necessity?",
    "gold_answer": "Conduct an ablation study: zero out or mean-ablate all S-Inhibition Heads (e.g., a8.h6, a7.h9, etc.) while keeping other circuit components intact. Run the model on IOI examples and measure accuracy. If S-Inhibition Heads are causally necessary, we would expect a significant drop in accuracy, specifically with the model now incorrectly predicting the subject (S) instead of the indirect object (IO). A large decrease (e.g., from 94% to below 60%, near chance for binary choice) would confirm their causal role in suppressing subject interference.",
    "student_answer": "**Proposed Experiment: Ablation Study of S-Inhibition Heads**\n\n**Procedure:**\n1. Identify all 12 S-Inhibition heads in the circuit (those with strong END \u2192 S2 attention, particularly a8.h6, a7.h9, a8.h10, a8.h5, and a9.h7)\n2. Run the model on the 100 IOI test examples with these heads ablated (set their outputs to zero or remove them from the circuit)\n3. Evaluate model predictions and compare to baseline performance\n\n**Measurements:**\n- Overall accuracy on IOI task (baseline: 94%)\n- Specifically",
    "score": 4.0,
    "feedback": "Good answer with solid coverage of key concepts. Answer references external concepts but also includes documentation-based content.",
    "reference": "Section 6 (Next Steps) - Ablation Studies suggestion and Section 5 understanding of S-Inhibition function",
    "external_reference": true
  },
  {
    "question_id": 13,
    "question_type": "multiple_choice",
    "question": "The circuit uses only 11,200 of 110,592 possible dimensions (10.1% of total model capacity). What does this suggest about how GPT2-small implements the IOI behavior?",
    "gold_answer": "IOI is implemented by a relatively sparse, specialized subcircuit rather than requiring the full model capacity, suggesting modular functional organization.",
    "student_answer": "Selected: A",
    "score": 1.0,
    "feedback": "Correct answer selected.",
    "reference": "Section 7 (Main Takeaways) - point #5 on efficient representation",
    "external_reference": false
  },
  {
    "question_id": 14,
    "question_type": "free_generation",
    "question": "Suppose you found a new attention head a5.h7 that shows strong attention from END to S1 (not S2). How would you classify this head relative to the three hypothesized categories, and what function might it serve?",
    "gold_answer": "This head doesn't fit the three standard categories (Duplicate Token: S2\u2192S1, S-Inhibition: END\u2192S2, Name-Mover: END\u2192IO). It could be a 'Negative Name Mover' or 'Subject Suppressor' that attends to S1 to actively suppress it from the output (mentioned as an alternative hypothesis). Alternatively, it might be part of a backup pathway that helps distinguish between S1 and IO positions. Its functional role would need to be tested through ablation or activation patching experiments.",
    "student_answer": "The head a5.h7 (END \u2192 S1 attention) does not fit into any of the three hypothesized categories (Duplicate Token, S-Inhibition, or Name-Mover).\n\n**Classification:** This would likely be a variant of an inhibition head, possibly what the documentation refers to as a \"Negative Name Mover\" in the alternative hypotheses section.\n\n**Proposed Function:** Since S1 is the first mention of the subject (which is the incorrect answer for IOI), this head might serve to suppress or inhibit attention to the fi",
    "score": 5.0,
    "feedback": "Excellent answer with comprehensive coverage.",
    "reference": "Section 6 (Next Steps) - Alternative Hypotheses: Negative Name Movers and understanding of the three main categories",
    "external_reference": false
  },
  {
    "question_id": 15,
    "question_type": "free_generation",
    "question": "Write code to verify the budget constraint calculation. Given the final circuit composition (31 attention heads and 12 MLPs), compute the total dimensional write and verify it equals 11,200. Also calculate what percentage of the budget is used by attention heads vs MLPs.",
    "gold_answer": "See code solution CQ1",
    "student_answer": "### Question 15 (CQ1): Budget Constraint Verification and Component Analysis\n\n**Question:** Write code to verify the budget constraint calculation. Given the final circuit composition (31 attention heads and 12 MLPs), compute the total dimensional write and verify it equals 11,200. Also calculate wh\n\n[Code provided: True]",
    "score": 5.0,
    "feedback": "Code provided. Code executed successfully. Correct total budget calculated (11,200 dimensions).",
    "reference": "Section 3.2 (Write Budget Constraints) and Section 4 (Budget Verification)",
    "external_reference": false
  },
  {
    "question_id": 16,
    "question_type": "free_generation",
    "question": "Write code to analyze the layer distribution of the 31 attention heads in the circuit. Count how many heads are in each layer (0-11) and identify which layer has the most heads. Then determine if there's a trend showing more heads in later layers.",
    "gold_answer": "See code solution CQ2",
    "student_answer": "### Question 16 (CQ2): Layer Distribution Analysis of Attention Heads\n\n**Question:** Write code to analyze the layer distribution of the 31 attention heads in the circuit. Count how many heads are in each layer (0-11) and identify which layer has the most heads. Then determine if there's a trend sho\n\n[Code provided: True]",
    "score": 5.0,
    "feedback": "Code provided. Code executed successfully. Correct layer distribution analysis (layer 10 has 7 heads).",
    "reference": "Section 4 (Results) - Layer Distribution and understanding of layered processing",
    "external_reference": false
  },
  {
    "question_id": 17,
    "question_type": "free_generation",
    "question": "Write code to simulate what would happen if you wanted to create a minimal IOI circuit using only the top-1 head from each functional category (Duplicate Token, S-Inhibition, Name-Mover) plus all 12 MLPs. Calculate the total budget used and how much budget remains unused compared to the full circuit.",
    "gold_answer": "See code solution CQ3",
    "student_answer": "### Question 17 (CQ3): Minimal Circuit Budget Calculation\n\n**Question:** Write code to simulate what would happen if you wanted to create a minimal IOI circuit using only the top-1 head from each functional category (Duplicate Token: a3.h0, S-Inhibition: a8.h6, Name-Mover: a9.h9) plus all 12 MLPs. C\n\n[Code provided: True]",
    "score": 5.0,
    "feedback": "Code provided. Code executed successfully. Correct minimal budget calculated (9,408 dimensions).",
    "reference": "Section 3.3 (Circuit Node Selection) and Section 4 (Budget Verification)",
    "external_reference": false
  }
]