[
  {
    "question_type": "multiple_choice",
    "question": "In the ICoT multiplication task, the numbers are represented in a specific digit order. How would the multiplication problem 8331 \u00d7 5015 be formatted as input to the model?",
    "choice": [
      "A) 8331 * 5015",
      "B) 1338 * 5105",
      "C) 3318 * 1505",
      "D) 5015 * 8331"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Which of the following statements correctly describes the performance difference between ICoT and standard fine-tuning (SFT) models on 4\u00d74 digit multiplication?",
    "choice": [
      "A) Both achieve >95% accuracy, but ICoT trains faster",
      "B) SFT achieves 81% accuracy while ICoT achieves 100%",
      "C) ICoT achieves 100% accuracy while SFT achieves <1% accuracy",
      "D) ICoT achieves 99% accuracy only when using auxiliary loss"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Describe the two-layer attention tree mechanism discovered in the ICoT model. What role does each layer play in computing the output digits?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "The ICoT model represents digits using Fourier bases. Which set of frequency components (k values) are used in the discovered Fourier basis representation?",
    "choice": [
      "A) k \u2208 {0, 1, 2, 3, 4}",
      "B) k \u2208 {0, 1, 2, 5}",
      "C) k \u2208 {0, 2, 4, 5}",
      "D) k \u2208 {1, 2, 3, 4, 5}"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Explain why standard fine-tuning (SFT) fails to learn multi-digit multiplication, despite the model having sufficient capacity. What specific pattern is observed in gradient norms and loss during SFT training?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "What geometric structure emerges in the ICoT model's attention head outputs, and how is it mathematically characterized?",
    "choice": [
      "A) Minkowski sums: ATT\u00b9(i,j) = \u03b1A\u1d62 + (1-\u03b1)B\u2c7c + \u03b5, creating nested cluster structures",
      "B) Euclidean products: ATT\u00b9(i,j) = A\u1d62 \u00b7 B\u2c7c, creating orthogonal representations",
      "C) Tensor products: ATT\u00b9(i,j) = A\u1d62 \u2297 B\u2c7c, creating high-dimensional embeddings",
      "D) Cartesian products: ATT\u00b9(i,j) = (A\u1d62, B\u2c7c), creating paired representations"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "How many chain-of-thought (CoT) tokens are removed per epoch during ICoT training, and how many total epochs are needed for convergence?",
    "choice": [
      "A) 5 tokens per epoch, 10 epochs total",
      "B) 10 tokens per epoch, 8 epochs total",
      "C) 13 tokens per epoch, 8 epochs total",
      "D) 8 tokens per epoch, 13 epochs total"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "What do the linear probe experiments reveal about the ICoT model's internal representations? Specifically, what is being probed, and what do the results indicate about the difference between ICoT and SFT models?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Based on the discovered mechanisms in the ICoT model for 4\u00d74 digit multiplication, predict what architectural changes would be necessary to successfully learn 6\u00d76 digit multiplication using the same ICoT training approach. Justify your prediction.",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "Suppose you wanted to train a model to perform multi-digit division using insights from the ICoT multiplication study. Which training approach would most likely succeed?",
    "choice": [
      "A) Standard fine-tuning on division problems with a 12-layer model",
      "B) Standard fine-tuning on division problems with more training data",
      "C) ICoT-style training with gradual removal of long-division step tokens",
      "D) Training only on the final quotient without any intermediate steps"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The ICoT model shows specific logit attribution patterns where input digit positions affect output digits. Based on the multiplication algorithm, explain why digit a\u1d62 should affect output digit c\u2096 most strongly when there exists a digit b\u2c7c such that i+j=k. What does this pattern reveal about the model's learned algorithm?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The auxiliary loss model achieves 99% accuracy without explicit chain-of-thought tokens by predicting \u0109\u2096 values. Why does this approach work? What inductive bias does it provide that standard fine-tuning lacks?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "A researcher hypothesizes that SFT fails at 4\u00d74 multiplication due to insufficient model capacity, and proposes training a 24-layer, 16-head model. Based on the paper's findings, what would be the most likely outcome?",
    "choice": [
      "A) The larger model would achieve >90% accuracy due to increased capacity",
      "B) The larger model would still achieve <1% accuracy, failing identically to smaller models",
      "C) The larger model would achieve 50-60% accuracy, showing partial improvement",
      "D) The larger model would require less training data to achieve 100% accuracy"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Describe the pentagonal prism geometry discovered in ICoT's 3D PCA analysis. What do the three principal components represent, and why does this geometry emerge?",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "When training linear probes to decode the running sum \u0109\u2096, which location in the model provides the best decoding accuracy?",
    "choice": [
      "A) Layer 0 residual stream (after first layer MLP)",
      "B) Layer 1 residual stream (after second layer MLP)",
      "C) Layer 2 mid-point (after attention, before MLP)",
      "D) Final hidden layer before unembedding"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "The paper identifies that SFT fails due to an 'optimization problem' rather than a 'capacity problem'. Explain what this distinction means and provide evidence from the paper supporting this conclusion.",
    "choice": [],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "multiple_choice",
    "question": "In the Fourier basis analysis, what median R\u00b2 value is achieved when fitting the ICoT model's final hidden layer representations using all Fourier frequencies k \u2208 {0,1,2,3,4,5}?",
    "choice": [
      "A) 0.84",
      "B) 0.95",
      "C) 0.99",
      "D) 1.0"
    ],
    "requires_code": false,
    "code_id": null
  },
  {
    "question_type": "free_generation",
    "question": "Write code to verify the logit attribution pattern discovered in ICoT models. Specifically, test the hypothesis that input digit position a\u1d62 has the strongest effect on output digit c\u2096 when there exists a position j such that i+j=k.\n\nYour code should:\n1. Load the ICoT model checkpoint\n2. Create a batch of 100 random 4\u00d74 multiplication problems\n3. For each problem, compute logit attribution by: (a) recording baseline logits for output positions c\u2082 through c\u2086, (b) for each input digit position, create a counterfactual by swapping the digit, (c) measure the change in logits\n4. For each output position c\u2096 (k=2 to 6), compute the average absolute logit change for each input position\n5. Identify which input positions have the highest attribution scores for each output position\n6. Verify if the pattern matches i+j=k (print the top-3 most influential input positions for each output)\n\nExpected output: For each output digit c\u2096, the positions with indices that sum to k should show highest attribution.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ1"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to replicate the linear probe experiment that demonstrates ICoT models encode the running sum \u0109\u2096 in their hidden states.\n\nYour code should:\n1. Load the ICoT model and extract hidden states at layer 2 mid-point (after attention, before MLP)\n2. For a validation set of 200 multiplication problems, extract hidden states at the timestep where c\u2096 is being computed (for k=2,3,4)\n3. Compute the ground truth \u0109\u2096 values (sum of all partial products with indices \u2264 k plus any carry)\n4. Train a simple linear regression probe (using sklearn or torch) to predict \u0109\u2096 from the hidden states\n5. Evaluate the Mean Absolute Error (MAE) on a held-out test set of 100 problems\n6. Print the MAE for each k \u2208 {2,3,4}\n\nExpected outcome: MAE should be low (<5.0) for all three positions, demonstrating that \u0109 values are linearly decodable from hidden states.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ2"
  },
  {
    "question_type": "free_generation",
    "question": "Write code to compute the R\u00b2 fit of the Fourier basis representation for digit embeddings in the ICoT model.\n\nYour code should:\n1. Load the ICoT model and extract the embedding matrix for digits 0-9\n2. Construct the Fourier basis matrix \u03a6 with frequencies k \u2208 {0, 1, 2, 5}:\n   - Column 0: constant (all 1s)\n   - Columns 1-2: cos(2\u03c0n/10), sin(2\u03c0n/10) for n=0..9\n   - Columns 3-4: cos(2\u03c0n/5), sin(2\u03c0n/5) for n=0..9  \n   - Column 5: parity p(n) = (-1)\u207f for n=0..9\n3. For each dimension d in the embedding (768 dims), extract the vector x_d of length 10 (values for digits 0-9)\n4. Fit coefficients: C_d = argmin_C ||x_d - \u03a6C||\u00b2\n5. Compute R\u00b2 = 1 - ||x_d - \u03a6C_d||\u00b2 / ||x_d - mean(x_d)||\u00b2\n6. Report the median R\u00b2 across all embedding dimensions\n\nExpected outcome: Median R\u00b2 should be high (>0.80), confirming Fourier basis structure in embeddings.",
    "choice": [],
    "requires_code": true,
    "code_id": "CQ3"
  }
]