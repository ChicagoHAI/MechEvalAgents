{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a71cd5",
   "metadata": {},
   "source": [
    "# ICoT Multiplication Research Assessment\n",
    "\n",
    "This notebook contains comprehensive questions designed to evaluate understanding of the research documented in the ICoT (Implicit Chain-of-Thought) multiplication study.\n",
    "\n",
    "**Important:** All questions should be answered based solely on the documentation provided. Reference the documentation sections as needed.\n",
    "\n",
    "## Knowledge Key Points\n",
    "\n",
    "The assessment covers the following key knowledge areas:\n",
    "\n",
    "### 1. Research Problem & Motivation\n",
    "- Why Transformers fail at multi-digit multiplication\n",
    "- The significance of 4×4 digit multiplication as a test case\n",
    "- The gap between model scale and reasoning capability\n",
    "\n",
    "### 2. Data Format & Processing  \n",
    "- Least-significant-digit-first representation\n",
    "- Chain-of-thought token format and gradual removal\n",
    "- Special delimiter tokens and their purpose\n",
    "\n",
    "### 3. Model Architectures & Training\n",
    "- ICoT vs SFT vs Auxiliary Loss models\n",
    "- Architecture specifications (layers, heads, dimensions)\n",
    "- Training hyperparameters and convergence\n",
    "\n",
    "### 4. Long-Range Dependencies\n",
    "- Definition and requirement for multiplication\n",
    "- Logit attribution patterns\n",
    "- Linear probe experiments and ĉₖ prediction\n",
    "\n",
    "### 5. Attention Tree Mechanism\n",
    "- Two-layer caching and retrieval structure\n",
    "- Binary tree-like information flow\n",
    "- Role of each layer in the computation\n",
    "\n",
    "### 6. Geometric Representations\n",
    "- Minkowski sums in attention heads\n",
    "- Fourier basis with frequencies k ∈ {0,1,2,5}\n",
    "- Pentagonal prism structure in 3D PCA\n",
    "\n",
    "### 7. Training Dynamics & Failure Modes\n",
    "- SFT learning pattern (c₀, c₁, c₇ first)\n",
    "- Gradient plateau for middle digits\n",
    "- Local optimum trap vs capacity limitation\n",
    "\n",
    "### 8. ICoT Success Factors\n",
    "- Implicit supervision through CoT removal\n",
    "- Guided discovery of computational structures\n",
    "- Internalization of intermediate computations\n",
    "\n",
    "### 9. Alternative Approaches\n",
    "- Auxiliary loss for ĉₖ prediction\n",
    "- Inductive biases that enable learning\n",
    "- Comparison with explicit supervision\n",
    "\n",
    "### 10. Broader Implications\n",
    "- Optimization challenges in transformer training\n",
    "- Value of mechanistic interpretability\n",
    "- Process supervision for complex reasoning\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745c87a",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "In the ICoT multiplication task, the numbers are represented in a specific digit order. How would the multiplication problem 8331 × 5015 be formatted as input to the model?\n",
    "\n",
    "A) 8331 * 5015\n",
    "B) 1338 * 5105\n",
    "C) 3318 * 1505\n",
    "D) 5015 * 8331\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da918d6d",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "Which of the following statements correctly describes the performance difference between ICoT and standard fine-tuning (SFT) models on 4×4 digit multiplication?\n",
    "\n",
    "A) Both achieve >95% accuracy, but ICoT trains faster\n",
    "B) SFT achieves 81% accuracy while ICoT achieves 100%\n",
    "C) ICoT achieves 100% accuracy while SFT achieves <1% accuracy\n",
    "D) ICoT achieves 99% accuracy only when using auxiliary loss\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2585acb",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "Describe the two-layer attention tree mechanism discovered in the ICoT model. What role does each layer play in computing the output digits?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa0c3",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "The ICoT model represents digits using Fourier bases. Which set of frequency components (k values) are used in the discovered Fourier basis representation?\n",
    "\n",
    "A) k ∈ {0, 1, 2, 3, 4}\n",
    "B) k ∈ {0, 1, 2, 5}\n",
    "C) k ∈ {0, 2, 4, 5}\n",
    "D) k ∈ {1, 2, 3, 4, 5}\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2e04b",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "Explain why standard fine-tuning (SFT) fails to learn multi-digit multiplication, despite the model having sufficient capacity. What specific pattern is observed in gradient norms and loss during SFT training?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80031979",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "What geometric structure emerges in the ICoT model's attention head outputs, and how is it mathematically characterized?\n",
    "\n",
    "A) Minkowski sums: ATT¹(i,j) = αAᵢ + (1-α)Bⱼ + ε, creating nested cluster structures\n",
    "B) Euclidean products: ATT¹(i,j) = Aᵢ · Bⱼ, creating orthogonal representations\n",
    "C) Tensor products: ATT¹(i,j) = Aᵢ ⊗ Bⱼ, creating high-dimensional embeddings\n",
    "D) Cartesian products: ATT¹(i,j) = (Aᵢ, Bⱼ), creating paired representations\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50f750",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "How many chain-of-thought (CoT) tokens are removed per epoch during ICoT training, and how many total epochs are needed for convergence?\n",
    "\n",
    "A) 5 tokens per epoch, 10 epochs total\n",
    "B) 10 tokens per epoch, 8 epochs total\n",
    "C) 13 tokens per epoch, 8 epochs total\n",
    "D) 8 tokens per epoch, 13 epochs total\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6c784",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "What do the linear probe experiments reveal about the ICoT model's internal representations? Specifically, what is being probed, and what do the results indicate about the difference between ICoT and SFT models?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def32ca0",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "Based on the discovered mechanisms in the ICoT model for 4×4 digit multiplication, predict what architectural changes would be necessary to successfully learn 6×6 digit multiplication using the same ICoT training approach. Justify your prediction.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccd9b0",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "Suppose you wanted to train a model to perform multi-digit division using insights from the ICoT multiplication study. Which training approach would most likely succeed?\n",
    "\n",
    "A) Standard fine-tuning on division problems with a 12-layer model\n",
    "B) Standard fine-tuning on division problems with more training data\n",
    "C) ICoT-style training with gradual removal of long-division step tokens\n",
    "D) Training only on the final quotient without any intermediate steps\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906fc14",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "The ICoT model shows specific logit attribution patterns where input digit positions affect output digits. Based on the multiplication algorithm, explain why digit aᵢ should affect output digit cₖ most strongly when there exists a digit bⱼ such that i+j=k. What does this pattern reveal about the model's learned algorithm?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8cc79a",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "The auxiliary loss model achieves 99% accuracy without explicit chain-of-thought tokens by predicting ĉₖ values. Why does this approach work? What inductive bias does it provide that standard fine-tuning lacks?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07805e0e",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "A researcher hypothesizes that SFT fails at 4×4 multiplication due to insufficient model capacity, and proposes training a 24-layer, 16-head model. Based on the paper's findings, what would be the most likely outcome?\n",
    "\n",
    "A) The larger model would achieve >90% accuracy due to increased capacity\n",
    "B) The larger model would still achieve <1% accuracy, failing identically to smaller models\n",
    "C) The larger model would achieve 50-60% accuracy, showing partial improvement\n",
    "D) The larger model would require less training data to achieve 100% accuracy\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5884f",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "Describe the pentagonal prism geometry discovered in ICoT's 3D PCA analysis. What do the three principal components represent, and why does this geometry emerge?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdefc8",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "When training linear probes to decode the running sum ĉₖ, which location in the model provides the best decoding accuracy?\n",
    "\n",
    "A) Layer 0 residual stream (after first layer MLP)\n",
    "B) Layer 1 residual stream (after second layer MLP)\n",
    "C) Layer 2 mid-point (after attention, before MLP)\n",
    "D) Final hidden layer before unembedding\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bcf3c",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "\n",
    "**Type:** Free Generation\n",
    "\n",
    "The paper identifies that SFT fails due to an 'optimization problem' rather than a 'capacity problem'. Explain what this distinction means and provide evidence from the paper supporting this conclusion.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebc249",
   "metadata": {},
   "source": [
    "## Question 17\n",
    "\n",
    "**Type:** Multiple Choice\n",
    "\n",
    "In the Fourier basis analysis, what median R² value is achieved when fitting the ICoT model's final hidden layer representations using all Fourier frequencies k ∈ {0,1,2,3,4,5}?\n",
    "\n",
    "A) 0.84\n",
    "B) 0.95\n",
    "C) 0.99\n",
    "D) 1.0\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0519b6",
   "metadata": {},
   "source": [
    "## Question 18 [CQ1]\n",
    "\n",
    "**Type:** Code-Based Question\n",
    "\n",
    "Write code to verify the logit attribution pattern discovered in ICoT models. Specifically, test the hypothesis that input digit position aᵢ has the strongest effect on output digit cₖ when there exists a position j such that i+j=k.\n",
    "\n",
    "Your code should:\n",
    "1. Load the ICoT model checkpoint\n",
    "2. Create a batch of 100 random 4×4 multiplication problems\n",
    "3. For each problem, compute logit attribution by: (a) recording baseline logits for output positions c₂ through c₆, (b) for each input digit position, create a counterfactual by swapping the digit, (c) measure the change in logits\n",
    "4. For each output position cₖ (k=2 to 6), compute the average absolute logit change for each input position\n",
    "5. Identify which input positions have the highest attribution scores for each output position\n",
    "6. Verify if the pattern matches i+j=k (print the top-3 most influential input positions for each output)\n",
    "\n",
    "Expected output: For each output digit cₖ, the positions with indices that sum to k should show highest attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb14768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ1: Logit Attribution Pattern Verification\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/smallyan/critic_model_mechinterp/icot')\n",
    "from src.model_utils import load_hf_model\n",
    "from src.data_utils import prompt_ci_raw_format_batch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# TODO: Load the ICoT model\n",
    "# model, tokenizer = load_hf_model(...)\n",
    "\n",
    "# TODO: Generate 100 random 4×4 multiplication problems\n",
    "# operands = [(a, b), ...] where a, b are 4-digit strings (reversed)\n",
    "\n",
    "# TODO: For each problem, compute baseline logits for output positions c2-c6\n",
    "\n",
    "# TODO: For each input digit position, create counterfactuals and measure logit changes\n",
    "\n",
    "# TODO: Aggregate attribution scores and identify top-3 influential positions for each output\n",
    "\n",
    "# TODO: Print results showing which input positions most affect each output position\n",
    "# Expected: positions where i+j=k should show highest attribution\n",
    "\n",
    "print(\"Top-3 influential input positions for each output digit:\")\n",
    "# Print your results here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283624f",
   "metadata": {},
   "source": [
    "## Question 19 [CQ2]\n",
    "\n",
    "**Type:** Code-Based Question\n",
    "\n",
    "Write code to replicate the linear probe experiment that demonstrates ICoT models encode the running sum ĉₖ in their hidden states.\n",
    "\n",
    "Your code should:\n",
    "1. Load the ICoT model and extract hidden states at layer 2 mid-point (after attention, before MLP)\n",
    "2. For a validation set of 200 multiplication problems, extract hidden states at the timestep where cₖ is being computed (for k=2,3,4)\n",
    "3. Compute the ground truth ĉₖ values (sum of all partial products with indices ≤ k plus any carry)\n",
    "4. Train a simple linear regression probe (using sklearn or torch) to predict ĉₖ from the hidden states\n",
    "5. Evaluate the Mean Absolute Error (MAE) on a held-out test set of 100 problems\n",
    "6. Print the MAE for each k ∈ {2,3,4}\n",
    "\n",
    "Expected outcome: MAE should be low (<5.0) for all three positions, demonstrating that ĉ values are linearly decodable from hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ2: Running Sum Linear Probe Accuracy\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "import sys\n",
    "sys.path.append('/home/smallyan/critic_model_mechinterp/icot')\n",
    "from src.model_utils import load_hf_model\n",
    "from src.data_utils import read_operands, get_ci\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# TODO: Load ICoT model\n",
    "# model, tokenizer = load_hf_model(...)\n",
    "\n",
    "# TODO: Load validation data (300 problems: 200 train, 100 test)\n",
    "# operands = read_operands('/home/smallyan/critic_model_mechinterp/icot/data/processed_valid.txt')\n",
    "\n",
    "# TODO: For k in {2, 3, 4}:\n",
    "#   1. Extract hidden states at layer 2 mid-point at timestep t_ck\n",
    "#   2. Compute ground truth ĉₖ (sum of products with indices ≤ k plus carry)\n",
    "#   3. Train linear regression probe on 200 samples\n",
    "#   4. Evaluate MAE on 100 test samples\n",
    "\n",
    "# TODO: Print MAE for each k\n",
    "print(\"Mean Absolute Error for running sum prediction:\")\n",
    "# print(f\"ĉ₂: {mae_2:.2f}\")\n",
    "# print(f\"ĉ₃: {mae_3:.2f}\")\n",
    "# print(f\"ĉ₄: {mae_4:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ffd16",
   "metadata": {},
   "source": [
    "## Question 20 [CQ3]\n",
    "\n",
    "**Type:** Code-Based Question\n",
    "\n",
    "Write code to compute the R² fit of the Fourier basis representation for digit embeddings in the ICoT model.\n",
    "\n",
    "Your code should:\n",
    "1. Load the ICoT model and extract the embedding matrix for digits 0-9\n",
    "2. Construct the Fourier basis matrix Φ with frequencies k ∈ {0, 1, 2, 5}:\n",
    "   - Column 0: constant (all 1s)\n",
    "   - Columns 1-2: cos(2πn/10), sin(2πn/10) for n=0..9\n",
    "   - Columns 3-4: cos(2πn/5), sin(2πn/5) for n=0..9  \n",
    "   - Column 5: parity p(n) = (-1)ⁿ for n=0..9\n",
    "3. For each dimension d in the embedding (768 dims), extract the vector x_d of length 10 (values for digits 0-9)\n",
    "4. Fit coefficients: C_d = argmin_C ||x_d - ΦC||²\n",
    "5. Compute R² = 1 - ||x_d - ΦC_d||² / ||x_d - mean(x_d)||²\n",
    "6. Report the median R² across all embedding dimensions\n",
    "\n",
    "Expected outcome: Median R² should be high (>0.80), confirming Fourier basis structure in embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d183165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ3: Fourier Basis R² Computation\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/smallyan/critic_model_mechinterp/icot')\n",
    "from src.model_utils import load_hf_model\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# TODO: Load ICoT model and extract embedding matrix for digits 0-9\n",
    "# model, tokenizer = load_hf_model(...)\n",
    "# embeddings = model.transformer.wte.weight[0:10, :].detach().cpu().numpy()  # Shape: (10, 768)\n",
    "\n",
    "# TODO: Construct Fourier basis matrix Φ (shape: 10 x 6)\n",
    "# Columns: [constant, cos(2πn/10), sin(2πn/10), cos(2πn/5), sin(2πn/5), parity]\n",
    "# n = np.arange(10)\n",
    "# phi = np.column_stack([\n",
    "#     np.ones(10),\n",
    "#     np.cos(2 * np.pi * n / 10),\n",
    "#     np.sin(2 * np.pi * n / 10),\n",
    "#     np.cos(2 * np.pi * n / 5),\n",
    "#     np.sin(2 * np.pi * n / 5),\n",
    "#     (-1) ** n\n",
    "# ])\n",
    "\n",
    "# TODO: For each embedding dimension d:\n",
    "#   1. Extract vector x_d of length 10 (values for digits 0-9)\n",
    "#   2. Fit: C_d = argmin_C ||x_d - Φ @ C||² (use least squares)\n",
    "#   3. Compute R² = 1 - ||x_d - Φ @ C_d||² / ||x_d - mean(x_d)||²\n",
    "\n",
    "# TODO: Compute median R² across all dimensions\n",
    "# median_r2 = np.median(r2_values)\n",
    "\n",
    "print(\"Median R² for Fourier basis fit:\")\n",
    "# print(f\"R² = {median_r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
